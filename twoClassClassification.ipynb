{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook twoClassClassification.ipynb to python\n",
      "[NbConvertApp] Writing 21528 bytes to test13-8.py\n"
     ]
    }
   ],
   "source": [
    "# # Convert to python script, remember to delete/comment the next line in the actual file\n",
    "# ! jupyter nbconvert --to python twoClassClassification.ipynb --output test13-8.py\n",
    "\n",
    "# # Run the notebook in Simpson GPU server\n",
    "# CUDA_VISIBLE_DEVICES=0 python testSamples2-8.py -batchSize=16 -epochs=100 -lr=0.001 -evalDetailLine=\"majourity voting on smote with 2 clases\" -hasBackground=f -usesLargestBox=f -segmentsMultiple=12 -dropoutRate=0.2 -grouped2D=t -modelChosen='Small2D' -votingSystem='majority'\n",
    "# CUDA_VISIBLE_DEVICES=0 python test13-8.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Parameters\n",
    "testName = 'testSamples13-8'\n",
    "evalDetailLine = \"test if k folds works\"\n",
    "batchSize = 16\n",
    "numOfEpochs = 2\n",
    "learningRate = 0.001\n",
    "hasBackground = True\n",
    "usesLargestBox = True\n",
    "segmentsMultiple = 6\n",
    "grouped2D = True\n",
    "weight_decay = 0.01\n",
    "modelChosen = 'ResNet50Small2D' #'ResNet50Small2D', 'VGG16Small2D', 'InceptionV3Small2D'\n",
    "votingSystem = 'average' #average, singleLargest\n",
    "patience = 10\n",
    "sampleStrategy = 'underSampling' # 'underSampling', 'overSampling', 'normal' \n",
    "\n",
    "testInformation = {\n",
    "    'testName' : testName,\n",
    "    'evalDetailLine' : evalDetailLine,\n",
    "    'batchSize': batchSize,\n",
    "    'numOfEpochs': numOfEpochs,\n",
    "    'learningRate': learningRate,\n",
    "    'hasBackground': hasBackground,\n",
    "    'usesLargestBox': usesLargestBox,\n",
    "    'segmentsMultiple': segmentsMultiple,\n",
    "    'grouped2D': grouped2D,\n",
    "    'weight_decay': weight_decay,\n",
    "    'modelChosen': modelChosen,\n",
    "    'votingSystem': votingSystem,\n",
    "    'patience': patience,\n",
    "    'sampleStrategy': sampleStrategy\n",
    "}\n",
    "\n",
    "for key, value in testInformation.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image reading and file handling \n",
    "import pandas as pd\n",
    "import SimpleITK as sitk \n",
    "import os \n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Image agumentaitons \n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Saving History\n",
    "import pickle as pkl\n",
    "\n",
    "# Train test set spliting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# Dataset building\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Model building\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import timm # For Xception model\n",
    "\n",
    "# Evaluation metrics and Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Oversampling\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip freeze > requirements.txt\n",
    "# ! pip uninstall -y -r requirements.txt\n",
    "\n",
    "## Make a python environment\n",
    "# ! python3.8 -m venv threeDresearchPip\n",
    "\n",
    "## Download necessary packages \n",
    "# ! pip install matplotlib opencv-python scipy simpleitk pandas openpyxl scikit-learn nbconvert imblearn\n",
    "# ! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \n",
    "\n",
    "## May need to download networkx 3.1 because of older python version of torch\n",
    "# ! pip install networkx==3.1\n",
    "\n",
    "## For 3D image classification\n",
    "# ! pip install foundation-cancer-image-biomarker -qq\n",
    "# ! pip install foundation-cancer-image-biomarker\n",
    "# ! pip3 install torchio\n",
    "\n",
    "## In case pip breaks \n",
    "# ! python -m ensurepip --upgrade\n",
    "\n",
    "## Check python version and packages\n",
    "# ! python --version\n",
    "# ! pip3 freeze > research3D.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes are all the same?  True\n",
      "dataset shape:\n",
      "89 (6, 118, 118)\n"
     ]
    }
   ],
   "source": [
    "# import sys, importlib\n",
    "# importlib.reload(sys.modules['ipynb.fs.full.twoClassClassificaitonMethods'])\n",
    "# from ipynb.fs.full.twoClassClassificaitonMethods import *\n",
    "# importlib.reload(sys.modules['twoClassClassificaitonMethods'])\n",
    "from twoClassClassificationMethods import *\n",
    "\n",
    "## Instantiate the values of the model\n",
    "# python testSamples2-8.py -batchSize=8 -epochs=100 -lr=0.001 -evalDetailLine=\"majourity voting on new data\" -hasBackground=f -usesLargestBox=f -segmentsMultiple=12 -dropoutRate=0.2 -grouped2D=t -modelChosen='Small2D' -votingSystem='majority'\n",
    "\n",
    "# Set random seeds\n",
    "randomSeed = 42\n",
    "seed_everything(randomSeed)\n",
    "\n",
    "## LOAD THE DATA\n",
    "## ==============================================================================================================\n",
    "name = f'preprocessCombinations/hasBackground={hasBackground}-usesLargestBox={usesLargestBox}-segmentsMultiple={segmentsMultiple}-size=(119,119)'\n",
    "\n",
    "def loadFromPickle(name):\n",
    "    with open(f'{name}.pkl', 'rb') as fp:\n",
    "        data = pkl.load(fp)\n",
    "    return data\n",
    "\n",
    "dataset = loadFromPickle(name)\n",
    "# print('readingTemp Shape:', readingTemp.shape)    \n",
    "\n",
    "def getDatasetShape(data):\n",
    "    imageSize = data[list(data.keys())[0]]['images'].shape\n",
    "    return [len(data), imageSize[0],imageSize[1],imageSize[2]]\n",
    "\n",
    "def checkShapesConsistent(data):\n",
    "    keys = list(data.keys())\n",
    "    size = data[keys[0]]['images'].shape\n",
    "    for i in range(len(data)):    \n",
    "        if size != data[keys[i]]['images'].shape:\n",
    "            print(f\"Error in shape at index {i} with shape {data[keys[i]]['images'].shape}\")\n",
    "            return False , size\n",
    "    return True , size\n",
    "\n",
    "consitencyCheck, instanceSize = checkShapesConsistent(dataset)\n",
    "print('Sizes are all the same? ', consitencyCheck)\n",
    "assert consitencyCheck\n",
    "print(f'dataset shape:')\n",
    "print(len(dataset), instanceSize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous difference 33\n",
      "[61, 12, 2, 69, 28, 26, 22, 13, 84, 11, 64, 75, 53, 10, 57, 43, 4, 86, 71, 21, 77, 51, 58, 72, 54, 19, 70, 42, 88, 44, 29, 76, 40]\n",
      "33\n",
      "New difference after undersampling 0\n"
     ]
    }
   ],
   "source": [
    "## SPLIT THE DATA\n",
    "if sampleStrategy == 'underSampling':\n",
    "    dataset = underSampleData(dataset)\n",
    "    \n",
    "# if sampleStrategy == 'overSampling':\n",
    "#     trainData = overSampleData(trainFolders)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generateKFoldsValidation(testInformation, dataset, dataframe, k=2, trainingTransform=None):\n",
    "    \n",
    "#     testInformation = {\n",
    "#     'testName' : testName,\n",
    "#     'batchSize': batchSize,\n",
    "#     'numOfEpochs': numOfEpochs,\n",
    "#     'evalDetailLine': evalDetailLine,\n",
    "#     'learningRate': learningRate,\n",
    "#     'hasBackground': hasBackground,\n",
    "#     'usesLargestBox': usesLargestBox,\n",
    "#     'segmentsMultiple': segmentsMultiple,\n",
    "#     'grouped2D': grouped2D,\n",
    "#     'weight_decay': weight_decay,\n",
    "#     'modelChosen': modelChosen,\n",
    "#     'votingSystem': votingSystem,\n",
    "#     'patience': patience,\n",
    "#     'sampleStrategy': sampleStrategy\n",
    "# }\n",
    "    randomSeed = 42\n",
    "    seed_everything(randomSeed)\n",
    "\n",
    "    patients = list(dataset.keys())\n",
    "    fakeData = [-1] * len(patients)\n",
    "    stratifiedFolds = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    stratifiedFolds.get_n_splits(patients, fakeData)\n",
    "    splits = enumerate(stratifiedFolds.split(patients,fakeData))\n",
    "\n",
    "\n",
    "    accuracies = []\n",
    "    f1s = []\n",
    "    recalls = []\n",
    "    predictionSplits = []\n",
    "    precisions = []\n",
    "    rocAucs = []\n",
    "    endingEpochs = []\n",
    "\n",
    "    histories = []\n",
    "    confusion_matricies = []\n",
    "    rocCurves = []\n",
    "\n",
    "    print(f\"\\n\\n====================Begin testing for {testInformation['evalDetailLine']}====================\")\n",
    "\n",
    "    for i, (trainIndicies, testIndicies) in splits:\n",
    "        patients = list(dataset.keys())\n",
    "        trainFolders = [patients[i] for i in trainIndicies]\n",
    "        testFolders = [patients[i] for i in testIndicies]\n",
    "        valFolders = testFolders\n",
    "\n",
    "        trainData, valData, testData, training_data_transforms = convertDataToLoaders(trainFolders, valFolders, testFolders, dataset, \n",
    "                                                                                      testInformation['modelChosen'], testInformation['grouped2D'], \n",
    "                                                                                      testInformation['segmentsMultiple'], training_data_transforms = None, \n",
    "                                                                                      batchSize=testInformation['batchSize'])\n",
    "        \n",
    "\n",
    "        print(f\"\\n--------------------------------{testInformation['evalDetailLine']} -- Fold #{i+1}--------------------------------\")\n",
    "        print('Train Data:', len(trainData))\n",
    "        print('Validation Data:', len(valData))\n",
    "        print('Test Data:', len(testData))\n",
    "\n",
    "        ## Select and Train Model\n",
    "        model, criterion, scheduler, optimizer = defineModel(learningRate=testInformation['learningRate'], weight_decay=testInformation['weight_decay'], \n",
    "                                                             model = testInformation['modelChosen'])\n",
    "        model, criterion, device, history, endingEpoch = trainModel(model, criterion, scheduler, optimizer, trainData, valData, \n",
    "                                                                    patience=testInformation['patience'],numOfEpochs=testInformation['numOfEpochs'])\n",
    "\n",
    "        saveResults('Tests/'+testInformation['testName']+f'/fold-{i+1}/', model, history, training_data_transforms, saveModel=False)\n",
    "\n",
    "        #Evaluate perforamnce on test set\n",
    "        confusionMatrixDisp, rocCurveDisplay, testingMetrics = evaluateModelOnTestSet('Tests/'+testInformation['testName']+f'/fold-{i+1}/', model, testData, criterion, device, \n",
    "                                                                                      testInformation['votingSystem'], testInformation['segmentsMultiple'], \n",
    "                                                                                      saveConfusionMatrix = False, showConfusionMatrix=False,\n",
    "                                                                                      showROCCurve=False, saveROCCurve=False)\n",
    "        \n",
    "        plotTraining('Tests/'+testInformation['testName']+f'/fold-{i+1}/', '-', history, saveFigure=False, showResult=False)\n",
    "\n",
    "        # Collect metrics\n",
    "        accuracies.append(testingMetrics['Accuracy'])\n",
    "        f1s.append(testingMetrics['F1'])\n",
    "        recalls.append(testingMetrics['Recall'])\n",
    "        predictionSplits.append(testingMetrics['PredictionSplits'])\n",
    "        precisions.append(testingMetrics['Precision'])\n",
    "        rocAucs.append(testingMetrics['ROC-AUC'])\n",
    "        endingEpochs.append(endingEpoch)\n",
    "        \n",
    "        histories.append(history)\n",
    "        confusion_matricies.append(confusionMatrixDisp)\n",
    "        rocCurves.append(rocCurveDisplay)\n",
    "\n",
    "    kFoldsTestMetrics = {'Accuracy':meanConfidenceInterval(accuracies), 'F1':averageMultilabelMetricScores(f1s), 'Recall':averageMultilabelMetricScores(recalls), \n",
    "                    'PredictionSplits':averagePredictionTotals(predictionSplits), 'Precision':averageMultilabelMetricScores(precisions), \n",
    "                    'ROC-AUC':meanConfidenceInterval(rocAucs), 'endingEpochs':endingEpochs}\n",
    "    \n",
    "    # Write the test information and testvalues to files\n",
    "    print(f\"\\n--------------------------------{testInformation['evalDetailLine']} -- AVERAGES --------------------------------\")\n",
    "    writeDictionaryToTxtFile('Tests/'+testInformation['testName']+'/kFoldsTestMetrics.txt',kFoldsTestMetrics, printLine=True)\n",
    "    writeDictionaryToTxtFile('Tests/'+testInformation['testName']+'/testInformation.txt',testInformation, printLine=False)\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    # Plot training, confusion matrix, and roc curves for each fold as a single .png\n",
    "    plotConfusionMatricies('Tests/'+testInformation['testName'], testInformation['testName'], confusion_matricies)\n",
    "    plotROCCurves('Tests/'+testInformation['testName'], testInformation['testName'], rocCurves)\n",
    "    plotTrainingPerformances('Tests/'+testInformation['testName'], testInformation['testName'], histories, saveFigure=True, showResult=True)\n",
    "\n",
    "    appendMetricsToXLSX(testInformation['evalDetailLine'], testInformation['testName'], kFoldsTestMetrics, dataframe)\n",
    "\n",
    "    #Make copies of the two scripts\n",
    "    for filename in os.listdir():\n",
    "        # Check if the file ends with .py\n",
    "        if filename.endswith('.py'):\n",
    "            # Copy the .py file\n",
    "            shutil.copy(filename, 'Tests/'+testInformation['testName']+'/'+filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['TestName','RunData','PredictionSplits','Accuracy','F1Average','RecallAverage','PrecisionAverage','ROC-AUC','EndingEpoch','AccuracyData','F1Data','RecallData','PrecisionData','ROC-AUCData']\n",
    "dataframePath='testResultsNew.xlsx'\n",
    "sheetName = 'KFolds'\n",
    "dataframe = pd.read_excel(dataframePath, sheetName,header=None, names=columns)\n",
    "#addEvalDetailToModel(testInformation['evalDetailLine'],dataframe)\n",
    "generateKFoldsValidation(testInformation, dataset, dataframe, k=5, trainingTransform=None) \n",
    "dataframe.to_excel(dataframePath, sheetName, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def generateKFoldsValidation(identifier,identifierValue, modelInformation, grouped2D, croppedSegmentsList, recistCriteria, cases, k=5,trainingTransform=None):\n",
    "\n",
    "#     # Set the random seed for reproducibility\n",
    "#     random.seed(0)\n",
    "#     torch.manual_seed(0) \n",
    "    \n",
    "#     #Keep history of values\n",
    "#     confusion_matricies = []\n",
    "#     histories = []\n",
    "#     accuracies = []\n",
    "#     f1s = []\n",
    "#     recalls = []\n",
    "#     predictionSplits = []\n",
    "#     endingEpochs = []\n",
    "\n",
    "#     print(recistCriteria)\n",
    "#     ## =======================\n",
    "#     ## For undersampling the 0 class\n",
    "#     differenceIn0sTo1s = recistCriteria.count(0) - recistCriteria.count(1)\n",
    "#     print('previous difference', differenceIn0sTo1s)\n",
    "#     indiesToConsiderDropping = []\n",
    "#     for i in range(len(recistCriteria)):\n",
    "#         if recistCriteria[i] == 0:\n",
    "#             indiesToConsiderDropping.append(i)\n",
    "    \n",
    "#     randomIndicies = random.sample(indiesToConsiderDropping, differenceIn0sTo1s)\n",
    "#     croppedSegmentsList = np.delete(croppedSegmentsList,randomIndicies, axis=0)\n",
    "#     recistCriteria = np.delete(recistCriteria,randomIndicies, axis=0).tolist()\n",
    "#     cases = np.delete(cases,randomIndicies, axis=0).tolist()\n",
    "\n",
    "#     differenceIn0sTo1s = recistCriteria.count(0) - recistCriteria.count(1)\n",
    "#     print('New difference after undersampling', differenceIn0sTo1s)\n",
    "\n",
    "#     ## =======================\n",
    "\n",
    "    \n",
    "#     if grouped2D: #if >100 then we are doing groupings of 2D images\n",
    "#         stratifiedGroupFolds = StratifiedGroupKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "#         stratifiedGroupFolds.get_n_splits(croppedSegmentsList, recistCriteria)\n",
    "#         splits = enumerate(stratifiedGroupFolds.split(croppedSegmentsList, recistCriteria, cases))\n",
    "        \n",
    "#     else:\n",
    "#         stratifiedFolds = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "#         stratifiedFolds.get_n_splits(croppedSegmentsList, recistCriteria)\n",
    "#         splits = enumerate(stratifiedFolds.split(croppedSegmentsList, recistCriteria))\n",
    "        \n",
    "#     for i, (train_index, test_index) in splits:\n",
    "        \n",
    "#         #Set the name of the test\n",
    "#         testName = f'{identifier}-{identifierValue}'\n",
    "        \n",
    "#         testPathName = 'Tests/'+testName+f'/foldn{i+1}'\n",
    "#         print(f'{identifier}: foldn{i+1} RUN\\n=========================================')\n",
    "#         xTest, yTest = [croppedSegmentsList[i] for i in test_index], [recistCriteria[i] for i in test_index]\n",
    "#         xTrain, yTrain = [croppedSegmentsList[i] for i in train_index], [recistCriteria[i] for i in train_index]\n",
    "#         xVal, yVal = xTest, yTest # Set the validation set to the same as the testing set\n",
    "#         #xVal, yVal = [croppedSegmentsList[i] for i in train_index[:len(xTest)]], [recistCriteria[i] for i in train_index[:len(yTest)]]\n",
    "        \n",
    "\n",
    "#         # ## Working with Numpy arrays\n",
    "#         xTrain = np.array(xTrain) \n",
    "#         xTest = np.array(xTest)\n",
    "#         xVal = np.array(xVal)\n",
    "#         yTrain = np.array(yTrain)\n",
    "#         yVal = np.array(yVal)\n",
    "#         yTest = np.array(yTest)\n",
    "\n",
    "#         ## ==============================================================\n",
    "#         ## Using SMOTE\n",
    "#         # smote = SMOTE(random_state=42)\n",
    "#         # if len(xTrain.shape)==3:\n",
    "#         #     oneDShape = xTrain[0].shape[0]*xTrain[0].shape[1]\n",
    "            \n",
    "#         # else:\n",
    "#         #     print('xTrain shape',xTrain.shape)\n",
    "#         #     oneDShape = xTrain[0].shape[0]*xTrain[0].shape[1]*xTrain[0].shape[2]\n",
    "\n",
    "#         # singleShape = xTrain[0].shape\n",
    "\n",
    "#         # print('xTrain reshape',xTrain.reshape(xTrain.shape[0],oneDShape).shape)\n",
    "#         # xTrainSmote, yTrain = smote.fit_resample(xTrain.reshape(xTrain.shape[0],oneDShape), yTrain)\n",
    "#         # if len(xTrain.shape)==3:\n",
    "#         #     xTrain = xTrainSmote.reshape(xTrainSmote.shape[0], xTrain[0].shape[0],xTrain[0].shape[1])\n",
    "#         # else:\n",
    "#         #     xTrain = xTrainSmote.reshape(xTrainSmote.shape[0], xTrain[0].shape[0],xTrain[0].shape[1],xTrain[0].shape[2])\n",
    "\n",
    "#         # print('xTrain after Smote', xTrain.shape)\n",
    "#         # print('yTrain after Smote', yTrain.shape)\n",
    "#         # from collections import Counter\n",
    "#         # counter  = Counter(yTest)\n",
    "#         # print('Splits for test Fold',sorted(counter.items()))\n",
    "\n",
    "#         ## ==============================================================\n",
    "        \n",
    "#         # May or may not need this, def not needed for grouped2D=True\n",
    "#         # xTrain = np.expand_dims(xTrain,axis=-1)\n",
    "#         # xVal = np.expand_dims(xVal,axis=-1)\n",
    "#         # xTest = np.expand_dims(xTest,axis=-1)\n",
    "\n",
    "#         print('xTrain', xTrain.shape)\n",
    "#         print('xVal', xVal.shape)\n",
    "#         print('xTest', xTest.shape)\n",
    "\n",
    "#         ## Get and save results for each fold        \n",
    "#         confusionMatrix, history, accuracy, f1, recall, predictsTotal, endingEpoch = runModelFullStack(testPathName, testName, xTrain, yTrain, xVal, yVal, xTest, yTest, trainingTransform=trainingTransform, modelInformation=modelInformation) \n",
    "\n",
    "#         confusion_matricies.append(confusionMatrix)\n",
    "#         histories.append(history)\n",
    "#         accuracies.append(accuracy)\n",
    "#         f1s.append(f1)\n",
    "#         recalls.append(recall)\n",
    "#         predictionSplits.append(predictsTotal)\n",
    "#         endingEpochs.append(endingEpoch)\n",
    "#         print('\\n\\n')\n",
    "\n",
    "#     #assert False\n",
    "#     # Calculate the average of the metrics for the kfolds of this transformation and save it\n",
    "#     kFoldsTestMetrics = {'Prediction averages': averagePredictionTotals(predictionSplits), 'Accuracy':meanConfidenceInterval(accuracies), 'F1 Score':averageMultilabelMetricScores(f1s), 'Recall':averageMultilabelMetricScores(recalls)}\n",
    "#     file = open('Tests/'+testName+'/kFoldsTestMetrics.txt','w')\n",
    "#     for key, value in kFoldsTestMetrics.items():\n",
    "#         file.write(f'{key}: {value}\\n')\n",
    "#         print(f'{key}: {value}')\n",
    "#     file.close()\n",
    "\n",
    "#     # Plot training and confusion matrix for each fold as a single .png\n",
    "#     plotConfusionMatricies('Tests/'+testName, testName, confusion_matricies)\n",
    "#     plotTrainingPerformances('Tests/'+testName, testName, histories, saveFigure=True, showResult=True)\n",
    "\n",
    "#     # Append results to the xlsx file\n",
    "#     appendMetricsToXLSX(testPathName, trainingTransform, meanConfidenceInterval(accuracies), averageMultilabelMetricScores(f1s), averageMultilabelMetricScores(recalls), averagePredictionTotals(predictionSplits), endingEpochs, modelInformation, dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Make all transforms that I am going to test:\n",
    "# transformsTested = {\n",
    "#     #\"0\":None\n",
    "#     #\"20\":generateTransform(RandomRotationValue=20, RandomElaticTransform=[20.0,2.0], brightnessConstant=20, contrastConstant=20, kernelSize=3, sigmaRange=(0.001,0.4)),\n",
    "#     # \"40\":generateTransform(RandomRotationValue=40, RandomElaticTransform=[40.0,4.0], brightnessConstant=40, contrastConstant=40, kernelSize=3, sigmaRange=(0.001,0.8)),\n",
    "#     # \"60\":generateTransform(RandomRotationValue=60, RandomElaticTransform=[60.0,6.0], brightnessConstant=60, contrastConstant=60, kernelSize=3, sigmaRange=(0.001,1.2)),\n",
    "#     # \"80\":generateTransform(RandomRotationValue=80, RandomElaticTransform=[80.0,8.0], brightnessConstant=80, contrastConstant=80, kernelSize=3, sigmaRange=(0.001,1.6)),\n",
    "#     #\"100\":generateTransform(RandomRotationValue=100, RandomElaticTransform=[100.0,10.0], brightnessConstant=100, contrastConstant=100, kernelSize=3, sigmaRange=(0.001,2.0)),\n",
    "#     \"defaults+50%\":generateTransform(RandomRotationValue=50, RandomElaticTransform=[50.0,5.0], brightnessConstant=50.0, contrastConstant=50.0, kernelSize=3, sigmaRange=(0.1,2.0))    \n",
    "# }\n",
    "\n",
    "# ## Open the dataframe and add the evaluation details\n",
    "# columns = ['name','numOfEpochs','batchSize','learningRate','dropoutRate','weight_decay', 'commandRan','RandomRotation','ElasticTransform','Brightness','Contrast','GaussianBlur','RandomHorizontalFlip','RandomVerticalFlip','PredictionAverage',\n",
    "#             'AccuracyAverage','F1Average', 'RecallAverage','AccuracySTD','F1STD','RecallSTD','AccuracyData','F1Data','RecallData', 'EndingEpoch']\n",
    "\n",
    "\n",
    "# # Generate the command ran for the test \n",
    "# commandRan = 'python'\n",
    "# for details in sys.argv:\n",
    "#     print('details',details)\n",
    "#     stringArgs = ['evalDetailLine','modelChosen','votingSystem']\n",
    "#     if details in stringArgs:\n",
    "#         detailArray = details.split('=') \n",
    "#         details = f'{detailArray[0]}=\\'{detailArray[1]}\\''\n",
    "#     commandRan += f' {details}'   \n",
    "# print(commandRan)\n",
    "\n",
    "# #Define patience\n",
    "# patience = 10\n",
    "\n",
    "# modelInformation = { 'learningRate': learningRate, 'dropoutRate': dropoutRate, 'batchSize': batchSize, 'numOfEpochs':numOfEpochs, 'weight_decay':weight_decay, 'commandRan': commandRan, 'model': modelChosen, 'patience':patience, 'votingSystem': votingSystem}\n",
    "# # modelInformation = { 'learningRate': learningRate, 'dropoutRate': dropoutRate, 'batchSize': batchSize, 'numOfEpochs':1, 'weight_decay':weight_decay, 'commandRan': commandRan, 'model': modelChosen, 'votingSystem': votingSystem}\n",
    "\n",
    "# # Run the tests\n",
    "# for key, value in transformsTested.items():\n",
    "#     generateKFoldsValidation(evalDetailLine,\"-\", croppedSegmentsList=croppedSegmentsList, recistCriteria=recistCriteria, cases=cases, k=5,trainingTransform=value, modelInformation = modelInformation, grouped2D=grouped2D)\n",
    "\n",
    "# dataframe.to_excel(dataframePath, index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
