{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to python script, remember to delete/comment the next line in the actual file\n",
    "# ! jupyter nbconvert --to python imageSegmentation2Classes.ipynb --output testSamples4-8-inceptionv3-finetune.py\n",
    "\n",
    "# # Run the notebook in Simpson GPU server\n",
    "# CUDA_VISIBLE_DEVICES=0 python testSamples2-8.py -batchSize=16 -epochs=100 -lr=0.001 -evalDetailLine=\"majourity voting on smote with 2 clases\" -hasBackground=f -usesLargestBox=f -segmentsMultiple=12 -dropoutRate=0.2 -grouped2D=t -modelChosen='Small2D' -votingSystem='majority'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image reading and file handling \n",
    "import pandas as pd\n",
    "import SimpleITK as sitk \n",
    "import os \n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Image agumentaitons \n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Saving History\n",
    "import pickle\n",
    "\n",
    "# Train test set spliting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# Dataset building\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Model building\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import timm # For Xception model\n",
    "\n",
    "# Evaluation metrics and Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from fmcib.models import fmcib_model\n",
    "# # fmcib_model()\n",
    "\n",
    "# Oversampling\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install foundation-cancer-image-biomarker\n",
    "# ! d:\\simpsonlab\\threedresearchpip\\scripts\\python.exe -m pip install foundation-cancer-image-biomarker\n",
    "# ! python -m ensurepip --upgrade\n",
    "# ! python -m pip install --upgrade pip setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2800.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\SimpsonLab\\threeDresearchPip\\Scripts\\pip3.exe\\__main__.py\", line 4, in <module>\n",
      "ModuleNotFoundError: No module named 'pip'\n"
     ]
    }
   ],
   "source": [
    "# ! pip freeze > requirements.txt\n",
    "# ! pip uninstall -y -r requirements.txt\n",
    "\n",
    "## Make a python environment\n",
    "# ! python3.8 -m venv threeDresearchPip\n",
    "\n",
    "## Download necessary packages \n",
    "# ! pip install matplotlib opencv-python scipy simpleitk pandas openpyxl scikit-learn nbconvert imblearn\n",
    "# ! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \n",
    "\n",
    "## May need to download networkx 3.1 because of older python version of torch\n",
    "# ! pip install networkx==3.1\n",
    "\n",
    "## For 3D image classification\n",
    "# ! pip install foundation-cancer-image-biomarker -qq\n",
    "# ! pip3 install foundation-cancer-image-biomarker\n",
    "# ! pip3 install torchio\n",
    "\n",
    "## In case pip breaks \n",
    "# ! python -m ensurepip --upgrade\n",
    "\n",
    "## Check python version and packages\n",
    "# ! python --version\n",
    "# ! pip3 freeze > research3D.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data from the .xsxl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients: 89\n",
      "Slice range: 6-29\n",
      "Distribution of classes previously:\n",
      "[(1, 21), (2, 40), (3, 28)]\n",
      "Distribution of classes after combining progressive disease with stable disease:\n",
      "[(1, 61), (2, 28)]\n",
      "Make recistCriteria 0 and 1: [0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "columns = ['TAPS_CaseIDs_PreNAT','RECIST_PostNAT', 'Slice_Thickness']\n",
    "data = pd.read_excel('PDAC-Response_working.xlsx', header=None,names = columns)\n",
    "data.drop(0, inplace=True) # Remove the header row\n",
    "data=data.sort_values(by=['TAPS_CaseIDs_PreNAT'])\n",
    "data.drop(data[data['Slice_Thickness'] < 6].index, inplace = True)\n",
    "\n",
    "\n",
    "# # Get the entire datasheet\n",
    "cases = list(data['TAPS_CaseIDs_PreNAT'])\n",
    "recistCriteria = list(data['RECIST_PostNAT'])\n",
    "\n",
    "sliceThickness = list(data['Slice_Thickness'])\n",
    "sliceThickness.sort()\n",
    "print(f'Number of patients: {len(sliceThickness)}\\nSlice range: {min(sliceThickness)}-{max(sliceThickness)}')\n",
    "\n",
    "\n",
    "print(f'Distribution of classes previously:')\n",
    "counter = Counter(recistCriteria)\n",
    "print(sorted(counter.items()))\n",
    "\n",
    "# recistNames = [ 'Progressive \\nDisease','Stable \\nDisease','Partial \\nResponse'] \n",
    "# names = ['Not evaluated/\\nNot recorded', 'Progressive \\nDisease','Stable \\nDisease','Partial \\nResponse','Complete \\nResponse'] \n",
    "\n",
    "## Combine the progressive disease and stable disease\n",
    "# Dataset does not have 0 or else cases\n",
    "for i in range(len(recistCriteria)):\n",
    "    if recistCriteria[i] == 0: # Not evaluated\n",
    "        recistCriteria[i] = 0\n",
    "    elif recistCriteria[i] == 1: # Progressive disease\n",
    "        recistCriteria[i] = 1\n",
    "    elif recistCriteria[i] == 2: # Stable disease\n",
    "        recistCriteria[i] = 1\n",
    "    elif recistCriteria[i] == 3: # Partial Response\n",
    "        recistCriteria[i] = 2\n",
    "    elif recistCriteria[i] == 4: # Complete Response\n",
    "        recistCriteria[i] = 2 \n",
    "    else: # Other\n",
    "        recistCriteria[i] = 0    \n",
    "\n",
    "print(f'Distribution of classes after combining progressive disease with stable disease:')\n",
    "counter = Counter(recistCriteria)\n",
    "print(sorted(counter.items()))\n",
    "\n",
    "recistCriteria = [i-1 for i in recistCriteria] # Make the classes 0, 1\n",
    "print('Make recistCriteria 0 and 1:',recistCriteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying segments\n",
    "#==========================================================================================\n",
    "\n",
    "def displayCroppedSegmentations(croppedSegment):\n",
    "    print(f'CroppedSegment shape: {croppedSegment.shape}')\n",
    "    # Display the segmented image slices \n",
    "\n",
    "    columnLen = 10\n",
    "    rowLen = max(2,croppedSegment.shape[0] // columnLen + 1) \n",
    "    figure,axis = plt.subplots( rowLen, columnLen, figsize=(10, 10))\n",
    "    \n",
    "    rowIdx = 0\n",
    "    for idx in range(croppedSegment.shape[0]):        \n",
    "        if idx%columnLen == 0 and idx>0:\n",
    "            rowIdx += 1        \n",
    "        # axis[rowIdx][idx%columnLen].imshow(croppedSegment[idx,:,:] , cmap=\"gray\", vmin = 40-(350)/2, vmax=40+(350)/2)\n",
    "        axis[rowIdx][idx%columnLen].imshow(croppedSegment[idx,:,:] , cmap=\"gray\")\n",
    "\n",
    "        axis[rowIdx][idx%columnLen].axis('off')\n",
    "\n",
    "    # Turn off the axis of the rest of the subplots\n",
    "    for i in range(idx+1, rowLen*columnLen):\n",
    "        if i%columnLen == 0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][i%columnLen].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def displayOverlayedSegmentations(segmentedSlices, augmented_whole, augmented_segment):\n",
    "    # Display the segmented image slices \n",
    "    columnLen = 10\n",
    "    rowLen = max(2,len(segmentedSlices) // columnLen + 1) \n",
    "    figure,axis = plt.subplots( rowLen, columnLen, figsize=(10, 10))\n",
    "    rowIdx = 0\n",
    "    for idx in range(len(segmentedSlices)):        \n",
    "        if idx%columnLen == 0 and idx>0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][idx%columnLen].imshow(augmented_whole[segmentedSlices[idx],:,:], cmap=\"gray\")\n",
    "        axis[rowIdx][idx%columnLen].imshow(augmented_segment[segmentedSlices[idx],:,:], cmap=\"Blues\", alpha=0.75)\n",
    "        axis[rowIdx][idx%columnLen].axis('off')\n",
    "\n",
    "    # Turn off the axis of the rest of the subplots\n",
    "    for i in range(idx+1, rowLen*columnLen):\n",
    "        if i%columnLen == 0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][i%columnLen].axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform preprocessing on multiple images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current System: d:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\ipykernel_launcher.py\n",
      "BatchSize: 8, Epochs: 100, Learning Rate: 0.001, Eval Detail Line: , Has Background: True, Uses Largest Box: True, Segments Multiple: 6,       Dropout Rate: 0.2, Grouped2D: True, weightDecay: 0.01, modelChosen: SmallFoundation, votignSystem: majourity\n"
     ]
    }
   ],
   "source": [
    "#ADD argparser\n",
    "import argparse\n",
    "import sys\n",
    "print('Current System:',sys.argv[0])\n",
    "\n",
    "\n",
    "# python testSamples2-8.py -batchSize=8 -epochs=100 -lr=0.001 -evalDetailLine=\"majourity voting on new data\" -hasBackground=f -usesLargestBox=f -segmentsMultiple=12 -dropoutRate=0.2 -grouped2D=t -modelChosen='Small2D' -votingSystem='majority'\n",
    "\n",
    "\n",
    "#Check if we are using a notebook or not\n",
    "if 'ipykernel_launcher' in sys.argv[0]:\n",
    "    batchSize = 8\n",
    "    numOfEpochs = 100\n",
    "    evalDetailLine = \"\"\n",
    "    learningRate = 0.001\n",
    "    hasBackground = True\n",
    "    usesLargestBox = True\n",
    "    segmentsMultiple = 6\n",
    "    dropoutRate = 0.2\n",
    "    grouped2D = True\n",
    "    weight_decay = 0.01\n",
    "    modelChosen = 'SmallFoundation' #Large2D, Small2D\n",
    "    votingSystem = 'majourity'\n",
    "\n",
    "else:\n",
    "    parser = argparse.ArgumentParser(description=\"Model information\")\n",
    "    parser.add_argument('-batchSize', type=int, help='batch size', default=8)\n",
    "    parser.add_argument('-epochs', type=int, help='Number of Epochs', default=100)\n",
    "    parser.add_argument('-lr', type=float, help='Learning Rate', default=0.001)\n",
    "    parser.add_argument('-evalDetailLine', type=str, help='Details of the evaluation', default='')\n",
    "    parser.add_argument('-hasBackground', type=str, help='Whether to use the background (t to, f to not)', default='f')\n",
    "    parser.add_argument('-usesLargestBox', type=str, help='Where to use the size of the largest box (t) or independent tumor boxes (f)', default='t')\n",
    "    parser.add_argument('-segmentsMultiple', type=int, help='Segments a # of slices, 1 by default', default=1)\n",
    "    parser.add_argument('-dropoutRate', type=float, help='Dropout rate for the model', default=0.2)\n",
    "    parser.add_argument('-grouped2D', type=str, help='Grouping the 3D scans as individual 2D images', default='f')\n",
    "    parser.add_argument('-weightDecay', type=float, help='Weight Decay for the model', default=0.01)\n",
    "    parser.add_argument('-modelChosen', type=str, help='Selected Model', default='Large2D')\n",
    "    parser.add_argument('-votingSystem', type=str, help='Either \\'majority\\' voting, \\'average\\' voting, or \\'singleLargest\\' ', default='majority')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    batchSize = args.batchSize\n",
    "    numOfEpochs = args.epochs\n",
    "    evalDetailLine = args.evalDetailLine\n",
    "    learningRate = args.lr\n",
    "    hasBackground = True if args.hasBackground=='t' else False\n",
    "    usesLargestBox = True if args.usesLargestBox=='t' else False\n",
    "    segmentsMultiple = args.segmentsMultiple\n",
    "    dropoutRate = args.dropoutRate\n",
    "    grouped2D = True if args.grouped2D=='t' else False\n",
    "    weight_decay = args.weightDecay\n",
    "    modelChosen = args.modelChosen\n",
    "    votingSystem = args.votingSystem\n",
    "\n",
    "\n",
    "print(f'BatchSize: {batchSize}, Epochs: {numOfEpochs}, Learning Rate: {learningRate}, Eval Detail Line: {evalDetailLine}, Has Background: {hasBackground}, Uses Largest Box: {usesLargestBox}, Segments Multiple: {segmentsMultiple}, \\\n",
    "      Dropout Rate: {dropoutRate}, Grouped2D: {grouped2D}, weightDecay: {weight_decay}, modelChosen: {modelChosen}, votignSystem: {votingSystem}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "croppedSegmentsList Shape: (89, 6, 299, 299)\n",
      "Single item Shape: (6, 299, 299)\n"
     ]
    }
   ],
   "source": [
    "#Get a saved copy of the dataset\n",
    "name = f'hasBackground={hasBackground}-usesLargestBox={usesLargestBox}-segmentsMultiple={segmentsMultiple}-size=(299,299)'        \n",
    "# name = f'hasBackground=True-usesLargestBox=False-segmentsMultiple=1'\n",
    "croppedSegmentsList = np.load(f'preprocessCombinations/{name}.npy')\n",
    "    \n",
    "print('croppedSegmentsList Shape:', croppedSegmentsList.shape)   \n",
    "print('Single item Shape:', croppedSegmentsList[0].shape)   \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure,axis = plt.subplots(1,len(croppedSegmentsList),figsize=(200,100))\n",
    "# for idx in range(len(croppedSegmentsList)):        \n",
    "#     axis[idx].imshow(croppedSegmentsList[idx], cmap=\"gray\")\n",
    "#     axis[idx].axis('off')\n",
    "# plt.savefig('padding10.png')\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting information about the transformations\n",
    "def generateTransform(RandomHorizontalFlipValue=0.5,RandomVerticalFlipValue=0.5, RandomRotationValue=50, RandomElaticTransform=[0,0], brightnessConstant=0, contrastConstant=0, kernelSize=3, sigmaRange=(0.1,1.0)):\n",
    "    training_data_transforms = transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomRotation(degrees=RandomRotationValue),\n",
    "        transforms.ElasticTransform(alpha=RandomElaticTransform[0], sigma=RandomElaticTransform[1]),\n",
    "        transforms.ColorJitter(brightnessConstant, contrastConstant),\n",
    "        transforms.GaussianBlur(kernel_size = kernelSize, sigma=sigmaRange),\n",
    "        transforms.RandomHorizontalFlip(p=RandomHorizontalFlipValue),\n",
    "        transforms.RandomVerticalFlip(p=RandomVerticalFlipValue),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]) \n",
    "    return training_data_transforms\n",
    "\n",
    "\n",
    "def getTransformValue(transform, desiredTranform, desiredTranformValue):\n",
    "    if transform==None or desiredTranform==None or desiredTranformValue==None:\n",
    "      return None\n",
    "    for t in transform.transforms:\n",
    "        if isinstance(t, desiredTranform):\n",
    "            return t.__getattribute__(desiredTranformValue)\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 3D images:\n",
    "# ## Working with pytorch tensors\n",
    "class PatientData(Dataset):\n",
    "    def __init__(self, image, classifications, transform=None):\n",
    "        self.data = image\n",
    "        # Convert classification to torch tensor\n",
    "        temp = []\n",
    "        for classification in classifications:\n",
    "            convert = torch.tensor(classification, dtype=torch.int64) # casting to long\n",
    "            convert = convert.type(torch.float32)\n",
    "            temp.append(convert)\n",
    "            \n",
    "        self.classification = temp\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.classification[idx]\n",
    "        \n",
    "        images = self.data[idx]\n",
    "        \n",
    "        # Save the random state before applying the image\n",
    "        state = random.getstate()\n",
    "        torch_state = torch.get_rng_state()\n",
    "        # random.seed(idx)\n",
    "        # torch.manual_seed(idx)\n",
    "\n",
    "        imagesTemp = []\n",
    "        if self.transform:\n",
    "            for image in images: \n",
    "                # Convert to RGB\n",
    "                image = Image.fromarray((image * 255).astype(np.uint16))\n",
    "                image = image.convert(\"RGB\")\n",
    "                # Set the random seed for consitent image transformations across the images\n",
    "                random.seed(state)\n",
    "                torch.set_rng_state(torch_state)\n",
    "                #Apply transformations and save result\n",
    "                image = self.transform(image)#.type(torch.float)\n",
    "                imagesTemp.append(image)\n",
    "            images = torch.stack(imagesTemp)\n",
    "        #Set the random state back to the original\n",
    "        # random.setstate(state)\n",
    "        # torch.cuda.set_rng_state(torch_state)\n",
    "\n",
    "        return images, label\n",
    "\n",
    "\n",
    "def convertDataToLoaders(xTrain, yTrain, xVal, yVal, xTest, yTest, training_data_transforms = None, batchSize=8):\n",
    "    ## Sample the data with 75% of the training set \n",
    "    # TrainBalancedSampler = WeightedRandomSampler(weightsForClasses, len(yTrain)//2+len(yTest)//4)\n",
    "    \n",
    "    # Testing data tranfrom, should be just the plain images\n",
    "    testing_data_transforms = transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]) \n",
    "\n",
    "    # Use the same default training transform as the testing transform if not specified\n",
    "    if training_data_transforms == None:\n",
    "        training_data_transforms = testing_data_transforms\n",
    "        \n",
    "    # Convert the testing sets to data loaders\n",
    "    trainingData = PatientData(xTrain, yTrain, transform=training_data_transforms)\n",
    "    trainingData = DataLoader(trainingData, batch_size=batchSize, shuffle=True)#, sampler= TrainBalancedSampler)\n",
    "\n",
    "    validationData = PatientData(xVal, yVal, transform=testing_data_transforms)\n",
    "    validationData = DataLoader(validationData, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "    testingData = PatientData(xTest, yTest, transform=testing_data_transforms)\n",
    "    testingData = DataLoader(testingData, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "    return trainingData, validationData, testingData, training_data_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For 3D images:\n",
    "\n",
    "# import torchio\n",
    "\n",
    "# # ## Working with pytorch tensors\n",
    "# class TorchDataset(Dataset):\n",
    "#     def __init__(self, images, classifications, transform=None):\n",
    "#         self.data = images\n",
    "#         temp = []\n",
    "#         for classification in classifications:\n",
    "#             convert = torch.tensor(classification, dtype=torch.int64) # casting to long\n",
    "#             convert = convert.type(torch.float32)\n",
    "#             temp.append(convert)\n",
    "            \n",
    "#         self.classification = temp\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         images = self.data[idx]\n",
    "#         # Normalize the images\n",
    "#         images = (images - np.min(images)) / (np.max(images) - np.min(images))\n",
    "\n",
    "\n",
    "#         images = torch.from_numpy(images)\n",
    "#         images = torch.FloatTensor(images)\n",
    "\n",
    "#         # images = torch.from_numpy(images)\n",
    "#         print(type(images))\n",
    "        \n",
    "\n",
    "#         label = self.classification[idx]\n",
    "#         # Convert sample to a tensor\n",
    "#         # sample = torch.tensor(sample, dtype=torch.float32).permute(2, 0, 1)  # Change shape to (C, H, W)\n",
    "\n",
    "#         if self.transform:\n",
    "#             images = self.transform(images)\n",
    "        \n",
    "#         return images, label\n",
    "    \n",
    "\n",
    "# # Set the random seed for reproducibility\n",
    "# random.seed(0)\n",
    "# torch.manual_seed(0) \n",
    "\n",
    "# ## Sample the data with 75% of the training set \n",
    "# # TrainBalancedSampler = WeightedRandomSampler(weightsForClasses, len(yTrain)//2+len(yTest)//4)\n",
    "\n",
    "# # Define data augmentation transforms\n",
    "# training_data_transforms = torchio.Compose([\n",
    "#     torchio.RandomFlip(axes=('Left','Right'), flip_probability=RandomVerticalFlipProbablility),\n",
    "#     torchio.RandomFlip(axes=('Anterior','Posterior'), flip_probability=RandomHorizontalFlipProbablility),\n",
    "#     torchio.RandomNoise(std=(0, 0.1)),\n",
    "#     torchio.RandomBlur(std=(0, 1))\n",
    "# ]) \n",
    "# # testing_data_transforms = transforms.Compose([\n",
    "# #     transforms.ToPILImage(),\n",
    "# #     transforms.ToTensor()\n",
    "# # ]) \n",
    "\n",
    "\n",
    "# # Convert the testing sets to data loaders\n",
    "# trainingData = TorchDataset(xTrain, yTrain, transform=training_data_transforms)\n",
    "\n",
    "# trainingData = DataLoader(trainingData, batch_size=batchSize, shuffle=False)#, sampler= TrainBalancedSampler)\n",
    "\n",
    "# testingData = TorchDataset(xTest, yTest, transform=None)# testing_data_transforms)\n",
    "# testingData = DataLoader(testingData, batch_size=batchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Small2D(torch.nn.Module):\n",
    "    def __init__(self, dropoutRate=0.2):\n",
    "        super(Small2D, self).__init__()\n",
    "\n",
    "        #Resnet50 as first layer\n",
    "        self.model = models.inception_v3(pretrained=True)\n",
    "        # self.resNet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Modify the first convolutional layer to accept single-channel input\n",
    "        # self.resNet50.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "\n",
    "        # Freeze all layers of the resNet\n",
    "        #for param in self.resNet50.parameters():\n",
    "        #    param.requires_grad = False\n",
    "        \n",
    "        \n",
    "        # Modify the final fully connected layer\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.model.training:\n",
    "          x = self.model(x)[0]\n",
    "        else:\n",
    "          x = self.model(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x \n",
    "    \n",
    "class Large2D(torch.nn.Module):\n",
    "    def __init__(self, dropoutRate=0.2):\n",
    "        super(Large2D, self).__init__()\n",
    "\n",
    "        #Resnet50 as first layer\n",
    "        self.model = models.inception_v3(pretrained=True)\n",
    "\n",
    "                \n",
    "        # Hidden layers with batchnorms\n",
    "        self.batchNormalization0 = nn.BatchNorm1d(self.model.fc.in_features)\n",
    "        self.hiddenLayer1 = nn.Linear(self.model.fc.in_features, 528)\n",
    "        self.batchNormalization1 = nn.BatchNorm1d(528)\n",
    "        self.hiddenLayer2 = nn.Linear(528, 128)\n",
    "        self.batchNormalization2 = nn.BatchNorm1d(128)\n",
    "        self.hiddenLayer3 = nn.Linear(128, 64)\n",
    "        self.batchNormalization3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        #Other layers\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Into Model\n",
    "        if self.model.training:\n",
    "            x = self.model(x)[0]\n",
    "        else:\n",
    "            x = self.model(x)\n",
    "    \n",
    "        x = self.batchNormalization0(x) if x.size(dim=0)>1 else x\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # Into hidden layer1\n",
    "        x = self.hiddenLayer1(x) \n",
    "        x = self.batchNormalization1(x) if x.size(dim=0)>1 else x\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # Into hidden layer2\n",
    "        x = self.hiddenLayer2(x)\n",
    "        x = self.batchNormalization2(x) if x.size(dim=0)>1 else x\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # Into hidden layer3\n",
    "        x = self.hiddenLayer3(x)\n",
    "        x = self.batchNormalization3(x) if x.size(dim=0)>1 else x\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # Output layer  \n",
    "        x = self.outputLayer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def defineModel(dropoutRate=0.2,learningRate=0.001, weight_decay=0.01, model = 'Small2D'):\n",
    "    if model == 'Small2D':\n",
    "        model = Small2D(dropoutRate)\n",
    "    elif model == 'Large2D':\n",
    "        model = Large2D(dropoutRate)\n",
    "    elif model == 'SmallFoundation':\n",
    "        model = SmallFoundation(dropoutRate)\n",
    "    \n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learningRate, weight_decay=weight_decay)\n",
    "\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    return model, criterion, scheduler, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fmcib_model()\n",
    "\n",
    "print(nn.Sequential(*list(model.children())[:-1]))\n",
    "# print(model.trunk.avgpool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SmallFoundation(torch.nn.Module):\n",
    "    def __init__(self, dropoutRate=0.2):\n",
    "        super(SmallFoundation, self).__init__()\n",
    "\n",
    "        # Foundation model as first layer\n",
    "        self.model = fmcib_model()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.output = nn.Linear(4096, 1) #Came from the last layer of the model that is not AdaptiveAvgPool3d(output_size=(1, 1, 1)) or the Identity layer\n",
    "        \n",
    "        # Modify the first convolutional layer to accept single-channel input\n",
    "        # self.model.trunk.conv1 = nn.Conv3d(1, 128, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "        \n",
    "        # # Freeze all layers of the foundation model\n",
    "        # for param in self.model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "                \n",
    "        # Hidden layers with batchnorms\n",
    "        # self.batchNormalization0 = nn.BatchNorm1d(self.model.fc.out_features)\n",
    "        # self.hiddenLayer1 = nn.Linear(self.model.fc.out_features, 528)\n",
    "        # self.batchNormalization1 = nn.BatchNorm1d(528)\n",
    "        # self.hiddenLayer2 = nn.Linear(528, 128)\n",
    "        # self.batchNormalization2 = nn.BatchNorm1d(128)\n",
    "        # self.hiddenLayer3 = nn.Linear(128, 64)\n",
    "        # self.batchNormalization3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        # Output layer\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        #Other layers\n",
    "        # self.relu = nn.ReLU() \n",
    "        # self.dropout = nn.Dropout(dropoutRate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Into foundation model\n",
    "        x = self.model(x) \n",
    "        x = self.flatten(x)\n",
    "        x = self.output(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, scheduler, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).view(-1, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = outputs > 0.5\n",
    "        total += labels.size(0)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        correct += torch.sum(predicted == labels.data).item()\n",
    "    \n",
    "    #scheduler.step()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, minDelta=0):\n",
    "        self.patience = patience\n",
    "        self.minDelta = minDelta\n",
    "        self.counter = 0\n",
    "        self.minValLoss = float('inf')\n",
    "        \n",
    "    def earlyStoppingCheck(self, currValLoss):\n",
    "        if np.isnan(currValLoss):\n",
    "            return True\n",
    "        if currValLoss < self.minValLoss:\n",
    "            self.minValLoss = currValLoss\n",
    "            self.counter = 0\n",
    "        elif currValLoss >= self.minValLoss + self.minDelta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "# def evaluateGroupVoting(model, loader, criterion, device):\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     probabilities = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for inputs, labels in loader:\n",
    "#             inputs = inputs.to(device)\n",
    "#             labels = labels.to(device).view(-1, 1)\n",
    "\n",
    "#             outputs = model(inputs)\n",
    "\n",
    "#             predicted = outputs > 0.5\n",
    "\n",
    "#             probabilities.append(outputs.cpu().numpy())\n",
    "#             predictions.append(predicted.cpu().numpy())\n",
    "    \n",
    "#     # Ignores grouped voting\n",
    "#     if votingSystem=='singleLargest' or segmentsMultiple==1:\n",
    "#         return predictions\n",
    "\n",
    "#     ## Grouping predictions by patient\n",
    "#     ## ==============================================================\n",
    "#     all_probs = np.concatenate(probabilities, axis=0)\n",
    "\n",
    "#     # Grouping predictions by patient\n",
    "#     patient_probs = [] # The confidence of the model\n",
    "#     patient_labels = [] # the label given by the model. >=0.5 is 1, <0.5 is 0\n",
    "     \n",
    "#     ## Get the classification based from the patient based on ...\n",
    "#     for i in range(0, len(all_probs), segmentsMultiple):\n",
    "#         # Get probabilties and labels for the patient\n",
    "#         patient_prob = all_probs[i:i + segmentsMultiple]\n",
    "#         patient_pred_labels = [pred>=0.5 for pred in patient_prob]\n",
    "\n",
    "#         ## Average all confidences to get the highest probability label, used as a tie breaker\n",
    "#         patient_prob = np.mean(patient_prob) \n",
    "#         prob_label_max = patient_prob >= 0.5 \n",
    "        \n",
    "#         ## MAJOURITY VOTING\n",
    "#         ##==============================================================\n",
    "#         # Get counts for the labels \n",
    "#         if votingSystem=='majority':\n",
    "#             uniqueLabels, label_counts = np.unique(patient_pred_labels, return_counts=True)\n",
    "            \n",
    "#             majorityVote = uniqueLabels[np.argmax(label_counts)]\n",
    "#             patient_labels.append(np.array([majorityVote]))\n",
    "#         ##==============================================================\n",
    "#         ## AVERAGE\n",
    "#         ##==============================================================\n",
    "#         elif votingSystem=='average':\n",
    "#             patient_probs.append(np.array(patient_prob))\n",
    "#             patient_labels.append(np.array([prob_label_max]))\n",
    "#         ##==============================================================\n",
    "\n",
    "#     return patient_labels\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).view(-1, 1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            predicted = outputs > 0.5\n",
    "            total += labels.size(0)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            correct += torch.sum(predicted == labels.data).item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, criterion, scheduler, optimizer, trainingData, validationData, patience=10,numOfEpochs=100):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using this device:', device)\n",
    "    #Send the model to the same device that the tensors are on\n",
    "    model.to(device)\n",
    "\n",
    "    earlyStopping = EarlyStopping(patience=patience, minDelta=0)\n",
    "    train_loss, train_acc = [], []\n",
    "    val_loss, val_acc = [], []\n",
    "\n",
    "    for epoch in range(numOfEpochs):\n",
    "        #Train model\n",
    "        curTrainLoss, curTrainAcc = train(model, trainingData, criterion, scheduler, optimizer, device)    \n",
    "        print(f\"Epoch {epoch+1}/{numOfEpochs}\")\n",
    "        print(f\"Train Loss: {curTrainLoss:.4f}, Train Acc: {curTrainAcc:.4f}\")\n",
    "        #Evaluate on validation set\n",
    "        curValLoss, curValAcc, _ = evaluate(model, validationData, criterion, device)    \n",
    "        print(f\"Val Loss: {curValLoss:.4f}, Val Acc: {curValAcc:.4f}\")\n",
    "\n",
    "        #Append metrics to lists\n",
    "        train_loss.append(curTrainLoss)\n",
    "        train_acc.append(curTrainAcc)\n",
    "        val_loss.append(curValLoss)\n",
    "        val_acc.append(curValAcc)\n",
    "\n",
    "        #Check for early stopping conditions\n",
    "        if earlyStopping.earlyStoppingCheck(curValLoss):\n",
    "            print(f'Early stopping - Val loss has not decreased in {earlyStopping.patience} epochs. Terminating training at epoch {epoch+1}.')\n",
    "            break\n",
    "\n",
    "    history = {'train_loss':train_loss, 'train_acc':train_acc, 'val_loss':val_loss, 'val_acc':val_acc}\n",
    "    return model, criterion, device, history, epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_history_from_pickle(testPathName):\n",
    "#     with open(testPathName+'/history.pkl', 'rb') as fp:\n",
    "#         history = pickle.load(fp)\n",
    "#     return history\n",
    "\n",
    "# #Read history\n",
    "# history = read_history_from_pickle(testPathName)\n",
    "\n",
    "# # Load and evalaute the model\n",
    "# modelWeightPath = testPathName+'/model.pt'\n",
    "# model = ResNet50ClassificaitonModel()\n",
    "# model.load_state_dict(torch.load(modelWeightPath))\n",
    "\n",
    "#Send the model to the device used\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print('Using this device:', device)\n",
    "# #Send the model to the same device that the tensors are on\n",
    "# model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the contents of this test\n",
    "\n",
    "def saveResults(testPathName, model, history, training_data_transforms, saveModel=True):\n",
    "    os.makedirs(testPathName, exist_ok=True)\n",
    "\n",
    "    #Save history as pickle\n",
    "    with open(testPathName+'/history.pkl', 'wb') as fp:\n",
    "        pickle.dump(history, fp)\n",
    "\n",
    "    # Save weigths of model\n",
    "    if saveModel:\n",
    "        torch.save(model.state_dict(), testPathName+'/model.pt')\n",
    "\n",
    "    # Save transformations for easy access\n",
    "    f = open(testPathName + '/training_data_transforms.txt', 'w')\n",
    "            \n",
    "    for line in training_data_transforms.__str__():\n",
    "        f.write(line)\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTensorToPredictions(tensorList, dictionary):\n",
    "    \"\"\"Converts a list of tensors/ndarrays into a single list and then feeds those values into a given dictionary.\"\"\"\n",
    "    values=[]\n",
    "    if grouped2D==True:\n",
    "        for batch in tensorList:\n",
    "            batchValues = batch.tolist()        \n",
    "            values+= batchValues\n",
    "        for value in values:\n",
    "            dictionary[value]+=1\n",
    "    else:\n",
    "        values=[]#tensorList\n",
    "        for batch in tensorList:\n",
    "            batchValues = batch.tolist()      \n",
    "            values.extend(batchValues)\n",
    "        for i in range(len(values)):\n",
    "            values[i] = values[i][0]\n",
    "        print('tensorlist', tensorList)\n",
    "        print('values', values)\n",
    "        \n",
    "        print(values)\n",
    "        for value in values:\n",
    "            dictionary[value]+=1\n",
    "    return values, dictionary\n",
    "\n",
    "# def convertTensorToPredictions(tensorList, dictionary):\n",
    "#     \"\"\"Converts a list of tensors/ndarrays into a single list and then feeds those values into a given dictionary.\"\"\"\n",
    "#     values=[]\n",
    "#     for batch in tensorList:\n",
    "#         batchValues = batch.tolist()      \n",
    "#         values.extend(batchValues)\n",
    "#     for i in range(len(values)):\n",
    "#         values[i] = values[i][0]\n",
    "#     print('tensorlist', tensorList)\n",
    "#     print('values', values)\n",
    "    \n",
    "#     print(values)\n",
    "#     for value in values:\n",
    "#         dictionary[value]+=1\n",
    "#     return values, dictionary\n",
    "\n",
    "def evaluateModelOnTestSet(testPathName, model, testingData, criterion, device, saveConfusionMatrix = True, showConfusionMatrix=True):\n",
    "    _, _2, predictions = evaluate(model, testingData, criterion, device)\n",
    "\n",
    "    predicts, predictsTotal = convertTensorToPredictions(predictions, {0:0,1:0})\n",
    "\n",
    "    # Get the correct answers\n",
    "    originalLabelsTemp = []\n",
    "    ans = []\n",
    "    \n",
    "    for _, labels in testingData:\n",
    "      originalLabelsTemp.extend(labels.tolist())\n",
    "    \n",
    "    for i in range(len(originalLabelsTemp)):  \n",
    "        if i%segmentsMultiple==0:\n",
    "            ans.append(originalLabelsTemp[i] >=0.5)\n",
    "        i+=1\n",
    " \n",
    "    ansTotal = dict(zip([0,1],[ans.count(False),ans.count(True)]))\n",
    "\n",
    "    print('ans length',len(ans))\n",
    "\n",
    "    # Test metrics\n",
    "    print('---------------------------------------\\nTesting Metrics')\n",
    "    \n",
    "    accuracy = accuracy_score(ans, predicts)\n",
    "    f1 = list(f1_score(ans, predicts, average=None))  # Use 'weighted' for multiclass classification\n",
    "    recall = list(recall_score(ans, predicts, average=None))  # Use 'weighted' for multiclass classification\n",
    "\n",
    "    testingMetrics = {'Predictions split': predictsTotal, 'Answers split': ansTotal, 'Predictions': predicts, 'Answers    ': ans,  'Accuracy':accuracy, 'F1 Score':f1, 'Recall':recall}\n",
    "\n",
    "    file = open(testPathName+'/testingMetrics.txt','w')\n",
    "    for key, value in testingMetrics.items():\n",
    "        file.write(f'{key}: {value}\\n')\n",
    "    file.close()\n",
    "\n",
    "    for key, value in testingMetrics.items():\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "\n",
    "    print('---------------------------------------\\nConfusion Matrix:')\n",
    "    # Confusion Matrix\n",
    "    result = confusion_matrix(ans,predicts,normalize='pred')\n",
    "    disp = ConfusionMatrixDisplay(result)\n",
    "    \n",
    "    if saveConfusionMatrix:\n",
    "        plt.savefig(testPathName+'/confusion_matrix.png')\n",
    "    \n",
    "    if showConfusionMatrix:\n",
    "        plt.show()\n",
    "\n",
    "    plt.clf() \n",
    "    return disp, accuracy, f1, recall, predictsTotal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Performance on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTraining(testPathName, testName, history, saveFigure=True, showResult=True):\n",
    "    plt.style.use('default')\n",
    "    \n",
    "    figure, ax = plt.subplots( 1, 2, figsize=(20, 10))\n",
    "    # plt.suptitle('Accuracy', fontsize=10)\n",
    "    ax[0].set_title(\"Loss\")\n",
    "    ax[0].set_ylabel('Loss', fontsize=16)\n",
    "    ax[0].set_xlabel('Epoch', fontsize=16)\n",
    "    ax[0].plot(history['train_loss'], label='Training Loss')\n",
    "    ax[0].plot(history['val_loss'], label='Validation Loss')\n",
    "    ax[0].legend(loc='upper right')\n",
    "\n",
    "    ax[1].set_title(\"Accuracy\")\n",
    "    ax[1].set_ylabel('Accuracy', fontsize=16)\n",
    "    ax[1].set_xlabel('Epoch', fontsize=16)\n",
    "    \n",
    "    ax[1].plot(history['train_acc'], label='Training Accuracy')\n",
    "    ax[1].plot(history['val_acc'], label='Validation Accuracy')\n",
    "    ax[1].legend(loc='lower right')\n",
    "\n",
    "    if saveFigure:\n",
    "        plt.savefig(testPathName+'/training_history.png')\n",
    "    \n",
    "    if showResult:\n",
    "        plt.show()\n",
    "\n",
    "    plt.clf() \n",
    "\n",
    "    return figure\n",
    "\n",
    "\n",
    "def plotTrainingPerformances(testPathName, testName, histories, saveFigure=True, showResult=True):\n",
    "    plt.style.use('default')\n",
    "\n",
    "    figure, ax = plt.subplots( 2, len(histories), figsize=(80, 20))\n",
    "    for idx, history in enumerate(histories):\n",
    "        # plt.suptitle('Accuracy', fontsize=10)\n",
    "        ax[0][idx].set_title(\"Loss\")\n",
    "        ax[0][idx].set_ylabel('Loss', fontsize=16)\n",
    "        ax[0][idx].set_xlabel('Epoch', fontsize=16)\n",
    "        ax[0][idx].plot(history['train_loss'], label='Training Loss')\n",
    "        ax[0][idx].plot(history['val_loss'], label='Validation Loss')\n",
    "        ax[0][idx].legend(loc='upper right')\n",
    "\n",
    "        ax[1][idx].set_title(\"Accuracy\")\n",
    "        ax[1][idx].set_ylabel('Accuracy', fontsize=16)\n",
    "        ax[1][idx].set_xlabel('Epoch', fontsize=16)\n",
    "        ax[1][idx].plot(history['train_acc'], label='Training Accuracy')\n",
    "        ax[1][idx].plot(history['val_acc'], label='Validation Accuracy')\n",
    "        ax[1][idx].legend(loc='lower right')\n",
    "\n",
    "    plt.suptitle(f'{testName} \\nTrainining Performance', fontsize=30)\n",
    "\n",
    "\n",
    "    if saveFigure:\n",
    "        plt.savefig(testPathName+'/training_history.png')\n",
    "    \n",
    "    if showResult:\n",
    "        plt.show()\n",
    "\n",
    "    plt.clf() \n",
    "\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run FullStack of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotConfusionMatricies(testPathName, testName, confusion_matricies):\n",
    "    figure,axis = plt.subplots(1,len(confusion_matricies),figsize=(20, 5))\n",
    "    for idx in range(len(confusion_matricies)):        \n",
    "        confusion_matricies[idx].plot(ax=axis[idx])\n",
    "        confusion_matricies[idx].im_.colorbar.remove()\n",
    "\n",
    "    figure.suptitle(f'{testName}\\nConfusion Matricies')\n",
    "    plt.savefig(testPathName+'/confusion_matrix.png')\n",
    "    plt.show()\n",
    "    plt.clf() \n",
    "    \n",
    "def meanConfidenceInterval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "\n",
    "    min = np.min(a)\n",
    "    max = np.max(a)\n",
    "\n",
    "    return [m, h, [min, max], data]\n",
    "\n",
    "def averagePredictionTotals(predictions, numberOfTrials=5):\n",
    "    average = {0:0,1:0}\n",
    "    for prediction in predictions:\n",
    "        for key, value in prediction.items():\n",
    "            average[key] += value\n",
    "    \n",
    "    for key,value in average.items():\n",
    "        average[key] = value/numberOfTrials\n",
    "\n",
    "    return average\n",
    "\n",
    "\n",
    "def meanConfidenceInterval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "\n",
    "    min = np.min(a)\n",
    "    max = np.max(a)\n",
    "\n",
    "    return [m, h, data]\n",
    "\n",
    "def averageMultilabelMetricScores(scores, numberOfTrials=5, numberOfClasses=2):\n",
    "    dict = {0:0,1:0}\n",
    "    averages= [0]*numberOfClasses\n",
    "    for score in scores:\n",
    "        for i in range(numberOfClasses):\n",
    "            averages[i] += score[i]\n",
    "    averages = [averages[i]/numberOfTrials for i in range(numberOfClasses)]\n",
    "    \n",
    "    for key in range(numberOfClasses):\n",
    "        dict[key] = averages[key]\n",
    "\n",
    "    mean = np.mean(averages)\n",
    "    return [mean, dict, scores]\n",
    "\n",
    "#Runs all model evaluations steps\n",
    "def runModelFullStack(testPathName, testName, xTrain, yTrain, xVal, yVal, xTest, yTest, modelInformation, trainingTransform=None):\n",
    "    trainingData, validationData, testingData, training_data_transforms = convertDataToLoaders(xTrain, yTrain, xVal, yVal, xTest, yTest, training_data_transforms = trainingTransform, batchSize=modelInformation['batchSize'])\n",
    "    model, criterion, scheduler, optimizer = defineModel(dropoutRate=modelInformation['dropoutRate'], learningRate = modelInformation['learningRate'], weight_decay=modelInformation['weight_decay'], model = modelInformation['model'])\n",
    "    model, criterion, device, history, endingEpoch = trainModel(model, criterion, scheduler, optimizer, trainingData,validationData, numOfEpochs=modelInformation['numOfEpochs'], patience=modelInformation['patience'])\n",
    "    saveResults(testPathName, model, history, training_data_transforms, saveModel=False)\n",
    "    confusionMatrix, accuracy, f1, recall, predictsTotal = evaluateModelOnTestSet(testPathName, model, testingData, criterion, device, saveConfusionMatrix = True, showConfusionMatrix=False)\n",
    "    #plotTraining(testPathName, testName, history, saveFigure=True, showResult=True)\n",
    "\n",
    "    return confusionMatrix, history, accuracy, f1, recall, predictsTotal, endingEpoch+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addEvalDetailToModel(evalDetailLine, dataframe):\n",
    "    dataframe.loc[dataframe.shape[0]] = [evalDetailLine] + ['']*(len(dataframe.columns)-1)\n",
    "    \n",
    "def appendMetricsToXLSX(testPathName, trainingTransform, accuracyInformation, f1Information, recallInformation, predictsTotal, endingEpochs, modelInformation, dataframe):\n",
    "    rotation = getTransformValue(trainingTransform, desiredTranform=transforms.RandomRotation, desiredTranformValue='degrees')\n",
    "    elasticTransform = [getTransformValue(trainingTransform, desiredTranform=transforms.ElasticTransform, desiredTranformValue='alpha'), getTransformValue(trainingTransform, desiredTranform=transforms.ElasticTransform, desiredTranformValue='sigma')]\n",
    "    brightness = getTransformValue(trainingTransform, desiredTranform=transforms.ColorJitter, desiredTranformValue='brightness')\n",
    "    contrast = getTransformValue(trainingTransform, desiredTranform=transforms.ColorJitter, desiredTranformValue='contrast')\n",
    "    guassianBlur = [ getTransformValue(trainingTransform, desiredTranform=transforms.GaussianBlur, desiredTranformValue='kernel_size'), getTransformValue(trainingTransform, desiredTranform=transforms.GaussianBlur, desiredTranformValue='sigma')]\n",
    "    randomHorizontalFlip = getTransformValue(trainingTransform, desiredTranform=transforms.RandomHorizontalFlip, desiredTranformValue='p')\n",
    "    randomVerticalFlip = getTransformValue(trainingTransform, desiredTranform=transforms.RandomVerticalFlip, desiredTranformValue='p')\n",
    "    \n",
    "    exportValue = [testPathName, modelInformation['numOfEpochs'],modelInformation['batchSize'], modelInformation['learningRate'],modelInformation['dropoutRate'],modelInformation['weight_decay'],modelInformation['commandRan'], \\\n",
    "        rotation, elasticTransform, brightness, contrast, guassianBlur, randomHorizontalFlip, randomVerticalFlip, predictsTotal, \\\n",
    "            accuracyInformation[0], f1Information[0], recallInformation[0], \\\n",
    "            accuracyInformation[1], f1Information[1],  recallInformation[1], \\\n",
    "            accuracyInformation[2], f1Information[2], recallInformation[2], \\\n",
    "            endingEpochs]\n",
    "    \n",
    "    dataframe.loc[dataframe.shape[0]] = exportValue + ['']*(len(dataframe.columns)-len(exportValue))\n",
    "\n",
    "def generateKFoldsValidation(identifier,identifierValue, modelInformation, grouped2D, croppedSegmentsList, recistCriteria, cases, k=5,trainingTransform=None):\n",
    "\n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(0)\n",
    "    torch.manual_seed(0) \n",
    "    \n",
    "    #Keep history of values\n",
    "    confusion_matricies = []\n",
    "    histories = []\n",
    "    accuracies = []\n",
    "    f1s = []\n",
    "    recalls = []\n",
    "    predictionSplits = []\n",
    "    endingEpochs = []\n",
    "\n",
    "    print(recistCriteria)\n",
    "    ## =======================\n",
    "    ## For undersampling the 0 class\n",
    "    differenceIn0sTo1s = recistCriteria.count(0) - recistCriteria.count(1)\n",
    "    print('previous difference', differenceIn0sTo1s)\n",
    "    indiesToConsiderDropping = []\n",
    "    for i in range(len(recistCriteria)):\n",
    "        if recistCriteria[i] == 0:\n",
    "            indiesToConsiderDropping.append(i)\n",
    "    \n",
    "    randomIndicies = random.sample(indiesToConsiderDropping, differenceIn0sTo1s)\n",
    "    croppedSegmentsList = np.delete(croppedSegmentsList,randomIndicies, axis=0)\n",
    "    recistCriteria = np.delete(recistCriteria,randomIndicies, axis=0).tolist()\n",
    "    cases = np.delete(cases,randomIndicies, axis=0).tolist()\n",
    "\n",
    "    differenceIn0sTo1s = recistCriteria.count(0) - recistCriteria.count(1)\n",
    "    print('New difference after undersampling', differenceIn0sTo1s)\n",
    "\n",
    "    ## =======================\n",
    "\n",
    "    \n",
    "    if grouped2D: #if >100 then we are doing groupings of 2D images\n",
    "        stratifiedGroupFolds = StratifiedGroupKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        stratifiedGroupFolds.get_n_splits(croppedSegmentsList, recistCriteria)\n",
    "        splits = enumerate(stratifiedGroupFolds.split(croppedSegmentsList, recistCriteria, cases))\n",
    "        \n",
    "    else:\n",
    "        stratifiedFolds = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        stratifiedFolds.get_n_splits(croppedSegmentsList, recistCriteria)\n",
    "        splits = enumerate(stratifiedFolds.split(croppedSegmentsList, recistCriteria))\n",
    "        \n",
    "    for i, (train_index, test_index) in splits:\n",
    "        \n",
    "        #Set the name of the test\n",
    "        testName = f'{identifier}-{identifierValue}'\n",
    "        \n",
    "        testPathName = 'Tests/'+testName+f'/foldn{i+1}'\n",
    "        print(f'{identifier}: foldn{i+1} RUN\\n=========================================')\n",
    "        xTest, yTest = [croppedSegmentsList[i] for i in test_index], [recistCriteria[i] for i in test_index]\n",
    "        xTrain, yTrain = [croppedSegmentsList[i] for i in train_index], [recistCriteria[i] for i in train_index]\n",
    "        xVal, yVal = xTest, yTest # Set the validation set to the same as the testing set\n",
    "        #xVal, yVal = [croppedSegmentsList[i] for i in train_index[:len(xTest)]], [recistCriteria[i] for i in train_index[:len(yTest)]]\n",
    "        \n",
    "\n",
    "        # ## Working with Numpy arrays\n",
    "        xTrain = np.array(xTrain) \n",
    "        xTest = np.array(xTest)\n",
    "        xVal = np.array(xVal)\n",
    "        yTrain = np.array(yTrain)\n",
    "        yVal = np.array(yVal)\n",
    "        yTest = np.array(yTest)\n",
    "\n",
    "        ## ==============================================================\n",
    "        ## Using SMOTE\n",
    "        # smote = SMOTE(random_state=42)\n",
    "        # if len(xTrain.shape)==3:\n",
    "        #     oneDShape = xTrain[0].shape[0]*xTrain[0].shape[1]\n",
    "            \n",
    "        # else:\n",
    "        #     print('xTrain shape',xTrain.shape)\n",
    "        #     oneDShape = xTrain[0].shape[0]*xTrain[0].shape[1]*xTrain[0].shape[2]\n",
    "\n",
    "        # singleShape = xTrain[0].shape\n",
    "\n",
    "        # print('xTrain reshape',xTrain.reshape(xTrain.shape[0],oneDShape).shape)\n",
    "        # xTrainSmote, yTrain = smote.fit_resample(xTrain.reshape(xTrain.shape[0],oneDShape), yTrain)\n",
    "        # if len(xTrain.shape)==3:\n",
    "        #     xTrain = xTrainSmote.reshape(xTrainSmote.shape[0], xTrain[0].shape[0],xTrain[0].shape[1])\n",
    "        # else:\n",
    "        #     xTrain = xTrainSmote.reshape(xTrainSmote.shape[0], xTrain[0].shape[0],xTrain[0].shape[1],xTrain[0].shape[2])\n",
    "\n",
    "        # print('xTrain after Smote', xTrain.shape)\n",
    "        # print('yTrain after Smote', yTrain.shape)\n",
    "        # from collections import Counter\n",
    "        # counter  = Counter(yTest)\n",
    "        # print('Splits for test Fold',sorted(counter.items()))\n",
    "\n",
    "        ## ==============================================================\n",
    "        \n",
    "        # May or may not need this, def not needed for grouped2D=True\n",
    "        # xTrain = np.expand_dims(xTrain,axis=-1)\n",
    "        # xVal = np.expand_dims(xVal,axis=-1)\n",
    "        # xTest = np.expand_dims(xTest,axis=-1)\n",
    "\n",
    "        print('xTrain', xTrain.shape)\n",
    "        print('xVal', xVal.shape)\n",
    "        print('xTest', xTest.shape)\n",
    "\n",
    "        ## Get and save results for each fold        \n",
    "        confusionMatrix, history, accuracy, f1, recall, predictsTotal, endingEpoch = runModelFullStack(testPathName, testName, xTrain, yTrain, xVal, yVal, xTest, yTest, trainingTransform=trainingTransform, modelInformation=modelInformation) \n",
    "\n",
    "        confusion_matricies.append(confusionMatrix)\n",
    "        histories.append(history)\n",
    "        accuracies.append(accuracy)\n",
    "        f1s.append(f1)\n",
    "        recalls.append(recall)\n",
    "        predictionSplits.append(predictsTotal)\n",
    "        endingEpochs.append(endingEpoch)\n",
    "        print('\\n\\n')\n",
    "\n",
    "    #assert False\n",
    "    # Calculate the average of the metrics for the kfolds of this transformation and save it\n",
    "    kFoldsTestMetrics = {'Prediction averages': averagePredictionTotals(predictionSplits), 'Accuracy':meanConfidenceInterval(accuracies), 'F1 Score':averageMultilabelMetricScores(f1s), 'Recall':averageMultilabelMetricScores(recalls)}\n",
    "    file = open('Tests/'+testName+'/kFoldsTestMetrics.txt','w')\n",
    "    for key, value in kFoldsTestMetrics.items():\n",
    "        file.write(f'{key}: {value}\\n')\n",
    "        print(f'{key}: {value}')\n",
    "    file.close()\n",
    "\n",
    "    # Plot training and confusion matrix for each fold as a single .png\n",
    "    plotConfusionMatricies('Tests/'+testName, testName, confusion_matricies)\n",
    "    plotTrainingPerformances('Tests/'+testName, testName, histories, saveFigure=True, showResult=True)\n",
    "\n",
    "    # Append results to the xlsx file\n",
    "    appendMetricsToXLSX(testPathName, trainingTransform, meanConfidenceInterval(accuracies), averageMultilabelMetricScores(f1s), averageMultilabelMetricScores(recalls), averagePredictionTotals(predictionSplits), endingEpochs, modelInformation, dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View what the splits are for the models, the test set is {0: 4, 1: 7-8, 2: 6-5}\n",
    "# stratifiedFolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# stratifiedFolds.get_n_splits(croppedSegmentsList, recistCriteria)\n",
    "# for i, (train_index, test_index) in enumerate(stratifiedFolds.split(croppedSegmentsList, recistCriteria)):\n",
    "#         xTrain, xTest = [croppedSegmentsList[i] for i in train_index], [croppedSegmentsList[i] for i in test_index]\n",
    "#         yTrain, yTest = [recistCriteria[i] for i in train_index], [recistCriteria[i] for i in test_index]\n",
    "\n",
    "#         # Convert the recist criteria to 0,1,2\n",
    "#         yTrain = [x-1 for x in yTrain]\n",
    "#         yTest = [x-1 for x in yTest]\n",
    "\n",
    "#         #count the categorization splits of the train and test set\n",
    "#         trainRecistSplit = {y: yTrain.count(y) for y in yTrain}\n",
    "#         testRecistSplit = {y: yTest.count(y) for y in yTest}\n",
    "\n",
    "#         # Format the splits nicely into an ordered dictionary\n",
    "#         myKeys = list(trainRecistSplit.keys())\n",
    "#         myKeys.sort()\n",
    "#         trainRecistSplitDisplay = {i: trainRecistSplit[i] for i in myKeys}\n",
    "#         myKeys = list(testRecistSplit.keys())\n",
    "#         myKeys.sort()\n",
    "#         testRecistSplitDisplay = {i: testRecistSplit[i] for i in myKeys}\n",
    "\n",
    "#         print(f'train recist category split: {trainRecistSplitDisplay}')\n",
    "#         print(f'test recist category split: {testRecistSplitDisplay}')\n",
    "\n",
    "#         # ## Working with Numpy arrays\n",
    "#         xTrain = np.array(xTrain) \n",
    "#         xTest = np.array(xTest)\n",
    "#         yTrain = np.array(yTrain)\n",
    "#         yTest = np.array(yTest)\n",
    "#         xTrain =  np.expand_dims(xTrain,axis=-1)\n",
    "#         xTest =  np.expand_dims(xTest,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "details d:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\ipykernel_launcher.py\n",
      "details --f=c:\\Users\\johnz\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-14200y9x0cFW4iARP.json\n",
      "python d:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\ipykernel_launcher.py --f=c:\\Users\\johnz\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-14200y9x0cFW4iARP.json\n",
      "[0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0]\n",
      "previous difference 33\n",
      "New difference after undersampling 0\n",
      ": foldn1 RUN\n",
      "=========================================\n",
      "xTrain (44, 6, 299, 299)\n",
      "xVal (12, 6, 299, 299)\n",
      "xTest (12, 6, 299, 299)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "2024-08-06 02:53:49.970 | WARNING  | fmcib.models.load_model:load:104 - Missing keys: [] and unexpected keys: []\n",
      "2024-08-06 02:53:49.972 | INFO     | fmcib.models.load_model:load:129 - Loaded pretrained model weights \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using this device: cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 1, 7, 7, 7], expected input[8, 6, 3, 299, 299] to have 1 channels, but got 6 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# modelInformation = { 'learningRate': learningRate, 'dropoutRate': dropoutRate, 'batchSize': batchSize, 'numOfEpochs':1, 'weight_decay':weight_decay, 'commandRan': commandRan, 'model': modelChosen, 'votingSystem': votingSystem}\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Run the tests\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m transformsTested\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 38\u001b[0m     \u001b[43mgenerateKFoldsValidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevalDetailLine\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcroppedSegmentsList\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcroppedSegmentsList\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecistCriteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrecistCriteria\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrainingTransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelInformation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodelInformation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrouped2D\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrouped2D\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m dataframe\u001b[38;5;241m.\u001b[39mto_excel(dataframePath, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[49], line 126\u001b[0m, in \u001b[0;36mgenerateKFoldsValidation\u001b[1;34m(identifier, identifierValue, modelInformation, grouped2D, croppedSegmentsList, recistCriteria, cases, k, trainingTransform)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxTest\u001b[39m\u001b[38;5;124m'\u001b[39m, xTest\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m## Get and save results for each fold        \u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m confusionMatrix, history, accuracy, f1, recall, predictsTotal, endingEpoch \u001b[38;5;241m=\u001b[39m \u001b[43mrunModelFullStack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestPathName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxVal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myVal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxTest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myTest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainingTransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainingTransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodelInformation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodelInformation\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    128\u001b[0m confusion_matricies\u001b[38;5;241m.\u001b[39mappend(confusionMatrix)\n\u001b[0;32m    129\u001b[0m histories\u001b[38;5;241m.\u001b[39mappend(history)\n",
      "Cell \u001b[1;32mIn[48], line 64\u001b[0m, in \u001b[0;36mrunModelFullStack\u001b[1;34m(testPathName, testName, xTrain, yTrain, xVal, yVal, xTest, yTest, modelInformation, trainingTransform)\u001b[0m\n\u001b[0;32m     62\u001b[0m trainingData, validationData, testingData, training_data_transforms \u001b[38;5;241m=\u001b[39m convertDataToLoaders(xTrain, yTrain, xVal, yVal, xTest, yTest, training_data_transforms \u001b[38;5;241m=\u001b[39m trainingTransform, batchSize\u001b[38;5;241m=\u001b[39mmodelInformation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatchSize\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     63\u001b[0m model, criterion, scheduler, optimizer \u001b[38;5;241m=\u001b[39m defineModel(dropoutRate\u001b[38;5;241m=\u001b[39mmodelInformation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropoutRate\u001b[39m\u001b[38;5;124m'\u001b[39m], learningRate \u001b[38;5;241m=\u001b[39m modelInformation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearningRate\u001b[39m\u001b[38;5;124m'\u001b[39m], weight_decay\u001b[38;5;241m=\u001b[39mmodelInformation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m], model \u001b[38;5;241m=\u001b[39m modelInformation[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 64\u001b[0m model, criterion, device, history, endingEpoch \u001b[38;5;241m=\u001b[39m \u001b[43mtrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainingData\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidationData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumOfEpochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodelInformation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnumOfEpochs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodelInformation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m saveResults(testPathName, model, history, training_data_transforms, saveModel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     66\u001b[0m confusionMatrix, accuracy, f1, recall, predictsTotal \u001b[38;5;241m=\u001b[39m evaluateModelOnTestSet(testPathName, model, testingData, criterion, device, saveConfusionMatrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, showConfusionMatrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[44], line 13\u001b[0m, in \u001b[0;36mtrainModel\u001b[1;34m(model, criterion, scheduler, optimizer, trainingData, validationData, patience, numOfEpochs)\u001b[0m\n\u001b[0;32m      9\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(numOfEpochs):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m#Train model\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     curTrainLoss, curTrainAcc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainingData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumOfEpochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurTrainLoss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurTrainAcc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[43], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loader, criterion, scheduler, optimizer, device)\u001b[0m\n\u001b[0;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[42], line 35\u001b[0m, in \u001b[0;36mSmallFoundation.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Into foundation model\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     36\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[0;32m     37\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n",
      "File \u001b[1;32md:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\fmcib\\models\\load_model.py:72\u001b[0m, in \u001b[0;36mLoadModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     63\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    Forward pass of the neural network.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m        torch.Tensor: The output tensor.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads(out)\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32md:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\monai\\networks\\nets\\resnet.py:336\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 336\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m    338\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n",
      "File \u001b[1;32md:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\torch\\nn\\modules\\conv.py:608\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\torch\\nn\\modules\\conv.py:603\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    593\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    594\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    601\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    602\u001b[0m     )\n\u001b[1;32m--> 603\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    605\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 1, 7, 7, 7], expected input[8, 6, 3, 299, 299] to have 1 channels, but got 6 channels instead"
     ]
    }
   ],
   "source": [
    "#Make all transforms that I am going to test:\n",
    "transformsTested = {\n",
    "    \"0\":None\n",
    "    #\"20\":generateTransform(RandomRotationValue=20, RandomElaticTransform=[20.0,2.0], brightnessConstant=20, contrastConstant=20, kernelSize=3, sigmaRange=(0.001,0.4)),\n",
    "    # \"40\":generateTransform(RandomRotationValue=40, RandomElaticTransform=[40.0,4.0], brightnessConstant=40, contrastConstant=40, kernelSize=3, sigmaRange=(0.001,0.8)),\n",
    "    # \"60\":generateTransform(RandomRotationValue=60, RandomElaticTransform=[60.0,6.0], brightnessConstant=60, contrastConstant=60, kernelSize=3, sigmaRange=(0.001,1.2)),\n",
    "    # \"80\":generateTransform(RandomRotationValue=80, RandomElaticTransform=[80.0,8.0], brightnessConstant=80, contrastConstant=80, kernelSize=3, sigmaRange=(0.001,1.6)),\n",
    "    #\"100\":generateTransform(RandomRotationValue=100, RandomElaticTransform=[100.0,10.0], brightnessConstant=100, contrastConstant=100, kernelSize=3, sigmaRange=(0.001,2.0)),\n",
    "    #\"defaults+50%\":generateTransform(RandomRotationValue=50, RandomElaticTransform=[50.0,5.0], brightnessConstant=50.0, contrastConstant=50.0, kernelSize=3, sigmaRange=(0.1,2.0))    \n",
    "}\n",
    "\n",
    "## Open the dataframe and add the evaluation details\n",
    "dataframePath='testResults.xlsx'\n",
    "columns = ['name','numOfEpochs','batchSize','learningRate','dropoutRate','weight_decay', 'commandRan','RandomRotation','ElasticTransform','Brightness','Contrast','GaussianBlur','RandomHorizontalFlip','RandomVerticalFlip','PredictionAverage',\n",
    "            'AccuracyAverage','F1Average', 'RecallAverage','AccuracySTD','F1STD','RecallSTD','AccuracyData','F1Data','RecallData', 'EndingEpoch']\n",
    "dataframe = pd.read_excel('testResults.xlsx', header=None, names=columns)\n",
    "addEvalDetailToModel(evalDetailLine, dataframe)\n",
    "\n",
    "# Generate the command ran for the test \n",
    "commandRan = 'python'\n",
    "for details in sys.argv:\n",
    "    print('details',details)\n",
    "    stringArgs = ['evalDetailLine','modelChosen','votingSystem']\n",
    "    if details in stringArgs:\n",
    "        detailArray = details.split('=') \n",
    "        details = f'{detailArray[0]}=\\'{detailArray[1]}\\''\n",
    "    commandRan += f' {details}'   \n",
    "print(commandRan)\n",
    "\n",
    "#Define patience\n",
    "patience = 10\n",
    "\n",
    "modelInformation = { 'learningRate': learningRate, 'dropoutRate': dropoutRate, 'batchSize': batchSize, 'numOfEpochs':numOfEpochs, 'weight_decay':weight_decay, 'commandRan': commandRan, 'model': modelChosen, 'patience':patience, 'votingSystem': votingSystem}\n",
    "# modelInformation = { 'learningRate': learningRate, 'dropoutRate': dropoutRate, 'batchSize': batchSize, 'numOfEpochs':1, 'weight_decay':weight_decay, 'commandRan': commandRan, 'model': modelChosen, 'votingSystem': votingSystem}\n",
    "\n",
    "# Run the tests\n",
    "for key, value in transformsTested.items():\n",
    "    generateKFoldsValidation(evalDetailLine,\"-\", croppedSegmentsList=croppedSegmentsList, recistCriteria=recistCriteria, cases=cases, k=5,trainingTransform=value, modelInformation = modelInformation, grouped2D=grouped2D)\n",
    "\n",
    "dataframe.to_excel(dataframePath, index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
