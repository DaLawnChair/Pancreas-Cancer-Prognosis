{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 1\n",
    "NUM_EPOCHS = 100\n",
    "testName = f'ResNet_singleSlice_epochs{NUM_EPOCHS}_batchsze{BATCHSIZE}_onLaptop_3classes_minimalAugmentationDataAugmentation'\n",
    "testPathName = 'Tests/'+testName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook imageSegmentationMultipleSingleSlice.ipynb to python\n",
      "[NbConvertApp] Writing 28103 bytes to imageSegmentationMultipleSingleSlice.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert to python script, remember to delte the next line in the actual file\n",
    "! jupyter nbconvert --to python imageSegmentationMultipleSingleSlice.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nrrd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import cv2\n",
    "import scipy\n",
    "\n",
    "import SimpleITK as sitk \n",
    "import os \n",
    "import shutil\n",
    "\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pynrrd matplotlib opencv-python scipy simpleitk pandas openpyxl scikit-learn\n",
    "# ! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \n",
    "# ! pip install foundation-cancer-image-biomarker -qq\n",
    "# ! pip install foundation-cancer-image-biomarker\n",
    "\n",
    "# ! pip install nbconvert\n",
    "# ! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi, notebook is working\n"
     ]
    }
   ],
   "source": [
    "# ! pip freeze > research.txt\n",
    "print('hi, notebook is working')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data from the .xsxl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['TAPS_CaseIDs_PreNAT','RECIST_PostNAT', 'Slice_Thickness']\n",
    "data = pd.read_excel('PDAC-Response_working.xlsx', header=None,names = columns)\n",
    "data.drop(0, inplace=True) # Remove the header row\n",
    "data=data.sort_values(by=['TAPS_CaseIDs_PreNAT'])\n",
    "\n",
    "# # Get the entire datasheet\n",
    "cases = list(data['TAPS_CaseIDs_PreNAT'])\n",
    "recistCriteria = list(data['RECIST_PostNAT'])\n",
    "\n",
    "# # Remove all folders that have >13 slices in the datasheet\n",
    "# data.drop(data[data['Slice_Thickness'] <= 13].index, inplace = True)\n",
    "# print(list(data['Slice_Thickness']))\n",
    "# cases = list(data['TAPS_CaseIDs_PreNAT'])\n",
    "# recistCriteria = list(data['RECIST_PostNAT'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform preprocessing on multiple images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_image_to_adbomen(image, window_center, window_width):\n",
    "    \n",
    "    img_max = window_center + int(window_width / 2)\n",
    "    img_min = window_center - int(window_width / 2)\n",
    "    return np.clip(image, img_min, img_max)\n",
    "\n",
    "def centerXYOfImage(overlay_mask, segment_mask, segmentedSlices, padding=25):\n",
    "    \"\"\" \n",
    "    Centers the X and Y of the image to crop the image. segmentedSlices is given as an array of z-value slices because the same approach to x_indicies and y_indicies does not work on overlay_segment (works for x and y though)\n",
    "    \"\"\"\n",
    "    x_indices, y_indices, _ = np.where(segment_mask == 1)\n",
    "    # Get the bounding box for x and y dimensions\n",
    "    min_x, max_x = x_indices.min(), x_indices.max()\n",
    "    min_y, max_y = y_indices.min(), y_indices.max()\n",
    "\n",
    "    center_x = (min_x + max_x) // 2\n",
    "    center_y = (min_y + max_y) // 2\n",
    "\n",
    "    width = abs(max_x - min_x) // 2\n",
    "    height = abs(max_y - min_y) // 2\n",
    "\n",
    "    \n",
    "    # Get the dimensions of the cropped image\n",
    "    start_x = max(0, center_x - width - padding)\n",
    "    end_x = min(segment_mask.shape[0], center_x + width + padding)\n",
    "    start_y = max(0, center_y - height - padding)\n",
    "    end_y = min(segment_mask.shape[1], center_y + height + padding)\n",
    "\n",
    "    # # Adjust the crop region if it's smaller than 100x100\n",
    "    # if end_x - start_x < 100:\n",
    "    #     if start_x == 0:\n",
    "    #         end_x = min(segment_mask.shape[0], 100)\n",
    "    #     elif end_x == segment_mask.shape[0]:\n",
    "    #         start_x = max(0, segment_mask.shape[0] - 100)\n",
    "    # if end_y - start_y < 100:\n",
    "    #     if start_y == 0:\n",
    "    #         end_y = min(segment_mask.shape[1], 100)\n",
    "    #     elif end_y == segment_mask.shape[1]:\n",
    "    #         start_y = max(0, segment_mask.shape[1] - 100)\n",
    "\n",
    "    segmentedSlices = np.sort(np.array(segmentedSlices))\n",
    "    return overlay_mask[start_x:end_x, start_y:end_y, np.array(segmentedSlices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertNdArrayToCV2Image(image, resolution = (64,64)):\n",
    "    \"\"\" Converts a numpy array to a cv2 image \"\"\"\n",
    "    image = np.array(image, dtype=np.uint8)\n",
    "    image = cv2.resize(image, resolution)\n",
    "    return image\n",
    "\n",
    "def makeAlign(image1,image2):\n",
    "    image1.SetDirection(image2.GetDirection())\n",
    "    image1.SetOrigin(image2.GetOrigin())\n",
    "    image1.SetSpacing(image2.GetSpacing())\n",
    "    return image1, image2\n",
    "\n",
    "def isAligned(image1, image2):\n",
    "    return image1.GetDirection() == image2.GetDirection() and image1.GetOrigin() == image2.GetOrigin() and image1.GetSpacing() == image2.GetSpacing()   \n",
    "\n",
    "def resampleSizes(wholeHeader, segmentHeader):\n",
    "    \"\"\" \n",
    "    Resamples the sitk image to have the same size based on the one with the largest size.\n",
    "    \"\"\"\n",
    "    if wholeHeader.GetSize()[-1] >= segmentHeader.GetSize()[-1]:\n",
    "        imageLarge = wholeHeader\n",
    "        imageSmall = segmentHeader\n",
    "        wholeThenSegmentOrder = True\n",
    "    else:\n",
    "        imageLarge = segmentHeader\n",
    "        imageSmall = wholeHeader \n",
    "        wholeThenSegmentOrder = False\n",
    "\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetReferenceImage(imageLarge)  \n",
    "    resample.SetInterpolator(sitk.sitkLinear)  # Choose the interpolation method (sitkLinear, sitkNearestNeighbor, etc.)\n",
    "    resample.SetDefaultPixelValue(0)  # Set default pixel value for areas outside the original image\n",
    "\n",
    "    imageSmall = resample.Execute(imageSmall)\n",
    "\n",
    "    print(f'imageLarge: {imageLarge.GetSize()}')\n",
    "    print(f'imageSmall: {imageSmall.GetSize()}')\n",
    "    \n",
    "\n",
    "    if wholeThenSegmentOrder:\n",
    "        return imageLarge, imageSmall # whole, then segment\n",
    "    else:\n",
    "        return imageSmall, imageLarge # segment, then whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twoImageAlignProceess(wholeHeader,segmentHeader,verbose):    \n",
    "\n",
    "    error = False\n",
    "    # Check if the images are aligned\n",
    "    wholeHeader, segmentHeader = makeAlign(wholeHeader, segmentHeader)\n",
    "    imagesAreAligned = isAligned(wholeHeader, segmentHeader)\n",
    "    print(f'Are the two images aligned now?: {imagesAreAligned}' if verbose==2 else '',end='')\n",
    "\n",
    "    if not imagesAreAligned:\n",
    "        error = True\n",
    "        return None, None, True\n",
    "    \n",
    "    # Set the spacing of the image to 1x1x1mm voxel spacing\n",
    "    wholeHeader.SetSpacing([1,1,1])\n",
    "    segmentHeader.SetSpacing([1,1,1])\n",
    "    imagesSpacingAligned = wholeHeader.GetSpacing() == segmentHeader.GetSpacing() \n",
    "    print(f'Are the two images aligned in terms of spacing?: {imagesSpacingAligned}' if verbose==2 else '',end='')\n",
    "\n",
    "    if not imagesSpacingAligned:\n",
    "        error = True\n",
    "        return None, None, True\n",
    "    \n",
    "\n",
    "    imagesSizeAligned = wholeHeader.GetSize() == segmentHeader.GetSize() \n",
    "    print(f'Are the two images aligned in terms of size?: {imagesSizeAligned}' if verbose==2 else '',end='')\n",
    "\n",
    "    if not imagesSizeAligned:\n",
    "        wholeHeader, segmentHeader = resampleSizes(wholeHeader, segmentHeader)\n",
    "        print(f'whole size: {wholeHeader.GetSize()}')\n",
    "        print(f'segment size: {segmentHeader.GetSize()}')\n",
    "        imagesSizeAligned = wholeHeader.GetSize() == segmentHeader.GetSize() \n",
    "        print(f'Are the two images aligned in terms of size now?: {imagesSizeAligned}' if verbose==2 else '',end='')\n",
    "        if not imagesSizeAligned:\n",
    "            error = True\n",
    "            return None, None, True\n",
    "\n",
    "    \n",
    "    return wholeHeader, segmentHeader, False\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayCroppedSegmentations(croppedSegment):\n",
    "    print(f'CroppedSegment shape: {croppedSegment.shape}')\n",
    "    # Display the segmented image slices \n",
    "\n",
    "    columnLen = 10\n",
    "    rowLen = max(2,croppedSegment.shape[-1] // columnLen + 1) \n",
    "    figure,axis = plt.subplots( rowLen, columnLen, figsize=(10, 10))\n",
    "    \n",
    "    rowIdx = 0\n",
    "    for idx in range(croppedSegment.shape[-1]):        \n",
    "        if idx%columnLen == 0 and idx>0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][idx%columnLen].imshow(croppedSegment[:,:,idx], cmap=\"gray\")\n",
    "        axis[rowIdx][idx%columnLen].axis('off')\n",
    "\n",
    "    # Turn off the axis of the rest of the subplots\n",
    "    for i in range(idx+1, rowLen*columnLen):\n",
    "        if i%columnLen == 0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][i%columnLen].axis('off')\n",
    "    \n",
    "    # plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def displayOverlayedSegmentations(segmentedSlices, augmented_whole, augmented_segment):\n",
    "\n",
    "    maskedWhole = np.ma.masked_where(augmented_segment==1, augmented_whole)\n",
    "    # Display the segmented image slices \n",
    "\n",
    "    columnLen = 10\n",
    "    rowLen = max(2,len(segmentedSlices) // columnLen + 1) \n",
    "    figure,axis = plt.subplots( rowLen, columnLen, figsize=(6, 6))\n",
    "    rowIdx = 0\n",
    "    for idx in range(len(segmentedSlices)):        \n",
    "        if idx%columnLen == 0 and idx>0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][idx%columnLen].imshow(augmented_whole[:,:,segmentedSlices[idx]], cmap=\"gray\")\n",
    "        axis[rowIdx][idx%columnLen].imshow(augmented_segment[:,:,segmentedSlices[idx]], cmap=\"Blues\", alpha=0.75)\n",
    "        axis[rowIdx][idx%columnLen].axis('off')\n",
    "\n",
    "    # Turn off the axis of the rest of the subplots\n",
    "    for i in range(idx+1, rowLen*columnLen):\n",
    "        if i%columnLen == 0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][i%columnLen].axis('off')\n",
    "    \n",
    "    # plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(wholeHeader, segmentHeader, verbose=0):\n",
    "    \"\"\"\n",
    "    Preprocesses the wholeHeader and segmentHeader sitk images to be ready for augmentation \n",
    "    Verbose = 0: No output\n",
    "    Verbose = 1: Only the CT scans slices and the array of slices it uses\n",
    "    Verbose = 2: Everything\n",
    "    Verbose = 3: Show the segment mask on top of the whole CT scan\n",
    "\n",
    "    Returns: a np array windowed whole image, a np array cropped segment image to 64x64x[] resolution, and boolean error flag.\n",
    "    \"\"\"\n",
    "    error = False # Error flag to check if there was an error in the preprocessing\n",
    "\n",
    "    # Align the two images \n",
    "    wholeHeader, segmentHeader, error = twoImageAlignProceess(wholeHeader, segmentHeader, verbose) \n",
    "    if error:\n",
    "        return None, None, error\n",
    "    \n",
    "    # Convert the images into numpy arrays for further processing, take the transpose as the format is z,y,x\n",
    "    whole = sitk.GetArrayFromImage(wholeHeader).T\n",
    "    segment = sitk.GetArrayFromImage(segmentHeader).T\n",
    "\n",
    "    print(f'Spacing of whole:{whole.shape}' if verbose==2 else '',end='')\n",
    "    print(f'Spacing of segment:{segment.shape}' if verbose==2 else '',end='')\n",
    "    \n",
    "    # Windowing parameters for the abdomen\n",
    "    ABDOMEN_UPPER_BOUND = 215\n",
    "    ABDOMEN_LOWER_BOUND = -135\n",
    "    window_center = (ABDOMEN_UPPER_BOUND+ABDOMEN_LOWER_BOUND) / 2\n",
    "    window_width = (ABDOMEN_UPPER_BOUND-ABDOMEN_LOWER_BOUND) / 2\n",
    "\n",
    "    # Window and resample the whole image\n",
    "    augmented_whole = window_image_to_adbomen(whole, window_center, window_width)\n",
    "\n",
    "    # Get the slice indices where the segment is present in \n",
    "    augmented_segment = segment\n",
    "    segmentedSlices = [] \n",
    "    for index in range(augmented_segment.shape[-1]):\n",
    "        if len(np.unique(augmented_segment[:,:,index])) > 1:\n",
    "            segmentedSlices.append(index)\n",
    "\n",
    "    print(f'Segment slice indices:{segmentedSlices}' if verbose==2 else '',end='')\n",
    "\n",
    "\n",
    "    #Segment the whole image with the segment mask\n",
    "    overlay_segment = augmented_whole * augmented_segment    \n",
    "    croppedSegment = centerXYOfImage(overlay_segment,augmented_segment,segmentedSlices) # Crop the image to the center of the segmented region     \n",
    "\n",
    "    croppedSegment = (croppedSegment - np.min(croppedSegment)) / (np.max(croppedSegment) - np.min(croppedSegment)) #Normalize the image to [0,1]\n",
    "    \n",
    "    # croppedSegment[croppedSegment<0.0001]=0 # Window the image so that the background is completely black for all slices\n",
    "    croppedSegment = convertNdArrayToCV2Image(croppedSegment) # Convert the image to a cv2 image\n",
    "\n",
    "    #Display the results of preprocessing\n",
    "    if verbose==1 or verbose==2:\n",
    "        displayCroppedSegmentations(croppedSegment)\n",
    "    elif verbose==3:\n",
    "        displayOverlayedSegmentations(segmentedSlices, augmented_whole, augmented_segment)\n",
    "    \n",
    "    return whole, croppedSegment, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLargestSlice(croppedSegment):\n",
    "    \"\"\"\n",
    "    Finds the index with the largest slice in the croppedSegment and returns the index as well as the sorted number of slices each index has\n",
    "    \"\"\"\n",
    "    max = 0\n",
    "    maxIndex = 0\n",
    "    \n",
    "    indices = []\n",
    "    sliceTotals = []\n",
    "    for idx in range(croppedSegment.shape[-1]):\n",
    "        unique, counts = np.unique(croppedSegment[:,:,idx], return_counts=True)\n",
    "        values = dict(zip(unique, counts))\n",
    "        sliceTotal = 0\n",
    "        for value,count in values.items():\n",
    "            sliceTotal += count if value > 0 else 0 \n",
    "        \n",
    "\n",
    "        indices.append(idx)\n",
    "        sliceTotals.append(sliceTotal)\n",
    "        \n",
    "        if sliceTotal > max: \n",
    "            max = sliceTotal\n",
    "            maxIndex = idx \n",
    "\n",
    "    values = dict(zip(sliceTotals,indices))\n",
    "    values = dict(sorted(values.items())) # Sort the values by number of slices\n",
    "\n",
    "    return maxIndex, values\n",
    "\n",
    "# def updateSlices(croppedSegment, desiredNumberOfSlices=1):\n",
    "#     \"\"\"\n",
    "#     Updates the number of slices to the number of slices given. \n",
    "#     If the numberOfSlices > the number of slices in the croppedSegment, it will duplicate the slices of the largest slices \n",
    "#     If the numberOfSlices < the number of slices in the croppedSegment, it will remove the slices with the least amount of information \n",
    "#     If the numberOfSlices == the number of slices in the croppedSegment, it will do nothing     \n",
    "#     \"\"\"\n",
    "#     if croppedSegment.shape[-1] == desiredNumberOfSlices:\n",
    "#         return croppedSegment\n",
    "#     elif croppedSegment.shape[-1] < desiredNumberOfSlices: # Duplicate slices from the largest slice\n",
    "\n",
    "#         # Specifications of croppedSegment\n",
    "#         original = np.copy(croppedSegment)\n",
    "#         largestSliceIndex, _ = getLargestSlice(croppedSegment)\n",
    "#         maxUpperBound = croppedSegment.shape[-1] -1\n",
    "#         minLowerBound = 0\n",
    "        \n",
    "#         # Specification of the values to duplicate\n",
    "#         numToDuplication = desiredNumberOfSlices - croppedSegment.shape[-1] \n",
    "#         ends = numToDuplication//2\n",
    "#         lowerRemainder = abs(largestSliceIndex - ends) if (largestSliceIndex - ends) < minLowerBound else 0   \n",
    "#         upperRemainder = abs(maxUpperBound - (largestSliceIndex + ends)) if largestSliceIndex + ends > maxUpperBound else 0 \n",
    "\n",
    "#         #Printing of the of the specifications\n",
    "#         print(f'LargestSegmentIdx = {largestSliceIndex}\\nNumber of slices to duplicate: {numToDuplication}\\n Ends: {ends}, \\nlowerRemainder: {lowerRemainder},\\n upperRemainder: {upperRemainder}')\n",
    "        \n",
    "#         #Making of the range to center the slices to duplicate\n",
    "#         duplicationRange = list(range( largestSliceIndex - ends - upperRemainder + lowerRemainder , largestSliceIndex + ends + lowerRemainder - upperRemainder))\n",
    "\n",
    "#         print('preAdd',duplicationRange)\n",
    "#         #Edge case where we only need 1 extra slice\n",
    "#         if len(duplicationRange) == 0:\n",
    "#             duplicationRange = [largestSliceIndex]\n",
    "\n",
    "#         # Fixes the slices if we are off by 1\n",
    "#         if len(duplicationRange)+croppedSegment.shape[-1] == desiredNumberOfSlices: \n",
    "#             pass \n",
    "#         else:\n",
    "#             # Add to the right side if the left will be out of bounds\n",
    "#             if duplicationRange[0] -1 < minLowerBound:\n",
    "#                 duplicationRange = duplicationRange + [duplicationRange[-1] + 1]\n",
    "#             # Add to the left side if the right will be out of bounds\n",
    "#             elif duplicationRange[-1] +1 > maxUpperBound:\n",
    "#                 duplicationRange = [duplicationRange[0] - 1] + duplicationRange\n",
    "#             else: #Default, add to the right side\n",
    "#                 duplicationRange = duplicationRange + [duplicationRange[-1] + 1]\n",
    "                \n",
    "#         print(f'CroppedSlices={list(range(0,croppedSegment.shape[-1]))}\\nSlices: {duplicationRange}')\n",
    "#         print(len(duplicationRange)+croppedSegment.shape[-1])\n",
    "#         assert(len(duplicationRange)+croppedSegment.shape[-1]== desiredNumberOfSlices) #Ensure that the desired number of slices is met\n",
    "\n",
    "#         #Insert the values\n",
    "#         croppedSegment = np.insert(croppedSegment, duplicationRange, original[:,:,duplicationRange], axis=-1)\n",
    "            \n",
    "#         print('greater than')\n",
    "#         return croppedSegment\n",
    "#     else:\n",
    "#         # Specifications of croppedSegment\n",
    "#         original = np.copy(croppedSegment)\n",
    "#         _, sliceValues = getLargestSlice(croppedSegment)\n",
    "#         numberOfSlicesToRemove =  croppedSegment.shape[-1] - desiredNumberOfSlices \n",
    "\n",
    "#         # Remove the slices with the least amount of information\n",
    "#         croppedSegment = np.delete(croppedSegment,list(sliceValues.values())[:numberOfSlicesToRemove], axis=-1)\n",
    "\n",
    "#         print('Less than')\n",
    "#         return croppedSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "CASE244 All files read:\n",
      "==============================================================\n",
      "CASE246 All files read:\n",
      "==============================================================\n",
      "CASE247 All files read:\n",
      "==============================================================\n",
      "CASE251 All files read:\n",
      "==============================================================\n",
      "CASE254 All files read:\n",
      "==============================================================\n",
      "CASE256 All files read:\n",
      "==============================================================\n",
      "CASE263 All files read:\n",
      "==============================================================\n",
      "CASE264 All files read:\n",
      "==============================================================\n",
      "CASE265 All files read:\n",
      "==============================================================\n",
      "CASE270 All files read:\n",
      "==============================================================\n",
      "CASE274 All files read:\n",
      "==============================================================\n",
      "CASE467 All files read:\n",
      "==============================================================\n",
      "CASE468 All files read:\n",
      "==============================================================\n",
      "CASE471 All files read:\n",
      "==============================================================\n",
      "CASE472 All files read:\n",
      "==============================================================\n",
      "CASE476 All files read:\n",
      "==============================================================\n",
      "CASE479 All files read:\n",
      "==============================================================\n",
      "CASE480 All files read:\n",
      "==============================================================\n",
      "CASE482 All files read:\n",
      "==============================================================\n",
      "CASE484 All files read:\n",
      "==============================================================\n",
      "CASE485 All files read:\n",
      "==============================================================\n",
      "CASE488 All files read:\n",
      "==============================================================\n",
      "CASE494 All files read:\n",
      "==============================================================\n",
      "CASE496 All files read:\n",
      "==============================================================\n",
      "CASE499 All files read:\n",
      "==============================================================\n",
      "CASE500 All files read:\n",
      "==============================================================\n",
      "CASE505 All files read:\n",
      "==============================================================\n",
      "CASE515 All files read:\n",
      "==============================================================\n",
      "CASE523 All files read:\n",
      "==============================================================\n",
      "CASE525 All files read:\n",
      "==============================================================\n",
      "CASE531 All files read:\n",
      "==============================================================\n",
      "CASE534 All files read:\n",
      "==============================================================\n",
      "CASE535 All files read:\n",
      "==============================================================\n",
      "CASE537 All files read:\n",
      "==============================================================\n",
      "CASE539 All files read:\n",
      "==============================================================\n",
      "CASE541 All files read:\n",
      "==============================================================\n",
      "CASE543 All files read:\n",
      "==============================================================\n",
      "CASE546 All files read:\n",
      "==============================================================\n",
      "CASE547 All files read:\n",
      "==============================================================\n",
      "CASE548 All files read:\n",
      "==============================================================\n",
      "CASE549 All files read:\n",
      "==============================================================\n",
      "CASE551 All files read:\n",
      "==============================================================\n",
      "CASE554 All files read:\n",
      "==============================================================\n",
      "CASE555 All files read:\n",
      "==============================================================\n",
      "CASE557 All files read:\n",
      "==============================================================\n",
      "CASE559 All files read:\n",
      "==============================================================\n",
      "CASE560 All files read:\n",
      "==============================================================\n",
      "CASE562 All files read:\n",
      "==============================================================\n",
      "CASE563 All files read:\n",
      "==============================================================\n",
      "CASE564 All files read:\n",
      "==============================================================\n",
      "CASE565 All files read:\n",
      "==============================================================\n",
      "CASE568 All files read:\n",
      "==============================================================\n",
      "CASE569 All files read:\n",
      "==============================================================\n",
      "CASE572 All files read:\n",
      "==============================================================\n",
      "CASE574 All files read:\n",
      "==============================================================\n",
      "CASE575 All files read:\n",
      "==============================================================\n",
      "CASE577 All files read:\n",
      "==============================================================\n",
      "CASE578 All files read:\n",
      "==============================================================\n",
      "CASE580 All files read:\n",
      "==============================================================\n",
      "CASE581 All files read:\n",
      "==============================================================\n",
      "CASE585 All files read:\n",
      "==============================================================\n",
      "CASE587 All files read:\n",
      "==============================================================\n",
      "CASE588 All files read:\n",
      "==============================================================\n",
      "CASE589 All files read:\n",
      "==============================================================\n",
      "CASE593 All files read:\n",
      "==============================================================\n",
      "CASE594 All files read:\n",
      "==============================================================\n",
      "CASE596 All files read:\n",
      "==============================================================\n",
      "CASE598 All files read:\n",
      "==============================================================\n",
      "CASE600 All files read:\n",
      "==============================================================\n",
      "CASE601 All files read:\n",
      "==============================================================\n",
      "CASE602 All files read:\n",
      "==============================================================\n",
      "CASE603 All files read:\n",
      "==============================================================\n",
      "CASE604 All files read:\n",
      "==============================================================\n",
      "CASE608 All files read:\n",
      "==============================================================\n",
      "CASE610 All files read:\n",
      "==============================================================\n",
      "CASE611 All files read:\n",
      "==============================================================\n",
      "CASE615 All files read:\n",
      "==============================================================\n",
      "CASE616 All files read:\n",
      "==============================================================\n",
      "CASE621 All files read:\n",
      "==============================================================\n",
      "CASE622 All files read:\n",
      "==============================================================\n",
      "CASE623 All files read:\n",
      "==============================================================\n",
      "CASE624 All files read:\n",
      "==============================================================\n",
      "CASE630 All files read:\n",
      "==============================================================\n",
      "CASE632 All files read:\n",
      "==============================================================\n",
      "CASE635 All files read:\n"
     ]
    }
   ],
   "source": [
    "desiredSliceNumber=13\n",
    "allFolders = ['CASE244','CASE246','CASE247','CASE251','CASE254','CASE256','CASE263','CASE264','CASE265','CASE270','CASE272','CASE274',\n",
    "                'CASE467','CASE468','CASE470','CASE471','CASE472','CASE476','CASE479','CASE480','CASE482','CASE484','CASE485','CASE488','CASE494','CASE496','CASE499',\n",
    "                'CASE500','CASE505','CASE515','CASE520','CASE523','CASE525','CASE531','CASE533','CASE534','CASE535','CASE537','CASE539','CASE541','CASE543','CASE546','CASE547','CASE548','CASE549','CASE550','CASE551','CASE554','CASE555','CASE557','CASE559','CASE560','CASE562','CASE563','CASE564','CASE565','CASE568','CASE569','CASE572','CASE574','CASE575','CASE577','CASE578','CASE580','CASE581','CASE585','CASE586','CASE587','CASE588','CASE589','CASE593','CASE594','CASE596','CASE598',\n",
    "                'CASE600','CASE601','CASE602','CASE603','CASE604','CASE605','CASE608','CASE610','CASE611','CASE615','CASE616','CASE621','CASE622','CASE623','CASE624','CASE629','CASE630','CASE632','CASE635']\n",
    "\n",
    "alreadySeen=[]\n",
    "baseFilepath = 'Pre-treatment-only-pres/'\n",
    "\n",
    "croppedSegmentsList = []\n",
    "\n",
    "for folder in os.listdir(baseFilepath):\n",
    "    # Skip cases that are not in the excel sheet\n",
    "    if folder not in cases:\n",
    "        continue\n",
    "    # Exclude to cases that we haven't seen yet\n",
    "    if folder in alreadySeen:\n",
    "        continue \n",
    "    count = 0\n",
    "    for file in os.listdir(os.path.join(baseFilepath,folder)):\n",
    "        \n",
    "        # if 'segmentation' in file or 'segmention' in file: # post-treatment segmentation \n",
    "        #     count+=1\n",
    "        #     postSegmentHeader = sitk.ReadImage(os.path.join(baseFilepath,folder,file))\n",
    "            \n",
    "        if 'TUM' in file or 'SMV' in file: # pre-treatment segmentation \n",
    "            # segment, segmentHeader = nrrd.read(os.path.join(baseFilepath,folder,file))\n",
    "            preSegmentHeader = sitk.ReadImage(os.path.join(baseFilepath,folder,file))\n",
    "        elif file.endswith('CT.nrrd'): # whole ct scan\n",
    "            wholeHeader = sitk.ReadImage(os.path.join(baseFilepath,folder,file))\n",
    "    \n",
    "    print('==============================================================')\n",
    "    print(folder, 'All files read:')\n",
    "      \n",
    "    whole, croppedSegment,error = preprocess(wholeHeader, preSegmentHeader, verbose=0) \n",
    "    if error:\n",
    "        print('Error in preprocessing')\n",
    "        # continue\n",
    "\n",
    "    #Update the number of slices so that it reaches desiredSliceNumber\n",
    "    # updatedCroppedSegment = updateSlices(croppedSegment,desiredSliceNumber)\n",
    "    largestSlice,_ = getLargestSlice(croppedSegment)\n",
    "    updatedCroppedSegment = croppedSegment[:,:,largestSlice]\n",
    "\n",
    "    croppedSegmentsList.append(updatedCroppedSegment)\n",
    "    \n",
    "    # displayCroppedSegmentations(updatedCroppedSegment)\n",
    "\n",
    "    \n",
    "# # For a single case\n",
    "# preSegmentHeader = sitk.ReadImage(os.path.join(baseFilepath,'CASE264/CASE264_BASE_PRT_TUM_CV.seg.nrrd'))\n",
    "# wholeHeader = sitk.ReadImage(os.path.join(baseFilepath,'CASE264/CASE264_BASE_PRT_WHOLE_CT.nrrd'))\n",
    "\n",
    "# whole, croppedSegment,error = preprocess(wholeHeader, preSegmentHeader, verbose=2) \n",
    "# if error:\n",
    "#     print('Error in preprocessing')\n",
    "#     # continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLITING THE DATA INTO TRAIN AND TEST SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xTrain: 72\n",
      "yTrain: 72\n",
      "xTest: 13\n",
      "yTest: 13\n",
      "train recist category split: {1: 16, 2: 30, 3: 24}\n",
      "test recist category split: {1: 4, 2: 7, 3: 4}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(croppedSegmentsList, recistCriteria,test_size=0.15, random_state=42)\n",
    "\n",
    "print(f'xTrain: {len(xTrain)}')\n",
    "print(f'yTrain: {len(yTrain)}')\n",
    "print(f'xTest: {len(xTest)}')\n",
    "print(f'yTest: {len(yTest)}')\n",
    "\n",
    "# VIEWING THE CATEGORIZATION SPLITS OF THE TRAIN AND TEST SET\n",
    "\n",
    "# Add in 2 more progessive disease (1) into the test set\n",
    "for i in range(2):\n",
    "    idx = yTrain.index(1)\n",
    "    xTest.append(xTrain[idx])\n",
    "    yTest.append(yTrain[idx])\n",
    "    xTrain.pop(idx)\n",
    "    yTrain.pop(idx)\n",
    "\n",
    "#count the categorization splits of the train and test set\n",
    "trainRecistSplit = {y: yTrain.count(y) for y in yTrain}\n",
    "testRecistSplit = {y: yTest.count(y) for y in yTest}\n",
    "\n",
    "# Format the splits nicely into an ordered dictionary\n",
    "myKeys = list(trainRecistSplit.keys())\n",
    "myKeys.sort()\n",
    "trainRecistSplitDisplay = {i: trainRecistSplit[i] for i in myKeys}\n",
    "myKeys = list(testRecistSplit.keys())\n",
    "myKeys.sort()\n",
    "testRecistSplitDisplay = {i: testRecistSplit[i] for i in myKeys}\n",
    "\n",
    "print(f'train recist category split: {trainRecistSplitDisplay}')\n",
    "print(f'test recist category split: {testRecistSplitDisplay}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTrain = [x-1 for x in yTrain]\n",
    "yTest = [x-1 for x in yTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 64, 64, 1)\n",
      "(70,)\n",
      "(15, 64, 64, 1)\n",
      "(15,)\n"
     ]
    }
   ],
   "source": [
    "## Working with Numpy arrays\n",
    "xTrain = np.array(xTrain) \n",
    "xTest = np.array(xTest)\n",
    "yTrain = np.array(yTrain)\n",
    "yTest = np.array(yTest)\n",
    "\n",
    "xTrain =  np.expand_dims(xTrain,axis=-1)\n",
    "xTest =  np.expand_dims(xTest,axis=-1)\n",
    "\n",
    "print(xTrain.shape)\n",
    "print(yTrain.shape)\n",
    "print(xTest.shape)\n",
    "print(yTest.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Working with pytorch tensors\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n",
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, images, classifications, transform=None):\n",
    "        self.data = images\n",
    "        self.classification = classifications\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        label = self.classification[idx]\n",
    "        # Convert sample to a tensor\n",
    "        # sample = torch.tensor(sample, dtype=torch.float32).permute(2, 0, 1)  # Change shape to (C, H, W)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "\n",
    "weightsForClasses = [0.33333,0.33333,0.33333]\n",
    "balancedSampler = WeightedRandomSampler(weightsForClasses, BATCHSIZE)\n",
    "    \n",
    "# Define data augmentation transforms\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(50),\n",
    "    transforms.ToTensor()\n",
    "]) \n",
    "trainingData = TorchDataset(xTrain, yTrain, transform=data_transforms)\n",
    "trainingData = DataLoader(trainingData, batch_size=BATCHSIZE, shuffle=False, sampler= balancedSampler)\n",
    "\n",
    "# print('Train Data shape:',trainingData.dataset)\n",
    "# print('Train Classifications shape:',trainingData.dataset.classification.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class ResNet50ClassificaitonModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50ClassificaitonModel, self).__init__()\n",
    "        self.resNet50 = models.resnet50(pretrained=True)\n",
    "        # Modify the first convolutional layer to accept single-channel input\n",
    "        self.resNet50.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.outputLayer = torch.nn.Linear(self.resNet50.fc.out_features, 3)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resNet50(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.outputLayer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "model = ResNet50ClassificaitonModel()\n",
    "# model = models.resnet50(pretrained=True)\n",
    "# # Modify the first convolutional layer to accept single-channel input\n",
    "# model.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "# num_ftrs = model.fc.in_features\n",
    "# num_classes = 3\n",
    "# model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "# print(model.fc.out_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in loader:\n",
    "        # inputs = inputs.type(torch.LongTensor)   # casting to long\n",
    "        labels = labels.type(torch.LongTensor)   # casting to long\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "# Step 9: Validation function\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using this device: cuda\n",
      "Epoch 1/100\n",
      "Train Loss: 1.1271, Train Acc: 0.0000\n",
      "Epoch 2/100\n",
      "Train Loss: 0.5521, Train Acc: 1.0000\n",
      "Epoch 3/100\n",
      "Train Loss: 0.5515, Train Acc: 1.0000\n",
      "Epoch 4/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 5/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 6/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 7/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 8/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 9/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 10/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 11/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 12/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 13/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 14/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 15/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 16/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 17/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 18/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 19/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 20/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 21/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 22/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 23/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 24/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 25/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 26/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 27/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 28/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 29/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 30/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 31/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 32/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 33/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 34/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 35/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 36/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 37/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 38/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 39/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 40/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 41/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 42/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 43/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 44/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 45/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 46/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 47/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 48/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 49/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 50/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 51/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 52/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 53/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 54/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 55/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 56/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 57/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 58/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 59/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 60/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 61/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 62/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 63/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 64/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 65/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 66/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 67/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 68/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 69/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 70/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 71/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 72/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 73/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 74/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 75/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 76/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 77/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 78/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 79/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 80/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 81/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 82/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 83/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 84/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 85/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 86/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 87/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 88/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 89/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 90/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 91/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 92/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 93/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 94/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 95/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 96/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 97/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 98/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 99/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n",
      "Epoch 100/100\n",
      "Train Loss: 0.5514, Train Acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using this device:', device)\n",
    "\n",
    "#Send the model to the same device that the tensors are on\n",
    "model.to(device)\n",
    "\n",
    "train_losses, train_accuracies = [], []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss, train_acc = train(model, trainingData, criterion, optimizer, device)\n",
    "    # val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    # print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the contents of this test\n",
    "os.makedirs(testPathName, exist_ok=True)\n",
    "\n",
    "#Save training history of model\n",
    "\n",
    "#Save history as pickle\n",
    "history = {'train_loss': train_losses, 'train_acc': train_accuracies}\n",
    "with open(testPathName+'/history.pkl', 'wb') as fp:\n",
    "    pickle.dump(history, fp)\n",
    "\n",
    "# Save weigths of model\n",
    "torch.save(model.state_dict(), testPathName+'/model.pt')\n",
    "\n",
    "#Save notebooks and scripts\n",
    "if os.path.exists('imageSegmentationMultipleSingleSlice.py'):\n",
    "    shutil.copy('imageSegmentationMultipleSingleSlice.py',testPathName+'/convertedScript.py')\n",
    "if os.path.exists('imageSegmentationMultipleSingleSlice.ipynb'):\n",
    "    shutil.copy('imageSegmentationMultipleSingleSlice.ipynb',testPathName+'/notebook.ipynb')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_history_from_pickle(testPathName):\n",
    "    with open(testPathName+'/history.pkl', 'rb') as fp:\n",
    "        history = pickle.load(fp)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SimpsonLab\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\SimpsonLab\\.venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and evalaute the model\n",
    "modelWeightPath = testName+'/model.pt'\n",
    "model = ResNet50ClassificaitonModel()\n",
    "model.load_state_dict(torch.load(modelWeightPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use('default')\n",
    "\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.suptitle('Accuracy', fontsize=10)\n",
    "# plt.ylabel('Loss', fontsize=16)\n",
    "# plt.plot(history.history['loss'], label='Training Loss')\n",
    "# # plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "# plt.legend(loc='upper right')\n",
    "\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.ylabel('Accuracy', fontsize=16)\n",
    "# plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "# # plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "# plt.legend(loc='lower right')\n",
    "\n",
    "# plt.savefig(testName+'/training_history.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# predictions = model.predict(xTest, batch_size=1)\n",
    "# y_predict = np.argmax(predictions,axis=1)\n",
    "# predicts = {0:0,1:0,2:0}\n",
    "# for pred in y_predict:\n",
    "#     predicts[pred]+=1\n",
    "# print('Predictions:')\n",
    "# print(predicts)\n",
    "\n",
    "# ans = {0:0,1:0,2:0}\n",
    "# for pred in yTest:\n",
    "#     ans[pred]+=1\n",
    "# print('Actual answers:')\n",
    "# print(ans)\n",
    "\n",
    "# result = confusion_matrix(yTest,y_predict,normalize='pred')\n",
    "# disp = ConfusionMatrixDisplay(result)\n",
    "# disp.plot()\n",
    "# plt.savefig(testName+'/confusion_matrix.png')\n",
    "# plt.show()\n",
    "\n",
    "# accuracy = accuracy_score(yTest, y_predict)\n",
    "# f1 = f1_score(yTest, y_predict, average='weighted')  # Use 'weighted' for multiclass classification\n",
    "# recall = recall_score(yTest, y_predict, average='weighted')  # Use 'weighted' for multiclass classification\n",
    "\n",
    "\n",
    "# testingMetrics = {'Accuracy':accuracy, 'F1 Score':f1, 'Recall':recall}\n",
    "# file = open(testName+'/testingMetrics.txt','w')\n",
    "# for key, value in testingMetrics.items():\n",
    "#     file.write(f'{key}: {value}\\n')\n",
    "# file.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
