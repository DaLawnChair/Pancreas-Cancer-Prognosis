{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook twoClassClassificationMethods.ipynb to python\n",
      "[NbConvertApp] Writing 31230 bytes to twoClassClassificationMethods.py\n"
     ]
    }
   ],
   "source": [
    "# ! jupyter nbconvert --to python twoClassClassificationMethods.ipynb --output twoClassClassificationMethods.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image reading and file handling \n",
    "import pandas as pd\n",
    "import SimpleITK as sitk \n",
    "import os \n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Image agumentaitons \n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Saving History\n",
    "import pickle as pkl\n",
    "\n",
    "# Train test set spliting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# Dataset building\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Model building\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import timm # For Xception model\n",
    "\n",
    "# Evaluation metrics and Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Oversampling\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_worker(worker_id=42): \n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def seed_everything(seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.Generator().manual_seed(seed)\n",
    "\n",
    "randomSeed = 42\n",
    "seed_everything(randomSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying segments\n",
    "#==========================================================================================\n",
    "\n",
    "def displayCroppedSegmentations(croppedSegment):\n",
    "    print(f'CroppedSegment shape: {croppedSegment.shape}')\n",
    "    # Display the segmented image slices \n",
    "\n",
    "    columnLen = 10\n",
    "    rowLen = max(2,croppedSegment.shape[0] // columnLen + 1) \n",
    "    figure,axis = plt.subplots( rowLen, columnLen, figsize=(10, 10))\n",
    "    \n",
    "    rowIdx = 0\n",
    "    for idx in range(croppedSegment.shape[0]):        \n",
    "        if idx%columnLen == 0 and idx>0:\n",
    "            rowIdx += 1        \n",
    "        # axis[rowIdx][idx%columnLen].imshow(croppedSegment[idx,:,:] , cmap=\"gray\", vmin = 40-(350)/2, vmax=40+(350)/2)\n",
    "        axis[rowIdx][idx%columnLen].imshow(croppedSegment[idx,:,:] , cmap=\"gray\")\n",
    "\n",
    "        axis[rowIdx][idx%columnLen].axis('off')\n",
    "\n",
    "    # Turn off the axis of the rest of the subplots\n",
    "    for i in range(idx+1, rowLen*columnLen):\n",
    "        if i%columnLen == 0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][i%columnLen].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def displayOverlayedSegmentations(segmentedSlices, augmented_whole, augmented_segment):\n",
    "    # Display the segmented image slices \n",
    "    columnLen = 10\n",
    "    rowLen = max(2,len(segmentedSlices) // columnLen + 1) \n",
    "    figure,axis = plt.subplots( rowLen, columnLen, figsize=(10, 10))\n",
    "    rowIdx = 0\n",
    "    for idx in range(len(segmentedSlices)):        \n",
    "        if idx%columnLen == 0 and idx>0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][idx%columnLen].imshow(augmented_whole[segmentedSlices[idx],:,:], cmap=\"gray\")\n",
    "        axis[rowIdx][idx%columnLen].imshow(augmented_segment[segmentedSlices[idx],:,:], cmap=\"Blues\", alpha=0.75)\n",
    "        axis[rowIdx][idx%columnLen].axis('off')\n",
    "\n",
    "    # Turn off the axis of the rest of the subplots\n",
    "    for i in range(idx+1, rowLen*columnLen):\n",
    "        if i%columnLen == 0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][i%columnLen].axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting information about the transformations\n",
    "def generateTransform(RandomHorizontalFlipValue=0.5,RandomVerticalFlipValue=0.5, RandomRotationValue=50, RandomElaticTransform=[0,0], brightnessConstant=0, contrastConstant=0, kernelSize=3, sigmaRange=(0.1,1.0)):\n",
    "    training_data_transforms = transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.Resize(299),\n",
    "        transforms.CenterCrop(299),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomRotation(degrees=RandomRotationValue),\n",
    "        transforms.ElasticTransform(alpha=RandomElaticTransform[0], sigma=RandomElaticTransform[1]),\n",
    "        transforms.ColorJitter(brightnessConstant, contrastConstant),\n",
    "        transforms.GaussianBlur(kernel_size = kernelSize, sigma=sigmaRange),\n",
    "        transforms.RandomHorizontalFlip(p=RandomHorizontalFlipValue),\n",
    "        transforms.RandomVerticalFlip(p=RandomVerticalFlipValue),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]) \n",
    "    return training_data_transforms\n",
    "\n",
    "\n",
    "def getTransformValue(transform, desiredTranform, desiredTranformValue):\n",
    "    if transform==None or desiredTranform==None or desiredTranformValue==None:\n",
    "      return None\n",
    "    for t in transform.transforms:\n",
    "        if isinstance(t, desiredTranform):\n",
    "            return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPLIT THE DATA\n",
    "\n",
    "def turnDatasetIntoArrays(dataset):\n",
    "    labels = []\n",
    "    images = []\n",
    "    patients = list(dataset.keys())\n",
    "    for patient in patients:\n",
    "        labels.append([dataset[patient]['label']])\n",
    "        images.append([dataset[patient]['images']]) \n",
    "    return patients, images, labels\n",
    "\n",
    "def underSampleData(dataset):\n",
    "    patients, _, labels = turnDatasetIntoArrays(dataset)\n",
    "    differenceIn0sTo1s = labels.count([torch.tensor(1, dtype=torch.int64)]) - labels.count([torch.tensor(0, dtype=torch.int64)]) \n",
    "\n",
    "    print('previous difference', differenceIn0sTo1s)\n",
    "    indiesToConsiderDropping = []\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] == [torch.tensor(1, dtype=torch.int64)]:\n",
    "            indiesToConsiderDropping.append(i)\n",
    "    \n",
    "    randomIndicies = random.sample(indiesToConsiderDropping, differenceIn0sTo1s)\n",
    "    print(randomIndicies)\n",
    "    print(len(randomIndicies))\n",
    "    \n",
    "    for i in range(len(randomIndicies)):\n",
    "        del dataset[patients[randomIndicies[i]]]\n",
    "\n",
    "    _1, _2, labels = turnDatasetIntoArrays(dataset)\n",
    "    differenceIn0sTo1s = labels.count([torch.tensor(1, dtype=torch.int64)]) - labels.count([torch.tensor(0, dtype=torch.int64)]) \n",
    "    print('New difference after undersampling', differenceIn0sTo1s)\n",
    "    return dataset\n",
    "\n",
    "# def overSampleData(trainFolders):\n",
    "#     smote = SMOTE(random_state=randomSeed)\n",
    "#     images = np.array()\n",
    "#     patients, images, labels = turnDatasetIntoArrays(trainData)\n",
    "#     images = np.array(images)\n",
    "\n",
    "#     # Get the 1D image\n",
    "#     if len(images[0].shape)==3:\n",
    "#         oneDShape = images[0].shape[0]*images[0].shape[1]\n",
    "#     else:\n",
    "#         oneDShape = images[0].shape[0]*images[0].shape[1]*images[0].shape[2]\n",
    "\n",
    "#     singleShape = images[0].shape\n",
    "\n",
    "#     # Use SMOTE to oversample the data\n",
    "#     print('images reshape',images.reshape(images.shape[0],oneDShape).shape)\n",
    "#     imagesSmote, labels = smote.fit_resample(images.reshape(images.shape[0],oneDShape), labels)\n",
    "#     if len(images.shape)==3:\n",
    "#         images = imagesSmote.reshape(imagesSmote.shape[0], images[0].shape[0],images[0].shape[1])\n",
    "#     else:\n",
    "#         images = imagesSmote.reshape(imagesSmote.shape[0], images[0].shape[0],images[0].shape[1],images[0].shape[2])\n",
    "\n",
    "#     #Make a new train dataset that is oversampled now\n",
    "#     newTrainData = {}\n",
    "#     for i in range(images.shape[0]):\n",
    "#         newTrainData[str(i)] = {'images':images[i], 'label':labels[i]}\n",
    "\n",
    "#     print('Dataset after Smote', getDatasetShape(newTrainData))\n",
    "#     from collections import Counter\n",
    "#     counter  = Counter(labels)\n",
    "#     print('Splits for training',sorted(counter.items()))\n",
    "\n",
    "#     return newTrainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2D images:\n",
    "# ## Working with pytorch tensors\n",
    "import copy\n",
    "\n",
    "class PatientData(Dataset):\n",
    "    def __init__(self, patientsList, allData, grouped2D, segmentsMultiple, transform=None):\n",
    "        self.patients = patientsList\n",
    "\n",
    "        # Make the dataset only contain its patients\n",
    "        self.setData = copy.deepcopy(allData)        \n",
    "        allPatients = allData.keys()\n",
    "        for patient in allPatients:\n",
    "            if patient not in self.patients:\n",
    "                del self.setData[patient]\n",
    "\n",
    "        self.transform = transform\n",
    "        self.grouped2D = grouped2D\n",
    "        self.segmentsMultiple = segmentsMultiple\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patients)* self.segmentsMultiple\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.grouped2D==False:\n",
    "            image = self.setData[self.patients[idx]]['images']\n",
    "            label = self.setData[self.patients[idx]]['label']\n",
    "        else:\n",
    "            patient_idx = idx // self.segmentsMultiple\n",
    "            slice_idx = idx % self.segmentsMultiple\n",
    "\n",
    "            image = self.setData[self.patients[patient_idx]]['images'][slice_idx]\n",
    "            label = self.setData[self.patients[patient_idx]]['label']\n",
    "\n",
    "        # Convert to RGB\n",
    "        image = Image.fromarray((image * 255).astype(np.uint16))\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "        # Apply augmentations if there are any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)#.type(torch.float)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "def convertDataToLoaders(trainPatientList, valPatientList,testPatientList, allData, model, grouped2D, segmentsMultiple, training_data_transforms = None, batchSize=8):\n",
    "    \n",
    "    # Testing data tranfrom, should be just the plain images\n",
    "    if model == 'ResNet50Small2D':\n",
    "        testing_data_transforms = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]) \n",
    "    \n",
    "    elif model == 'VGG16Small2D':\n",
    "        testing_data_transforms = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize( mean=[0.48235, 0.45882, 0.40784], std=[0.00392156862745098, 0.00392156862745098, 0.00392156862745098])\n",
    "        ]) \n",
    "    \n",
    "    elif model == 'InceptionV3Small2D':\n",
    "        testing_data_transforms = transforms.Compose([\n",
    "            transforms.Resize(299),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]) \n",
    "\n",
    "    elif model == 'XceptionSmall2D':\n",
    "        testing_data_transforms = transforms.Compose([\n",
    "            transforms.Resize(299),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "        ]) \n",
    "        \n",
    "    # Use the same default training transform as the testing transform if not specified\n",
    "    if training_data_transforms == None:\n",
    "        training_data_transforms = testing_data_transforms\n",
    "        \n",
    "    # Convert the testing sets to data loaders\n",
    "    trainingData = PatientData(trainPatientList, allData, grouped2D, segmentsMultiple, transform=training_data_transforms)\n",
    "\n",
    "    # Special case because models with batch normalization layers do not accept ununiform sizes across batches\n",
    "    if segmentsMultiple == 1:\n",
    "        trainingData = DataLoader(trainingData, batch_size=batchSize, shuffle=True, worker_init_fn=seed_worker, drop_last=True)\n",
    "    else:\n",
    "        trainingData = DataLoader(trainingData, batch_size=batchSize, shuffle=True, worker_init_fn=seed_worker)#, sampler= TrainBalancedSampler)\n",
    "\n",
    "    validationData = PatientData(valPatientList, allData, grouped2D, segmentsMultiple, transform=testing_data_transforms)\n",
    "    validationData = DataLoader(validationData, batch_size=batchSize, shuffle=False, worker_init_fn=seed_worker)\n",
    "\n",
    "    testingData = PatientData(testPatientList, allData, grouped2D, segmentsMultiple, transform=testing_data_transforms)\n",
    "    testingData = DataLoader(testingData, batch_size=batchSize, shuffle=False, worker_init_fn=seed_worker)\n",
    "\n",
    "    return trainingData, validationData, testingData, training_data_transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Models and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV3Small2D(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InceptionV3Small2D, self).__init__()\n",
    "\n",
    "        # inceptionv3 as first layer\n",
    "        self.model = models.inception_v3(pretrained=True)\n",
    "\n",
    "        # Modify the final fully connected layer\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.model.training:\n",
    "            x = self.model(x)[0]\n",
    "        else:\n",
    "            x = self.model(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x \n",
    "        \n",
    "class XceptionSmall2D(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XceptionSmall2D, self).__init__()\n",
    "\n",
    "        # Resnet50 as first layer\n",
    "        self.model = timm.create_model('xception', pretrained=True, num_classes=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "class VGG16Small2D(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG16Small2D, self).__init__()\n",
    "\n",
    "        # vgg16 as first layer\n",
    "        self.model = models.vgg16(weights='DEFAULT') \n",
    "        \n",
    "        # Modify the final fully connected layer\n",
    "        num_features = self.model.classifier[-1].in_features\n",
    "        self.model.classifier[-1] = nn.Linear(num_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x \n",
    "    \n",
    "\n",
    "class ResNet50Small2D(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet50Small2D, self).__init__()\n",
    "\n",
    "        #Resnet50 as first layer\n",
    "        self.model = models.resnet50(weights='IMAGENET1K_V2')\n",
    "\n",
    "        # Modify the final fully connected layer\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x \n",
    "    \n",
    "def defineModel(learningRate=0.001, weight_decay=0.01, model = 'ResNet50Small2D'):\n",
    "    if model == 'ResNet50Small2D':\n",
    "        model = ResNet50Small2D()\n",
    "    elif model == 'VGG16Small2D':\n",
    "        model = VGG16Small2D()\n",
    "    elif model == 'InceptionV3Small2D':\n",
    "        model = InceptionV3Small2D()\n",
    "    elif model == 'XceptionSmall2D':\n",
    "        model = XceptionSmall2D()\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learningRate, weight_decay=weight_decay)\n",
    "\n",
    "    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    return model, criterion, scheduler, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, scheduler, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).view(-1, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = outputs > 0.5\n",
    "        total += labels.size(0)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        correct += torch.sum(predicted == labels.data).item()\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, minDelta=0):\n",
    "        self.patience = patience\n",
    "        self.minDelta = minDelta\n",
    "        self.counter = 0\n",
    "        self.minValLoss = float('inf')\n",
    "        \n",
    "    def earlyStoppingCheck(self, currValLoss):\n",
    "        if np.isnan(currValLoss):\n",
    "            return True\n",
    "        if currValLoss < self.minValLoss:\n",
    "            self.minValLoss = currValLoss\n",
    "            self.counter = 0\n",
    "        elif currValLoss >= self.minValLoss + self.minDelta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).view(-1, 1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            predicted = outputs > 0.5\n",
    "            total += labels.size(0)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            correct += torch.sum(predicted == labels.data).item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, criterion, scheduler, optimizer, trainingData, validationData, patience=10,numOfEpochs=100):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using this device:', device)\n",
    "    #Send the model to the same device that the tensors are on\n",
    "    model.to(device)\n",
    "\n",
    "    earlyStopping = EarlyStopping(patience=patience, minDelta=0)\n",
    "    train_loss, train_acc = [], []\n",
    "    val_loss, val_acc = [], []\n",
    "\n",
    "    for epoch in range(numOfEpochs):\n",
    "        #Train model\n",
    "        curTrainLoss, curTrainAcc = train(model, trainingData, criterion, scheduler, optimizer, device)    \n",
    "        print(f\"Epoch {epoch+1}/{numOfEpochs}\")\n",
    "        print(f\"Train Loss: {curTrainLoss:.4f}, Train Acc: {curTrainAcc:.4f}\")\n",
    "        #Evaluate on validation set\n",
    "        curValLoss, curValAcc, _ = evaluate(model, validationData, criterion, device)    \n",
    "        print(f\"Val Loss: {curValLoss:.4f}, Val Acc: {curValAcc:.4f}\")\n",
    "\n",
    "        #Append metrics to lists\n",
    "        train_loss.append(curTrainLoss)\n",
    "        train_acc.append(curTrainAcc)\n",
    "        val_loss.append(curValLoss)\n",
    "        val_acc.append(curValAcc)\n",
    "\n",
    "        #Check for early stopping conditions\n",
    "        if earlyStopping.earlyStoppingCheck(curValLoss):\n",
    "            print(f'Early stopping - Val loss has not decreased in {earlyStopping.patience} epochs. Terminating training at epoch {epoch+1}.')\n",
    "            break\n",
    "\n",
    "    history = {'train_loss':train_loss, 'train_acc':train_acc, 'val_loss':val_loss, 'val_acc':val_acc}\n",
    "    print('Done Training')\n",
    "    return model, criterion, device, history, epoch+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_history_from_pickle(testPathName):\n",
    "#     with open(testPathName+'/history.pkl', 'rb') as fp:\n",
    "#         history = pickle.load(fp)\n",
    "#     return history\n",
    "\n",
    "# #Read history\n",
    "# history = read_history_from_pickle(testPathName)\n",
    "\n",
    "# # Load and evalaute the model\n",
    "# modelWeightPath = testPathName+'/model.pt'\n",
    "# model = ResNet50ClassificaitonModel()\n",
    "# model.load_state_dict(torch.load(modelWeightPath))\n",
    "\n",
    "#Send the model to the device used\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print('Using this device:', device)\n",
    "# #Send the model to the same device that the tensors are on\n",
    "# model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAVE CONTENTS\n",
    "def saveResults(testPathName, model, history, training_data_transforms, saveModel=True):\n",
    "    os.makedirs(testPathName, exist_ok=True)\n",
    "\n",
    "    #Save history as pickle\n",
    "    with open(testPathName+'/history.pkl', 'wb') as fp:\n",
    "        pkl.dump(history, fp)\n",
    "\n",
    "    # Save weigths of model\n",
    "    if saveModel:\n",
    "        torch.save(model.state_dict(), testPathName+'/model.pt')\n",
    "\n",
    "    # Save transformations for easy access\n",
    "    f = open(testPathName + '/training_data_transforms.txt', 'w')\n",
    "            \n",
    "    for line in training_data_transforms.__str__():\n",
    "        f.write(line)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeDictionaryToTxtFile(filePath,dictionary, printLine=False):\n",
    "    f = open(filePath, 'w')\n",
    "    for key, value in dictionary.items():\n",
    "        f.write(f'{key}: {value}\\n')\n",
    "        if printLine:\n",
    "            print(f'{key}: {value}')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EVALUATE PERFORMANCE ON TESTING SET\n",
    "def formatDataFromGroupVoting(outputs):\n",
    "    ## Outputs are in the format of [array([[False],[False],[False],[False],[False],[False], ....]), array([[False], .....)]\n",
    "    return [pred.tolist()[0] for sublist in outputs for pred in sublist]\n",
    "\n",
    "def evaluateGroupVoting(model, loader, criterion, device, votingSystem, segmentsMultiple):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    correctLabelsTemp = []\n",
    "    correctLabels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).view(-1, 1)\n",
    "\n",
    "            labels = labels > 0.5\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            predicted = outputs > 0.5\n",
    "\n",
    "            probabilities.append(outputs.cpu().numpy())\n",
    "            predictions.append(predicted.cpu().numpy())\n",
    "            correctLabelsTemp.append(labels.cpu().numpy())\n",
    "    \n",
    "    # Ignores grouped voting\n",
    "    probabilities = formatDataFromGroupVoting(probabilities)\n",
    "    predictions = formatDataFromGroupVoting(predictions)\n",
    "    correctLabelsTemp = formatDataFromGroupVoting(correctLabelsTemp)\n",
    "\n",
    "    if votingSystem=='singleLargest' or segmentsMultiple==1:\n",
    "        return probabilities, predictions, correctLabelsTemp\n",
    "\n",
    "    ## Grouping predictions by patient\n",
    "    ## ==============================================================\n",
    "    # all_probs = np.concatenate(probabilities, axis=0)\n",
    "\n",
    "    # Grouping predictions by patient\n",
    "    patient_probs = [] # The confidence of the model\n",
    "    patient_labels = [] # the label given by the model. >=0.5 is 1, <0.5 is 0\n",
    "     \n",
    "    \n",
    "    ## Get the classification based from the patient based on ...\n",
    "    for i in range(0, len(probabilities), segmentsMultiple):\n",
    "        # Get probabilties and labels for the patient\n",
    "        patient_prob = probabilities[i:i + segmentsMultiple]\n",
    "        patient_pred_labels = [pred>=0.5 for pred in patient_prob]\n",
    "\n",
    "        correctLabel = correctLabelsTemp[i]\n",
    "        correctLabels.append(correctLabel)\n",
    "\n",
    "        ## Average all confidences to get the highest probability label, used as a tie breaker\n",
    "        patient_prob = np.mean(patient_prob) \n",
    "        prob_label_max = patient_prob >= 0.5 \n",
    "        \n",
    "        ## MAJOURITY VOTING\n",
    "        ##==============================================================\n",
    "        # Get counts for the labels \n",
    "        if votingSystem=='majority':\n",
    "            predictedLabels = {True:0,False:0}\n",
    "\n",
    "            uniqueLabels, label_counts = np.unique(patient_pred_labels, return_counts=True)\n",
    "\n",
    "            for i in range(len(uniqueLabels)):\n",
    "                predictedLabels[uniqueLabels[i]] = label_counts[i]\n",
    "\n",
    "            majorityVote = True if predictedLabels[True] > predictedLabels[False] else False\n",
    "\n",
    "            #Tie breaker based on the highest probability\n",
    "            if predictedLabels[False] == predictedLabels[True]:\n",
    "                majorityVote = prob_label_max\n",
    "            patient_probs.append(patient_prob)\n",
    "            patient_labels.append(majorityVote)\n",
    "        ##==============================================================\n",
    "        ## AVERAGE\n",
    "        ##==============================================================\n",
    "        elif votingSystem=='average':\n",
    "            patient_probs.append(patient_prob)\n",
    "            patient_labels.append(prob_label_max)\n",
    "\n",
    "    return patient_probs, patient_labels, correctLabels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModelOnTestSet(testPathName, model, testingData, criterion, device, votingSystem, segmentsMultiple, saveConfusionMatrix = True, showConfusionMatrix=True, showROCCurve=True, saveROCCurve=True):\n",
    "    predictProbs, predictions, ans = evaluateGroupVoting(model, testingData, criterion, device, votingSystem, segmentsMultiple)\n",
    "    \n",
    "    predictsTotal = dict(zip([0,1],[predictions.count(False),predictions.count(True)]))\n",
    "    ansTotal = dict(zip([0,1],[ans.count(False),ans.count(True)]))\n",
    "\n",
    "    print('ans length',len(ans))\n",
    "\n",
    "    # Test metrics\n",
    "    print('---------------------------------------\\nTesting Metrics')\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(ans, predictions)\n",
    "    f1 = list(metrics.f1_score(ans, predictions, average=None))  \n",
    "    recall = list(metrics.recall_score(ans, predictions, average=None))  \n",
    "    precision = list(metrics.precision_score(ans, predictions, average=None))\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(ans, predictProbs, pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr,tpr)\n",
    "    rocCurveDisplay = metrics.RocCurveDisplay(fpr=fpr,tpr=tpr,roc_auc=roc_auc, pos_label=1)\n",
    "\n",
    "    \n",
    "    testingMetrics = {'PredictionSplits': predictsTotal, 'AnswerSplits': ansTotal, 'Predictions': [1 if ans else 0 for ans in predictions], 'Answers    ': [ 1 if ans else 0 for ans in ans],  \n",
    "                      'Accuracy':accuracy, 'F1':f1, 'Recall':recall, 'Precision':precision, 'ROC-AUC': roc_auc, 'fpr': fpr, 'tpr': tpr, 'thresholds': thresholds}\n",
    "\n",
    "    if showROCCurve:\n",
    "        plt.close()\n",
    "        rocCurveDisplay.plot()\n",
    "    \n",
    "    if saveROCCurve:\n",
    "        plt.savefig(testPathName+'/rocCurve.png')\n",
    "\n",
    "    file = open(testPathName+'/testingMetrics.txt','w')\n",
    "    for key, value in testingMetrics.items():\n",
    "        file.write(f'{key}: {value}\\n')\n",
    "    file.close()\n",
    "\n",
    "\n",
    "    for key, value in testingMetrics.items():\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "\n",
    "    print('---------------------------------------\\nConfusion Matrix:')\n",
    "    # Confusion Matrix\n",
    "    confusionMatrixResult = confusion_matrix(ans,predictions,normalize='pred')\n",
    "    \n",
    "    confusionMatrixDisp = ConfusionMatrixDisplay(confusionMatrixResult)\n",
    "    if showConfusionMatrix:\n",
    "        plt.close()\n",
    "        confusionMatrixDisp.plot()\n",
    "\n",
    "    if saveConfusionMatrix:\n",
    "        plt.savefig(testPathName+'/confusion_matrix.png')\n",
    "    \n",
    "    return confusionMatrixDisp, rocCurveDisplay, testingMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOT TRAINING AND CONFUSION MATRICIES\n",
    "def plotTraining(testPathName, testName, history, saveFigure=True, showResult=True):\n",
    "    plt.style.use('default')\n",
    "    \n",
    "    figure, ax = plt.subplots( 1, 2, figsize=(10, 5))\n",
    "    # plt.suptitle('Accuracy', fontsize=10)\n",
    "    ax[0].set_title(\"Loss\")\n",
    "    ax[0].set_ylabel('Loss', fontsize=16)\n",
    "    ax[0].set_xlabel('Epoch', fontsize=16)\n",
    "    ax[0].plot(history['train_loss'], label='Training Loss')\n",
    "    ax[0].plot(history['val_loss'], label='Validation Loss')\n",
    "    ax[0].legend(loc='upper right')\n",
    "\n",
    "    ax[1].set_title(\"Accuracy\")\n",
    "    ax[1].set_ylabel('Accuracy', fontsize=16)\n",
    "    ax[1].set_xlabel('Epoch', fontsize=16)\n",
    "    \n",
    "    ax[1].plot(history['train_acc'], label='Training Accuracy')\n",
    "    ax[1].plot(history['val_acc'], label='Validation Accuracy')\n",
    "    ax[1].legend(loc='lower right')\n",
    "\n",
    "    if saveFigure:\n",
    "        plt.savefig(testPathName+'/training_history.png')\n",
    "    \n",
    "    if showResult:\n",
    "        plt.close()\n",
    "        plt.show()\n",
    "\n",
    "    return figure\n",
    "\n",
    "\n",
    "\n",
    "def plotTrainingPerformances(testPathName, testName, histories, saveFigure=True, showResult=True):\n",
    "    plt.style.use('default')\n",
    "\n",
    "    figure, ax = plt.subplots( 2, len(histories), figsize=(40, 20))\n",
    "    for idx, history in enumerate(histories):\n",
    "        # plt.suptitle('Accuracy', fontsize=10)\n",
    "        ax[0][idx].set_title(\"Loss\")\n",
    "        ax[0][idx].set_ylabel('Loss', fontsize=16)\n",
    "        ax[0][idx].set_xlabel('Epoch', fontsize=16)\n",
    "        ax[0][idx].plot(history['train_loss'], label='Training Loss')\n",
    "        ax[0][idx].plot(history['val_loss'], label='Validation Loss')\n",
    "        ax[0][idx].legend(loc='upper right')\n",
    "\n",
    "        ax[1][idx].set_title(\"Accuracy\")\n",
    "        ax[1][idx].set_ylabel('Accuracy', fontsize=16)\n",
    "        ax[1][idx].set_xlabel('Epoch', fontsize=16)\n",
    "        ax[1][idx].plot(history['train_acc'], label='Training Accuracy')\n",
    "        ax[1][idx].plot(history['val_acc'], label='Validation Accuracy')\n",
    "        ax[1][idx].legend(loc='lower right')\n",
    "\n",
    "    plt.suptitle(f'{testName} \\nTrainining Performance', fontsize=30)\n",
    "\n",
    "    if saveFigure:\n",
    "        plt.savefig(testPathName+'/training_histories.png', format='png')\n",
    "    \n",
    "    if showResult:\n",
    "        plt.close()\n",
    "        plt.show()\n",
    "\n",
    "    return figure\n",
    "\n",
    "def plotConfusionMatricies(testPathName, testName, confusion_matricies, showMatricies=True):\n",
    "    figure,axis = plt.subplots(1,len(confusion_matricies),figsize=(30, 5))\n",
    "    for idx in range(len(confusion_matricies)):        \n",
    "        confusion_matricies[idx].plot(ax=axis[idx])\n",
    "        confusion_matricies[idx].im_.colorbar.remove()\n",
    "\n",
    "    figure.suptitle(f'{testName}\\nConfusion Matricies')\n",
    "    plt.savefig(testPathName+'/confusion_matrix.png')\n",
    "    if showMatricies:\n",
    "        plt.close()\n",
    "        plt.show()\n",
    "\n",
    "def plotROCCurves(testPathName, testName, rocCurves, showMatricies=True):\n",
    "    figure,axis = plt.subplots(1,len(rocCurves),figsize=(30, 5))\n",
    "    for idx in range(len(rocCurves)):        \n",
    "        rocCurves[idx].plot(ax=axis[idx])\n",
    "        #confusion_matricies[idx].im_.colorbar.remove()\n",
    "\n",
    "    figure.suptitle(f'{testName}\\nROC-AUC curves')\n",
    "    plt.savefig(testPathName+'/ROC-AUC.png')\n",
    "    if showMatricies:\n",
    "        plt.close()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AVERAGES CALCULATIONS\n",
    "\n",
    "def formatValues(value, significantDigits=4):\n",
    "    return round(value,significantDigits)\n",
    "\n",
    "def meanConfidenceInterval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "\n",
    "    min = np.min(a)\n",
    "    max = np.max(a)\n",
    "    return [formatValues(m), formatValues(h), [formatValues(val) for val in data]]\n",
    "\n",
    "def averagePredictionTotals(predictions, numberOfTrials=5):\n",
    "    average = {0:0,1:0}\n",
    "    for prediction in predictions:\n",
    "        for key, value in prediction.items():\n",
    "            average[key] += value\n",
    "    \n",
    "    for key,value in average.items():\n",
    "        average[key] = formatValues(value/numberOfTrials)\n",
    "\n",
    "    return average\n",
    "\n",
    "def averageMultilabelMetricScores(scores, numberOfTrials=5, numberOfClasses=2):\n",
    "    dict = {0:0,1:0}\n",
    "    averages= [0]*numberOfClasses\n",
    "    for score in scores:\n",
    "        for i in range(numberOfClasses):\n",
    "            averages[i] += score[i]\n",
    "    averages = [averages[i]/numberOfTrials for i in range(numberOfClasses)]\n",
    "    \n",
    "    for key in range(numberOfClasses):\n",
    "        dict[key] = formatValues(averages[key])\n",
    "\n",
    "    mean = np.mean(averages)\n",
    "    return [formatValues(mean), dict, [ [formatValues(val) for val in score] for score in scores]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## APPEND RESULTS TO XLSX\n",
    "def addEvalDetailToModel(evalDetailLine, dataframe):\n",
    "    exportValue = [evalDetailLine]\n",
    "    dataframe.loc[dataframe.shape[0]] = exportValue + ['']* (len(dataframe.columns) - len(exportValue))\n",
    "\n",
    "def appendMetricsToXLSX(evalDetailLine, testName, kFoldsTestMetrics, dataframe):\n",
    "\n",
    "    predictionSplits = f\"{kFoldsTestMetrics['PredictionSplits']}\"\n",
    "    average = f\"{kFoldsTestMetrics['Accuracy'][0]} ± {kFoldsTestMetrics['Accuracy'][1]}\"\n",
    "    f1 = f\"{kFoldsTestMetrics['F1'][0]}, {kFoldsTestMetrics['F1'][1]}\"\n",
    "    recall = f\"{kFoldsTestMetrics['Recall'][0]}, {kFoldsTestMetrics['Recall'][1]}\"\n",
    "    precision = f\"{kFoldsTestMetrics['Precision'][0]}, {kFoldsTestMetrics['Precision'][1]}\"\n",
    "    rocAuc = f\"{kFoldsTestMetrics['ROC-AUC'][0]} ± {kFoldsTestMetrics['ROC-AUC'][1]}\"\n",
    "    endingEpochs = f\"{kFoldsTestMetrics['endingEpochs']}\"\n",
    "    accuracyData = f\"{kFoldsTestMetrics['Accuracy'][2]}\"\n",
    "    f1Data = f\"{kFoldsTestMetrics['F1'][2]}\"\n",
    "    recallData = f\"{kFoldsTestMetrics['Recall'][2]}\"\n",
    "    precisionData = f\"{kFoldsTestMetrics['Precision'][2]}\"\n",
    "    rocAUCData = f\"{kFoldsTestMetrics['ROC-AUC'][2]}\"\n",
    "    \n",
    "    exportValue = [evalDetailLine, testName, predictionSplits, average, f1, recall, precision, rocAuc, endingEpochs, accuracyData, f1Data, recallData, precisionData, rocAUCData]\n",
    "  \n",
    "    dataframe.loc[dataframe.shape[0]] = exportValue "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "threeDresearchPip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
