{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert to python script, remember to delete/comment the next line in the actual file\n",
    "# ! jupyter nbconvert --to python imageSegmentation2Classes.ipynb --output testSamples2-8.py\n",
    "\n",
    "# # Run the notebook in Simpson GPU server\n",
    "# CUDA_VISIBLE_DEVICES=0 python testSamples2-8.py -batchSize=16 -epochs=100 -lr=0.001 -evalDetailLine=\"majourity voting on smote with 2 clases\" -hasBackground=f -usesLargestBox=f -segmentsMultiple=12 -dropoutRate=0.2 -grouped2D=t -modelChosen='Small2D' -votingSystem='majority'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image reading and file handling \n",
    "import pandas as pd\n",
    "import SimpleITK as sitk \n",
    "import os \n",
    "import shutil\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Image agumentaitons \n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "# Saving History\n",
    "import pickle\n",
    "\n",
    "# Train test set spliting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# Dataset building\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Model building\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation metrics and Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Oversampling\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\n"
     ]
    }
   ],
   "source": [
    "# ! pip freeze > requirements.txt\n",
    "# ! pip uninstall -y -r requirements.txt\n",
    "\n",
    "## Make a python environment\n",
    "# ! python3.8 -m venv threeDresearchPip\n",
    "\n",
    "## Download necessary packages \n",
    "# ! pip install matplotlib opencv-python scipy simpleitk pandas openpyxl scikit-learn nbconvert imblearn\n",
    "# ! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \n",
    "\n",
    "## May need to download networkx 3.1 because of older python version of torch\n",
    "# ! pip install networkx==3.1\n",
    "\n",
    "## For 3D image classification\n",
    "# ! pip install foundation-cancer-image-biomarker -qq\n",
    "# ! pip install foundation-cancer-image-biomarker\n",
    "# ! pip3 install torchio\n",
    "\n",
    "## In case pip breaks \n",
    "# ! python -m ensurepip --upgrade\n",
    "\n",
    "## Check python version and packages\n",
    "# ! python --version\n",
    "# ! pip3 freeze > research3D.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data from the .xsxl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patients: 89\n",
      "Slice range: 6-6\n",
      "Distribution of classes previously:\n",
      "[(1, 21), (2, 40), (3, 28)]\n",
      "Distribution of classes after combining progressive disease with stable disease:\n",
      "[(1, 61), (2, 28)]\n"
     ]
    }
   ],
   "source": [
    "columns = ['TAPS_CaseIDs_PreNAT','RECIST_PostNAT', 'Slice_Thickness']\n",
    "data = pd.read_excel('PDAC-Response_working.xlsx', header=None,names = columns)\n",
    "data.drop(0, inplace=True) # Remove the header row\n",
    "data=data.sort_values(by=['TAPS_CaseIDs_PreNAT'])\n",
    "data.drop(data[data['Slice_Thickness'] < 6].index, inplace = True)\n",
    "\n",
    "\n",
    "# # Get the entire datasheet\n",
    "cases = list(data['TAPS_CaseIDs_PreNAT'])\n",
    "recistCriteria = list(data['RECIST_PostNAT'])\n",
    "\n",
    "sliceThickness = list(data['Slice_Thickness'])\n",
    "sliceThickness.sort()\n",
    "print(f'Number of patients: {len(sliceThickness)}\\nSlice range: {min(sliceThickness)}-{min(sliceThickness)}')\n",
    "\n",
    "\n",
    "print(f'Distribution of classes previously:')\n",
    "counter = Counter(recistCriteria)\n",
    "print(sorted(counter.items()))\n",
    "\n",
    "# recistNames = [ 'Progressive \\nDisease','Stable \\nDisease','Partial \\nResponse'] \n",
    "# names = ['Not evaluated/\\nNot recorded', 'Progressive \\nDisease','Stable \\nDisease','Partial \\nResponse','Complete \\nResponse'] \n",
    "\n",
    "## Combine the progressive disease and stable disease\n",
    "# Dataset does not have 0 or else cases\n",
    "for i in range(len(recistCriteria)):\n",
    "    if recistCriteria[i] == 0: # Not evaluated\n",
    "        recistCriteria[i] = 0\n",
    "    elif recistCriteria[i] == 1: # Progressive disease\n",
    "        recistCriteria[i] = 1\n",
    "    elif recistCriteria[i] == 2: # Stable disease\n",
    "        recistCriteria[i] = 1\n",
    "    elif recistCriteria[i] == 3: # Partial Response\n",
    "        recistCriteria[i] = 2\n",
    "    elif recistCriteria[i] == 4: # Complete Response\n",
    "        recistCriteria[i] = 2 \n",
    "    else: # Other\n",
    "        recistCriteria[i] = 0    \n",
    "\n",
    "print(f'Distribution of classes after combining progressive disease with stable disease:')\n",
    "counter = Counter(recistCriteria)\n",
    "print(sorted(counter.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying segments\n",
    "#==========================================================================================\n",
    "\n",
    "def displayCroppedSegmentations(croppedSegment):\n",
    "    print(f'CroppedSegment shape: {croppedSegment.shape}')\n",
    "    # Display the segmented image slices \n",
    "\n",
    "    columnLen = 10\n",
    "    rowLen = max(2,croppedSegment.shape[0] // columnLen + 1) \n",
    "    figure,axis = plt.subplots( rowLen, columnLen, figsize=(10, 10))\n",
    "    \n",
    "    rowIdx = 0\n",
    "    for idx in range(croppedSegment.shape[0]):        \n",
    "        if idx%columnLen == 0 and idx>0:\n",
    "            rowIdx += 1        \n",
    "        # axis[rowIdx][idx%columnLen].imshow(croppedSegment[idx,:,:] , cmap=\"gray\", vmin = 40-(350)/2, vmax=40+(350)/2)\n",
    "        axis[rowIdx][idx%columnLen].imshow(croppedSegment[idx,:,:] , cmap=\"gray\")\n",
    "\n",
    "        axis[rowIdx][idx%columnLen].axis('off')\n",
    "\n",
    "    # Turn off the axis of the rest of the subplots\n",
    "    for i in range(idx+1, rowLen*columnLen):\n",
    "        if i%columnLen == 0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][i%columnLen].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def displayOverlayedSegmentations(segmentedSlices, augmented_whole, augmented_segment):\n",
    "    # Display the segmented image slices \n",
    "    columnLen = 10\n",
    "    rowLen = max(2,len(segmentedSlices) // columnLen + 1) \n",
    "    figure,axis = plt.subplots( rowLen, columnLen, figsize=(10, 10))\n",
    "    rowIdx = 0\n",
    "    for idx in range(len(segmentedSlices)):        \n",
    "        if idx%columnLen == 0 and idx>0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][idx%columnLen].imshow(augmented_whole[segmentedSlices[idx],:,:], cmap=\"gray\")\n",
    "        axis[rowIdx][idx%columnLen].imshow(augmented_segment[segmentedSlices[idx],:,:], cmap=\"Blues\", alpha=0.75)\n",
    "        axis[rowIdx][idx%columnLen].axis('off')\n",
    "\n",
    "    # Turn off the axis of the rest of the subplots\n",
    "    for i in range(idx+1, rowLen*columnLen):\n",
    "        if i%columnLen == 0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][i%columnLen].axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform preprocessing on multiple images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current System: d:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\ipykernel_launcher.py\n",
      "BatchSize: 8, Epochs: 100, Learning Rate: 0.001, Eval Detail Line: , Has Background: False, Uses Largest Box: True, Segments Multiple: 12,       Dropout Rate: 0.2, Grouped2D: True, weightDecay: 0.01, modelChosen: Small2D, votignSystem: majourity\n"
     ]
    }
   ],
   "source": [
    "#ADD argparser\n",
    "import argparse\n",
    "import sys\n",
    "print('Current System:',sys.argv[0])\n",
    "\n",
    "\n",
    "# python testSamples2-8.py -batchSize=8 -epochs=100 -lr=0.001 -evalDetailLine=\"majourity voting on new data\" -hasBackground=f -usesLargestBox=f -segmentsMultiple=12 -dropoutRate=0.2 -grouped2D=t -modelChosen='Small2D' -votingSystem='majority'\n",
    "\n",
    "\n",
    "#Check if we are using a notebook or not\n",
    "if 'ipykernel_launcher' in sys.argv[0]:\n",
    "    batchSize = 8\n",
    "    numOfEpochs = 100\n",
    "    evalDetailLine = \"\"\n",
    "    learningRate = 0.001\n",
    "    hasBackground = False\n",
    "    usesLargestBox = True\n",
    "    segmentsMultiple = 12\n",
    "    dropoutRate = 0.2\n",
    "    grouped2D = True\n",
    "    weight_decay = 0.01\n",
    "    modelChosen = 'Small2D' #Large2D, Small2D\n",
    "    votingSystem = 'majourity'\n",
    "\n",
    "else:\n",
    "    parser = argparse.ArgumentParser(description=\"Model information\")\n",
    "    parser.add_argument('-batchSize', type=int, help='batch size', default=8)\n",
    "    parser.add_argument('-epochs', type=int, help='Number of Epochs', default=100)\n",
    "    parser.add_argument('-lr', type=float, help='Learning Rate', default=0.001)\n",
    "    parser.add_argument('-evalDetailLine', type=str, help='Details of the evaluation', default='')\n",
    "    parser.add_argument('-hasBackground', type=str, help='Whether to use the background (t to, f to not)', default='f')\n",
    "    parser.add_argument('-usesLargestBox', type=str, help='Where to use the size of the largest box (t) or independent tumor boxes (f)', default='t')\n",
    "    parser.add_argument('-segmentsMultiple', type=int, help='Segments a # of slices, 1 by default', default=1)\n",
    "    parser.add_argument('-dropoutRate', type=float, help='Dropout rate for the model', default=0.2)\n",
    "    parser.add_argument('-grouped2D', type=str, help='Grouping the 3D scans as individual 2D images', default='f')\n",
    "    parser.add_argument('-weightDecay', type=float, help='Weight Decay for the model', default=0.01)\n",
    "    parser.add_argument('-modelChosen', type=str, help='Selected Model', default='Large2D')\n",
    "    parser.add_argument('-votingSystem', type=str, help='Either \\'majority\\' voting or \\'average\\' voting', default='majority')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    batchSize = args.batchSize\n",
    "    numOfEpochs = args.epochs\n",
    "    evalDetailLine = args.evalDetailLine\n",
    "    learningRate = args.lr\n",
    "    hasBackground = True if args.hasBackground=='t' else False\n",
    "    usesLargestBox = True if args.usesLargestBox=='t' else False\n",
    "    segmentsMultiple = args.segmentsMultiple\n",
    "    dropoutRate = args.dropoutRate\n",
    "    grouped2D = True if args.grouped2D=='t' else False\n",
    "    weight_decay = args.weightDecay\n",
    "    modelChosen = args.modelChosen\n",
    "    votingSystem = True if args.votingSystem=='majority' else False\n",
    "\n",
    "\n",
    "print(f'BatchSize: {batchSize}, Epochs: {numOfEpochs}, Learning Rate: {learningRate}, Eval Detail Line: {evalDetailLine}, Has Background: {hasBackground}, Uses Largest Box: {usesLargestBox}, Segments Multiple: {segmentsMultiple}, \\\n",
    "      Dropout Rate: {dropoutRate}, Grouped2D: {grouped2D}, weightDecay: {weight_decay}, modelChosen: {modelChosen}, votignSystem: {votingSystem}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "croppedSegmentsList Shape: (89, 12, 224, 224)\n",
      "Single item Shape: (12, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "#Get a saved copy of the dataset\n",
    "name = f'hasBackground={hasBackground}-usesLargestBox={usesLargestBox}-segmentsMultiple={segmentsMultiple}'        \n",
    "# name = f'hasBackground=True-usesLargestBox=False-segmentsMultiple=1'\n",
    "croppedSegmentsList = np.load(f'preprocessCombinations/{name}.npy')\n",
    "    \n",
    "print('croppedSegmentsList Shape:', croppedSegmentsList.shape)   \n",
    "print('Single item Shape:', croppedSegmentsList[0].shape)   \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure,axis = plt.subplots(1,len(croppedSegmentsList),figsize=(200,100))\n",
    "# for idx in range(len(croppedSegmentsList)):        \n",
    "#     axis[idx].imshow(croppedSegmentsList[idx], cmap=\"gray\")\n",
    "#     axis[idx].axis('off')\n",
    "# plt.savefig('padding10.png')\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting information about the transformations\n",
    "def generateTransform(RandomHorizontalFlipValue=0.5,RandomVerticalFlipValue=0.5, RandomRotationValue=50, RandomElaticTransform=[0,0], brightnessConstant=0, contrastConstant=0, kernelSize=3, sigmaRange=(0.1,1.0)):\n",
    "    training_data_transforms = transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.RandomRotation(degrees=RandomRotationValue),\n",
    "        transforms.ElasticTransform(alpha=RandomElaticTransform[0], sigma=RandomElaticTransform[1]),\n",
    "        transforms.ColorJitter(brightnessConstant, contrastConstant),\n",
    "        transforms.GaussianBlur(kernel_size = kernelSize, sigma=sigmaRange),\n",
    "        transforms.RandomHorizontalFlip(p=RandomHorizontalFlipValue),\n",
    "        transforms.RandomVerticalFlip(p=RandomVerticalFlipValue),\n",
    "        transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "    ]) \n",
    "    return training_data_transforms\n",
    "\n",
    "\n",
    "def getTransformValue(transform, desiredTranform, desiredTranformValue):\n",
    "    if transform==None or desiredTranform==None or desiredTranformValue==None:\n",
    "      return None\n",
    "    for t in transform.transforms:\n",
    "        if isinstance(t, desiredTranform):\n",
    "            return t.__getattribute__(desiredTranformValue)\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2D images:\n",
    "# ## Working with pytorch tensors\n",
    "class PatientData(Dataset):\n",
    "    def __init__(self, image, classifications, transform=None):\n",
    "        self.data = image\n",
    "        # Convert classification to torch tensor\n",
    "        temp = []\n",
    "        for classification in classifications:\n",
    "            convert = torch.tensor(classification, dtype=torch.int64) # casting to long\n",
    "            convert = convert.type(torch.float32)\n",
    "            temp.append(convert)\n",
    "            \n",
    "        self.classification = temp\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)* segmentsMultiple\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if grouped2D==False:\n",
    "            image = self.data[idx]\n",
    "            label = self.classification[idx]\n",
    "        else:\n",
    "            patient_idx = idx // segmentsMultiple\n",
    "            slice_idx = idx % segmentsMultiple\n",
    "            image = self.data[patient_idx][slice_idx]\n",
    "            label = self.classification[patient_idx]\n",
    "\n",
    "        # Convert to RGB\n",
    "        image = Image.fromarray((image * 255).astype(np.uint16))\n",
    "        image = image.convert(\"RGB\")\n",
    "\n",
    "        # Apply augmentations if there are any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)#.type(torch.float)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "def convertDataToLoaders(xTrain, yTrain, xVal, yVal, xTest, yTest, training_data_transforms = None, batchSize=8):\n",
    "    ## Sample the data with 75% of the training set \n",
    "    # TrainBalancedSampler = WeightedRandomSampler(weightsForClasses, len(yTrain)//2+len(yTest)//4)\n",
    "    \n",
    "    # Testing data tranfrom, should be just the plain images\n",
    "    testing_data_transforms = transforms.Compose([\n",
    "        #transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
    "    ]) \n",
    "\n",
    "    # Use the same default training transform as the testing transform if not specified\n",
    "    if training_data_transforms == None:\n",
    "        training_data_transforms = testing_data_transforms\n",
    "        \n",
    "    # Convert the testing sets to data loaders\n",
    "    trainingData = PatientData(xTrain, yTrain, transform=training_data_transforms)\n",
    "    trainingData = DataLoader(trainingData, batch_size=batchSize, shuffle=True)#, sampler= TrainBalancedSampler)\n",
    "\n",
    "    validationData = PatientData(xVal, yVal, transform=testing_data_transforms)\n",
    "    validationData = DataLoader(validationData, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "    testingData = PatientData(xTest, yTest, transform=testing_data_transforms)\n",
    "    testingData = DataLoader(testingData, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "    return trainingData, validationData, testingData, training_data_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For 3D images:\n",
    "\n",
    "# import torchio\n",
    "\n",
    "# # ## Working with pytorch tensors\n",
    "# class TorchDataset(Dataset):\n",
    "#     def __init__(self, images, classifications, transform=None):\n",
    "#         self.data = images\n",
    "#         temp = []\n",
    "#         for classification in classifications:\n",
    "#             convert = torch.tensor(classification, dtype=torch.int64) # casting to long\n",
    "#             convert = convert.type(torch.float32)\n",
    "#             temp.append(convert)\n",
    "            \n",
    "#         self.classification = temp\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         images = self.data[idx]\n",
    "#         # Normalize the images\n",
    "#         images = (images - np.min(images)) / (np.max(images) - np.min(images))\n",
    "\n",
    "\n",
    "#         images = torch.from_numpy(images)\n",
    "#         images = torch.FloatTensor(images)\n",
    "\n",
    "#         # images = torch.from_numpy(images)\n",
    "#         print(type(images))\n",
    "        \n",
    "\n",
    "#         label = self.classification[idx]\n",
    "#         # Convert sample to a tensor\n",
    "#         # sample = torch.tensor(sample, dtype=torch.float32).permute(2, 0, 1)  # Change shape to (C, H, W)\n",
    "\n",
    "#         if self.transform:\n",
    "#             images = self.transform(images)\n",
    "        \n",
    "#         return images, label\n",
    "    \n",
    "\n",
    "# # Set the random seed for reproducibility\n",
    "# random.seed(0)\n",
    "# torch.manual_seed(0) \n",
    "\n",
    "# ## Sample the data with 75% of the training set \n",
    "# # TrainBalancedSampler = WeightedRandomSampler(weightsForClasses, len(yTrain)//2+len(yTest)//4)\n",
    "\n",
    "# # Define data augmentation transforms\n",
    "# training_data_transforms = torchio.Compose([\n",
    "#     torchio.RandomFlip(axes=('Left','Right'), flip_probability=RandomVerticalFlipProbablility),\n",
    "#     torchio.RandomFlip(axes=('Anterior','Posterior'), flip_probability=RandomHorizontalFlipProbablility),\n",
    "#     torchio.RandomNoise(std=(0, 0.1)),\n",
    "#     torchio.RandomBlur(std=(0, 1))\n",
    "# ]) \n",
    "# # testing_data_transforms = transforms.Compose([\n",
    "# #     transforms.ToPILImage(),\n",
    "# #     transforms.ToTensor()\n",
    "# # ]) \n",
    "\n",
    "\n",
    "# # Convert the testing sets to data loaders\n",
    "# trainingData = TorchDataset(xTrain, yTrain, transform=training_data_transforms)\n",
    "\n",
    "# trainingData = DataLoader(trainingData, batch_size=batchSize, shuffle=False)#, sampler= TrainBalancedSampler)\n",
    "\n",
    "# testingData = TorchDataset(xTest, yTest, transform=None)# testing_data_transforms)\n",
    "# testingData = DataLoader(testingData, batch_size=batchSize, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Small2D(torch.nn.Module):\n",
    "    def __init__(self, dropoutRate=0.2):\n",
    "        super(Small2D, self).__init__()\n",
    "\n",
    "        #Resnet50 as first layer\n",
    "        self.resNet50 = models.resnet50(weights='IMAGENET1K_V2')\n",
    "        # self.resNet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Modify the first convolutional layer to accept single-channel input\n",
    "        # self.resNet50.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "\n",
    "        # Freeze all layers of the resNet\n",
    "        for param in self.resNet50.parameters():\n",
    "            param.requires_grad = False\n",
    "                \n",
    "        # Modify the final fully connected layer\n",
    "        num_features = self.resNet50.fc.out_features\n",
    "        self.fc = nn.Linear(num_features, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resNet50(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x \n",
    "    \n",
    "class Large2D(torch.nn.Module):\n",
    "    def __init__(self, dropoutRate=0.2):\n",
    "        super(Large2D, self).__init__()\n",
    "\n",
    "        #Resnet50 as first layer\n",
    "        self.resNet50 = models.resnet50(weights='IMAGENET1K_V2')\n",
    "        # self.resNet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Modify the first convolutional layer to accept single-channel input\n",
    "        # self.resNet50.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "\n",
    "        # Freeze all layers of the resNet\n",
    "        for param in self.resNet50.parameters():\n",
    "            param.requires_grad = False\n",
    "                \n",
    "        # Hidden layers with batchnorms\n",
    "        self.batchNormalization0 = nn.BatchNorm1d(self.resNet50.fc.out_features)\n",
    "        self.hiddenLayer1 = nn.Linear(self.resNet50.fc.out_features, 528)\n",
    "        self.batchNormalization1 = nn.BatchNorm1d(528)\n",
    "        self.hiddenLayer2 = nn.Linear(528, 128)\n",
    "        self.batchNormalization2 = nn.BatchNorm1d(128)\n",
    "        self.hiddenLayer3 = nn.Linear(128, 64)\n",
    "        self.batchNormalization3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        #Other layers\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Into ResNet\n",
    "        x = self.resNet50(x)\n",
    "        x = self.batchNormalization0(x) if x.size(dim=0)>1 else x\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # Into hidden layer1\n",
    "        x = self.hiddenLayer1(x) \n",
    "        x = self.batchNormalization1(x) if x.size(dim=0)>1 else x\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # Into hidden layer2\n",
    "        x = self.hiddenLayer2(x)\n",
    "        x = self.batchNormalization2(x) if x.size(dim=0)>1 else x\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # Into hidden layer3\n",
    "        x = self.hiddenLayer3(x)\n",
    "        x = self.batchNormalization3(x) if x.size(dim=0)>1 else x\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # Output layer\n",
    "        x = self.outputLayer(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "def defineModel(dropoutRate=0.2,learningRate=0.001, weight_decay=0.01, model = 'Small2D'):\n",
    "    if model == 'Small2D':\n",
    "        model = Small2D(dropoutRate)\n",
    "    elif model == 'Large2D':\n",
    "        model = Large2D(dropoutRate)\n",
    "    elif model == 'foundationSimple':\n",
    "        model = SmallFoundation(dropoutRate)\n",
    "    \n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learningRate, weight_decay=weight_decay)\n",
    "\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fmcib.models import fmcib_model\n",
    "\n",
    "class SmallFoundation(torch.nn.Module):\n",
    "    def __init__(self, dropoutRate=0.2):\n",
    "        super(SmallFoundation, self).__init__()\n",
    "\n",
    "        # Foundation model as first layer\n",
    "        self.foundationModel = fmcib_model()\n",
    "        \n",
    "        # Modify the first convolutional layer to accept single-channel input\n",
    "        self.foundationModel.trunk.conv1 = nn.Conv3d(1, 128, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "        \n",
    "        # Freeze all layers of the foundation model\n",
    "        for param in self.foundationModel.parameters():\n",
    "            param.requires_grad = False\n",
    "                \n",
    "        # Hidden layers with batchnorms\n",
    "        self.batchNormalization0 = nn.BatchNorm1d(self.foundationModel.fc.out_features)\n",
    "        self.hiddenLayer1 = nn.Linear(self.foundationModel.fc.out_features, 528)\n",
    "        self.batchNormalization1 = nn.BatchNorm1d(528)\n",
    "        self.hiddenLayer2 = nn.Linear(528, 128)\n",
    "        self.batchNormalization2 = nn.BatchNorm1d(128)\n",
    "        self.hiddenLayer3 = nn.Linear(128, 64)\n",
    "        self.batchNormalization3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(64, 3)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        #Other layers\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Into foundation model\n",
    "#         x = self.foundationModel(x)\n",
    "#         x = self.batchNormalization0(x) if x.size(dim=0)>1 else x\n",
    "#         x = self.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "#         # Into hidden layer1\n",
    "#         x = self.hiddenLayer1(x) \n",
    "#         x = self.batchNormalization1(x) if x.size(dim=0)>1 else x\n",
    "#         x = self.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "#         # Into hidden layer2\n",
    "#         x = self.hiddenLayer2(x)\n",
    "#         x = self.batchNormalization2(x) if x.size(dim=0)>1 else x\n",
    "#         x = self.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "#         # Into hidden layer3\n",
    "#         x = self.hiddenLayer3(x)\n",
    "#         x = self.batchNormalization3(x) if x.size(dim=0)>1 else x\n",
    "#         x = self.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "#         # Output layer\n",
    "#         x = self.outputLayer(x)\n",
    "#         x = self.softmax(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device).view(-1, 1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        predicted = outputs > 0.5\n",
    "        total += labels.size(0)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        correct += torch.sum(predicted == labels.data).item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, minDelta=0):\n",
    "        self.patience = patience\n",
    "        self.minDelta = minDelta\n",
    "        self.counter = 0\n",
    "        self.minValLoss = float('inf')\n",
    "        \n",
    "    def earlyStoppingCheck(self, currValLoss):\n",
    "        if np.isnan(currValLoss):\n",
    "            return True\n",
    "        if currValLoss < self.minValLoss:\n",
    "            self.minValLoss = currValLoss\n",
    "            self.counter = 0\n",
    "        elif currValLoss > self.minValLoss + self.minDelta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def evaluateGroupVoting(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).view(-1, 1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            predicted = outputs > 0.5\n",
    "\n",
    "            probabilities.append(outputs.cpu().numpy())\n",
    "            predictions.append(predicted.cpu().numpy())\n",
    "    \n",
    "    # Ignores grouped voting\n",
    "    if segmentsMultiple==1 or grouped2D==False:\n",
    "        return predictions\n",
    "\n",
    "    ## Grouping predictions by patient\n",
    "    ## ==============================================================\n",
    "    all_probs = np.concatenate(probabilities, axis=0)\n",
    "\n",
    "    # Grouping predictions by patient\n",
    "    patient_probs = [] # The confidence of the model\n",
    "    patient_labels = [] # the label given by the model. >=0.5 is 1, <0.5 is 0\n",
    "     \n",
    "    ## Get the classification based from the patient based on ...\n",
    "    for i in range(0, len(all_probs), segmentsMultiple):\n",
    "        # Get probabilties and labels for the patient\n",
    "        patient_prob = all_probs[i:i + segmentsMultiple]\n",
    "        patient_pred_labels = [pred>=0.5 for pred in patient_prob]\n",
    "\n",
    "        ## Average all confidences to get the highest probability label, used as a tie breaker\n",
    "        patient_prob = np.mean(patient_prob) \n",
    "        prob_label_max = patient_prob >= 0.5 \n",
    "        \n",
    "        ## MAJOURITY VOTING\n",
    "        ##==============================================================\n",
    "        # Get counts for the labels \n",
    "        if votingSystem==True:\n",
    "            uniqueLabels, label_counts = np.unique(patient_pred_labels, return_counts=True)\n",
    "            \n",
    "            majorityVote = uniqueLabels[np.argmax(label_counts)]\n",
    "            patient_labels.append(np.array([majorityVote]))\n",
    "        ##==============================================================\n",
    "        ## AVERAGE\n",
    "        ##==============================================================\n",
    "        else:\n",
    "            patient_probs.append(np.array(patient_prob))\n",
    "            patient_labels.append(np.array([prob_label_max]))\n",
    "        ##==============================================================\n",
    "\n",
    "    return patient_labels\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).view(-1, 1)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            predicted = outputs > 0.5\n",
    "            total += labels.size(0)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            correct += torch.sum(predicted == labels.data).item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    return epoch_loss, epoch_acc, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, criterion, optimizer, trainingData, validationData, numOfEpochs=100):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using this device:', device)\n",
    "    #Send the model to the same device that the tensors are on\n",
    "    model.to(device)\n",
    "\n",
    "    earlyStopping = EarlyStopping(patience=10, minDelta=0)\n",
    "    train_loss, train_acc = [], []\n",
    "    val_loss, val_acc = [], []\n",
    "\n",
    "    for epoch in range(numOfEpochs):\n",
    "        #Train model\n",
    "        curTrainLoss, curTrainAcc = train(model, trainingData, criterion, optimizer, device)    \n",
    "        print(f\"Epoch {epoch+1}/{numOfEpochs}\")\n",
    "        print(f\"Train Loss: {curTrainLoss:.4f}, Train Acc: {curTrainAcc:.4f}\")\n",
    "        #Evaluate on validation set\n",
    "        curValLoss, curValAcc, _ = evaluate(model, validationData, criterion, device)    \n",
    "        print(f\"Val Loss: {curValLoss:.4f}, Val Acc: {curValAcc:.4f}\")\n",
    "\n",
    "        #Append metrics to lists\n",
    "        train_loss.append(curTrainLoss)\n",
    "        train_acc.append(curTrainAcc)\n",
    "        val_loss.append(curValLoss)\n",
    "        val_acc.append(curValAcc)\n",
    "\n",
    "        #Check for early stopping conditions\n",
    "        if earlyStopping.earlyStoppingCheck(curValLoss):\n",
    "            print(f'Early stopping - Val loss has not decreased in {earlyStopping.patience} epochs. Terminating training at epoch {epoch+1}.')\n",
    "            break\n",
    "\n",
    "    history = {'train_loss':train_loss, 'train_acc':train_acc, 'val_loss':val_loss, 'val_acc':val_acc}\n",
    "    return model, criterion, device, history, epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_history_from_pickle(testPathName):\n",
    "#     with open(testPathName+'/history.pkl', 'rb') as fp:\n",
    "#         history = pickle.load(fp)\n",
    "#     return history\n",
    "\n",
    "# #Read history\n",
    "# history = read_history_from_pickle(testPathName)\n",
    "\n",
    "# # Load and evalaute the model\n",
    "# modelWeightPath = testPathName+'/model.pt'\n",
    "# model = ResNet50ClassificaitonModel()\n",
    "# model.load_state_dict(torch.load(modelWeightPath))\n",
    "\n",
    "#Send the model to the device used\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print('Using this device:', device)\n",
    "# #Send the model to the same device that the tensors are on\n",
    "# model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the contents of this test\n",
    "\n",
    "def saveResults(testPathName, model, history, training_data_transforms, saveModel=True):\n",
    "    os.makedirs(testPathName, exist_ok=True)\n",
    "\n",
    "    #Save history as pickle\n",
    "    with open(testPathName+'/history.pkl', 'wb') as fp:\n",
    "        pickle.dump(history, fp)\n",
    "\n",
    "    # Save weigths of model\n",
    "    if saveModel:\n",
    "        torch.save(model.state_dict(), testPathName+'/model.pt')\n",
    "\n",
    "    # Save transformations for easy access\n",
    "    f = open(testPathName + '/training_data_transforms.txt', 'w')\n",
    "            \n",
    "    for line in training_data_transforms.__str__():\n",
    "        f.write(line)\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTensorToPredictions(tensorList, dictionary):\n",
    "    \"\"\"Converts a list of tensors/ndarrays into a single list and then feeds those values into a given dictionary.\"\"\"\n",
    "    values=[]\n",
    "    if grouped2D==True:\n",
    "        for batch in tensorList:\n",
    "            batchValues = batch.tolist()        \n",
    "            values+= batchValues\n",
    "        for value in values:\n",
    "            dictionary[value]+=1\n",
    "    else:\n",
    "        values=[]#tensorList\n",
    "        for batch in tensorList:\n",
    "            batchValues = batch.tolist()      \n",
    "        for value in batchValues:\n",
    "            values.append(value[0])\n",
    "        print(values)\n",
    "        for value in values:\n",
    "            dictionary[value]+=1\n",
    "    return values, dictionary\n",
    "\n",
    "def evaluateModelOnTestSet(testPathName, model, testingData, criterion, device, saveConfusionMatrix = True, showConfusionMatrix=True):\n",
    "    predictions = evaluateGroupVoting(model, testingData, criterion, device)\n",
    "\n",
    "    predicts, predictsTotal = convertTensorToPredictions(predictions, {0:0,1:0})\n",
    "\n",
    "    # Get the correct answers\n",
    "    originalLabelsTemp = []\n",
    "    ans = []\n",
    "    \n",
    "    for _, labels in testingData:\n",
    "      originalLabelsTemp.extend(labels.tolist())\n",
    "    \n",
    "    for i in range(len(originalLabelsTemp)):  \n",
    "        if i%segmentsMultiple==0:\n",
    "            ans.append(originalLabelsTemp[i] >=0.5)\n",
    "        i+=1\n",
    " \n",
    "    ansTotal = dict(zip([0,1],[ans.count(False),ans.count(True)]))\n",
    "\n",
    "    print('ans length',len(ans))\n",
    "\n",
    "    # Test metrics\n",
    "    print('---------------------------------------\\nTesting Metrics')\n",
    "    print('ans', ans)\n",
    "    print('predicts',predicts)\n",
    "    \n",
    "    accuracy = accuracy_score(ans, predicts)\n",
    "    f1 = list(f1_score(ans, predicts, average=None))  # Use 'weighted' for multiclass classification\n",
    "    recall = list(recall_score(ans, predicts, average=None))  # Use 'weighted' for multiclass classification\n",
    "\n",
    "    testingMetrics = {'Predictions split': predictsTotal, 'Answers split': ansTotal, 'Predictions': predicts, 'Answers    ': ans,  'Accuracy':accuracy, 'F1 Score':f1, 'Recall':recall}\n",
    "\n",
    "    file = open(testPathName+'/testingMetrics.txt','w')\n",
    "    for key, value in testingMetrics.items():\n",
    "        file.write(f'{key}: {value}\\n')\n",
    "    file.close()\n",
    "\n",
    "    for key, value in testingMetrics.items():\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "\n",
    "    print('---------------------------------------\\nConfusion Matrix:')\n",
    "    # Confusion Matrix\n",
    "    result = confusion_matrix(ans,predicts,normalize='pred')\n",
    "    disp = ConfusionMatrixDisplay(result)\n",
    "    \n",
    "    if saveConfusionMatrix:\n",
    "        plt.savefig(testPathName+'/confusion_matrix.png')\n",
    "    \n",
    "    if showConfusionMatrix:\n",
    "        plt.show()\n",
    "\n",
    "    plt.clf() \n",
    "    return disp, accuracy, f1, recall, predictsTotal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Performance on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTraining(testPathName, testName, history, saveFigure=True, showResult=True):\n",
    "    plt.style.use('default')\n",
    "    \n",
    "    figure, ax = plt.subplots( 1, 2, figsize=(20, 10))\n",
    "    # plt.suptitle('Accuracy', fontsize=10)\n",
    "    ax[0].set_title(\"Loss\")\n",
    "    ax[0].set_ylabel('Loss', fontsize=16)\n",
    "    ax[0].set_xlabel('Epoch', fontsize=16)\n",
    "    ax[0].plot(history['train_loss'], label='Training Loss')\n",
    "    ax[0].plot(history['val_loss'], label='Validation Loss')\n",
    "    ax[0].legend(loc='upper right')\n",
    "\n",
    "    ax[1].set_title(\"Accuracy\")\n",
    "    ax[1].set_ylabel('Accuracy', fontsize=16)\n",
    "    ax[1].set_xlabel('Epoch', fontsize=16)\n",
    "    \n",
    "    ax[1].plot(history['train_acc'], label='Training Accuracy')\n",
    "    ax[1].plot(history['val_acc'], label='Validation Accuracy')\n",
    "    ax[1].legend(loc='lower right')\n",
    "\n",
    "    if saveFigure:\n",
    "        plt.savefig(testPathName+'/training_history.png')\n",
    "    \n",
    "    if showResult:\n",
    "        plt.show()\n",
    "\n",
    "    plt.clf() \n",
    "\n",
    "    return figure\n",
    "\n",
    "\n",
    "def plotTrainingPerformances(testPathName, testName, histories, saveFigure=True, showResult=True):\n",
    "    plt.style.use('default')\n",
    "\n",
    "    figure, ax = plt.subplots( 2, len(histories), figsize=(80, 20))\n",
    "    for idx, history in enumerate(histories):\n",
    "        # plt.suptitle('Accuracy', fontsize=10)\n",
    "        ax[0][idx].set_title(\"Loss\")\n",
    "        ax[0][idx].set_ylabel('Loss', fontsize=16)\n",
    "        ax[0][idx].set_xlabel('Epoch', fontsize=16)\n",
    "        ax[0][idx].plot(history['train_loss'], label='Training Loss')\n",
    "        ax[0][idx].plot(history['val_loss'], label='Validation Loss')\n",
    "        ax[0][idx].legend(loc='upper right')\n",
    "\n",
    "        ax[1][idx].set_title(\"Accuracy\")\n",
    "        ax[1][idx].set_ylabel('Accuracy', fontsize=16)\n",
    "        ax[1][idx].set_xlabel('Epoch', fontsize=16)\n",
    "        ax[1][idx].plot(history['train_acc'], label='Training Accuracy')\n",
    "        ax[1][idx].plot(history['val_acc'], label='Validation Accuracy')\n",
    "        ax[1][idx].legend(loc='lower right')\n",
    "\n",
    "    plt.suptitle(f'{testName} \\nTrainining Performance', fontsize=30)\n",
    "\n",
    "\n",
    "    if saveFigure:\n",
    "        plt.savefig(testPathName+'/training_history.png')\n",
    "    \n",
    "    if showResult:\n",
    "        plt.show()\n",
    "\n",
    "    plt.clf() \n",
    "\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run FullStack of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotConfusionMatricies(testPathName, testName, confusion_matricies):\n",
    "    figure,axis = plt.subplots(1,len(confusion_matricies),figsize=(20, 5))\n",
    "    for idx in range(len(confusion_matricies)):        \n",
    "        confusion_matricies[idx].plot(ax=axis[idx])\n",
    "        confusion_matricies[idx].im_.colorbar.remove()\n",
    "\n",
    "    figure.suptitle(f'{testName}\\nConfusion Matricies')\n",
    "    plt.savefig(testPathName+'/confusion_matrix.png')\n",
    "    plt.show()\n",
    "    plt.clf() \n",
    "    \n",
    "def meanConfidenceInterval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "\n",
    "    min = np.min(a)\n",
    "    max = np.max(a)\n",
    "\n",
    "    return [m, h, [min, max], data]\n",
    "\n",
    "def averagePredictionTotals(predictions, numberOfTrials=5):\n",
    "    average = {0:0,1:0}\n",
    "    for prediction in predictions:\n",
    "        for key, value in prediction.items():\n",
    "            average[key] += value\n",
    "    \n",
    "    for key,value in average.items():\n",
    "        average[key] = value/numberOfTrials\n",
    "\n",
    "    return average\n",
    "\n",
    "\n",
    "def meanConfidenceInterval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "\n",
    "    min = np.min(a)\n",
    "    max = np.max(a)\n",
    "\n",
    "    return [m, h, data]\n",
    "\n",
    "def averageMultilabelMetricScores(scores, numberOfTrials=5, numberOfClasses=2):\n",
    "    dict = {0:0,1:0}\n",
    "    averages= [0]*numberOfClasses\n",
    "    for score in scores:\n",
    "        for i in range(numberOfClasses):\n",
    "            averages[i] += score[i]\n",
    "    averages = [averages[i]/numberOfTrials for i in range(numberOfClasses)]\n",
    "    \n",
    "    for key in range(numberOfClasses):\n",
    "        dict[key] = averages[key]\n",
    "\n",
    "    mean = np.mean(averages)\n",
    "    return [mean, dict, scores]\n",
    "\n",
    "#Runs all model evaluations steps\n",
    "def runModelFullStack(testPathName, testName, xTrain, yTrain, xVal, yVal, xTest, yTest, modelInformation, trainingTransform=None):\n",
    "    trainingData, validationData, testingData, training_data_transforms = convertDataToLoaders(xTrain, yTrain, xVal, yVal, xTest, yTest, training_data_transforms = trainingTransform, batchSize=modelInformation['batchSize'])\n",
    "    model, criterion, optimizer = defineModel(dropoutRate=modelInformation['dropoutRate'], learningRate = modelInformation['learningRate'], weight_decay=modelInformation['weight_decay'], model = modelInformation['model'])\n",
    "    model, criterion, device, history, endingEpoch = trainModel(model, criterion, optimizer, trainingData,validationData, numOfEpochs=modelInformation['numOfEpochs'])\n",
    "    saveResults(testPathName, model, history, training_data_transforms, saveModel=False)\n",
    "    confusionMatrix, accuracy, f1, recall, predictsTotal = evaluateModelOnTestSet(testPathName, model, testingData, criterion, device, saveConfusionMatrix = True, showConfusionMatrix=False)\n",
    "    #plotTraining(testPathName, testName, history, saveFigure=True, showResult=True)\n",
    "\n",
    "    return confusionMatrix, history, accuracy, f1, recall, predictsTotal, endingEpoch+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addEvalDetailToModel(evalDetailLine, dataframe):\n",
    "    dataframe.loc[dataframe.shape[0]] = [evalDetailLine] + ['']*(len(dataframe.columns)-1)\n",
    "    \n",
    "def appendMetricsToXLSX(testPathName, trainingTransform, accuracyInformation, f1Information, recallInformation, predictsTotal, endingEpochs, modelInformation, dataframe):\n",
    "    rotation = getTransformValue(trainingTransform, desiredTranform=transforms.RandomRotation, desiredTranformValue='degrees')\n",
    "    elasticTransform = [getTransformValue(trainingTransform, desiredTranform=transforms.ElasticTransform, desiredTranformValue='alpha'), getTransformValue(trainingTransform, desiredTranform=transforms.ElasticTransform, desiredTranformValue='sigma')]\n",
    "    brightness = getTransformValue(trainingTransform, desiredTranform=transforms.ColorJitter, desiredTranformValue='brightness')\n",
    "    contrast = getTransformValue(trainingTransform, desiredTranform=transforms.ColorJitter, desiredTranformValue='contrast')\n",
    "    guassianBlur = [ getTransformValue(trainingTransform, desiredTranform=transforms.GaussianBlur, desiredTranformValue='kernel_size'), getTransformValue(trainingTransform, desiredTranform=transforms.GaussianBlur, desiredTranformValue='sigma')]\n",
    "    randomHorizontalFlip = getTransformValue(trainingTransform, desiredTranform=transforms.RandomHorizontalFlip, desiredTranformValue='p')\n",
    "    randomVerticalFlip = getTransformValue(trainingTransform, desiredTranform=transforms.RandomVerticalFlip, desiredTranformValue='p')\n",
    "    \n",
    "    exportValue = [testPathName, modelInformation['numOfEpochs'],modelInformation['batchSize'], modelInformation['learningRate'],modelInformation['dropoutRate'],modelInformation['weight_decay'],modelInformation['commandRan'], \\\n",
    "        rotation, elasticTransform, brightness, contrast, guassianBlur, randomHorizontalFlip, randomVerticalFlip, predictsTotal, \\\n",
    "            accuracyInformation[0], f1Information[0], recallInformation[0], \\\n",
    "            accuracyInformation[1], f1Information[1],  recallInformation[1], \\\n",
    "            accuracyInformation[2], f1Information[2], recallInformation[2], \\\n",
    "            endingEpochs]\n",
    "    \n",
    "    dataframe.loc[dataframe.shape[0]] = exportValue + ['']*(len(dataframe.columns)-len(exportValue))\n",
    "\n",
    "def generateKFoldsValidation(identifier,identifierValue, modelInformation, grouped2D, k=5,trainingTransform=None):\n",
    "\n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(0)\n",
    "    torch.manual_seed(0) \n",
    "    \n",
    "    #Keep history of values\n",
    "    confusion_matricies = []\n",
    "    histories = []\n",
    "    accuracies = []\n",
    "    f1s = []\n",
    "    recalls = []\n",
    "    predictionSplits = []\n",
    "    endingEpochs = []\n",
    "\n",
    "    # run datasplit with stratified kfolds:\n",
    "    \n",
    "    if grouped2D: #if >100 then we are doing groupings of 2D images\n",
    "        stratifiedGroupFolds = StratifiedGroupKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        stratifiedGroupFolds.get_n_splits(croppedSegmentsList, recistCriteria)\n",
    "        splits = enumerate(stratifiedGroupFolds.split(croppedSegmentsList, recistCriteria, cases))\n",
    "        \n",
    "    else:\n",
    "        stratifiedFolds = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        stratifiedFolds.get_n_splits(croppedSegmentsList, recistCriteria)\n",
    "        splits = enumerate(stratifiedFolds.split(croppedSegmentsList, recistCriteria))\n",
    "        \n",
    "    for i, (train_index, test_index) in splits:\n",
    "        \n",
    "        #Set the name of the test\n",
    "        testName = f'{identifier}-{identifierValue}'\n",
    "        \n",
    "        testPathName = 'Tests/'+testName+f'/foldn{i+1}'\n",
    "        print(f'{identifier}: foldn{i+1} RUN\\n=========================================')\n",
    "        xTest, yTest = [croppedSegmentsList[i] for i in test_index], [recistCriteria[i] for i in test_index]\n",
    "        xTrain, yTrain = [croppedSegmentsList[i] for i in train_index], [recistCriteria[i] for i in train_index]\n",
    "        xVal, yVal = xTest, yTest # Set the validation set to the same as the testing set\n",
    "        #xVal, yVal = [croppedSegmentsList[i] for i in train_index[:len(xTest)]], [recistCriteria[i] for i in train_index[:len(yTest)]]\n",
    "        \n",
    "\n",
    "\n",
    "        # Convert the recist criteria to 0,1\n",
    "        yTrain = [x-1 for x in yTrain]\n",
    "        yVal = [x-1 for x in yVal]\n",
    "        yTest = [x-1 for x in yTest]\n",
    "\n",
    "        # ## Working with Numpy arrays\n",
    "        xTrain = np.array(xTrain) \n",
    "        xTest = np.array(xTest)\n",
    "        xVal = np.array(xVal)\n",
    "        yTrain = np.array(yTrain)\n",
    "        yVal = np.array(yVal)\n",
    "        yTest = np.array(yTest)\n",
    "\n",
    "        ## ==============================================================\n",
    "        ## Using SMOTE\n",
    "        smote = SMOTE(random_state=42)\n",
    "        if len(xTrain.shape)==3:\n",
    "            oneDShape = xTrain[0].shape[0]*xTrain[0].shape[1]\n",
    "            \n",
    "        else:\n",
    "            print('xTrain shape',xTrain.shape)\n",
    "            oneDShape = xTrain[0].shape[0]*xTrain[0].shape[1]*xTrain[0].shape[2]\n",
    "\n",
    "        singleShape = xTrain[0].shape\n",
    "\n",
    "        print('xTrain reshape',xTrain.reshape(xTrain.shape[0],oneDShape).shape)\n",
    "        xTrainSmote, yTrain = smote.fit_resample(xTrain.reshape(xTrain.shape[0],oneDShape), yTrain)\n",
    "        if len(xTrain.shape)==3:\n",
    "            xTrain = xTrainSmote.reshape(xTrainSmote.shape[0], xTrain[0].shape[0],xTrain[0].shape[1])\n",
    "        else:\n",
    "            xTrain = xTrainSmote.reshape(xTrainSmote.shape[0], xTrain[0].shape[0],xTrain[0].shape[1],xTrain[0].shape[2])\n",
    "\n",
    "        print('xTrain after Smote', xTrain.shape)\n",
    "        print('yTrain after Smote', yTrain.shape)\n",
    "        from collections import Counter\n",
    "        counter  = Counter(yTest)\n",
    "        print('Splits for test Fold',sorted(counter.items()))\n",
    "\n",
    "        ## ==============================================================\n",
    "        \n",
    "        # May or may not need this, def not needed for grouped2D=True\n",
    "        # xTrain = np.expand_dims(xTrain,axis=-1)\n",
    "        # xVal = np.expand_dims(xVal,axis=-1)\n",
    "        # xTest = np.expand_dims(xTest,axis=-1)\n",
    "\n",
    "        print('xTrain', xTrain.shape)\n",
    "        print('xVal', xVal.shape)\n",
    "        print('xTest', xTest.shape)\n",
    "\n",
    "        ## Get and save results for each fold        \n",
    "        confusionMatrix, history, accuracy, f1, recall, predictsTotal, endingEpoch = runModelFullStack(testPathName, testName, xTrain, yTrain, xVal, yVal, xTest, yTest, trainingTransform=trainingTransform, modelInformation=modelInformation) \n",
    "\n",
    "        confusion_matricies.append(confusionMatrix)\n",
    "        histories.append(history)\n",
    "        accuracies.append(accuracy)\n",
    "        f1s.append(f1)\n",
    "        recalls.append(recall)\n",
    "        predictionSplits.append(predictsTotal)\n",
    "        endingEpochs.append(endingEpoch)\n",
    "        print('\\n\\n')\n",
    "\n",
    "    #assert False\n",
    "    # Calculate the average of the metrics for the kfolds of this transformation and save it\n",
    "    kFoldsTestMetrics = {'Prediction averages': averagePredictionTotals(predictionSplits), 'Accuracy':meanConfidenceInterval(accuracies), 'F1 Score':averageMultilabelMetricScores(f1s), 'Recall':averageMultilabelMetricScores(recalls)}\n",
    "    file = open('Tests/'+testName+'/kFoldsTestMetrics.txt','w')\n",
    "    for key, value in kFoldsTestMetrics.items():\n",
    "        file.write(f'{key}: {value}\\n')\n",
    "        print(f'{key}: {value}')\n",
    "    file.close()\n",
    "\n",
    "    # Plot training and confusion matrix for each fold as a single .png\n",
    "    plotConfusionMatricies('Tests/'+testName, testName, confusion_matricies)\n",
    "    plotTrainingPerformances('Tests/'+testName, testName, histories, saveFigure=True, showResult=True)\n",
    "\n",
    "    # Append results to the xlsx file\n",
    "    appendMetricsToXLSX(testPathName, trainingTransform, meanConfidenceInterval(accuracies), averageMultilabelMetricScores(f1s), averageMultilabelMetricScores(recalls), averagePredictionTotals(predictionSplits), endingEpochs, modelInformation, dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # View what the splits are for the models, the test set is {0: 4, 1: 7-8, 2: 6-5}\n",
    "# stratifiedFolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# stratifiedFolds.get_n_splits(croppedSegmentsList, recistCriteria)\n",
    "# for i, (train_index, test_index) in enumerate(stratifiedFolds.split(croppedSegmentsList, recistCriteria)):\n",
    "#         xTrain, xTest = [croppedSegmentsList[i] for i in train_index], [croppedSegmentsList[i] for i in test_index]\n",
    "#         yTrain, yTest = [recistCriteria[i] for i in train_index], [recistCriteria[i] for i in test_index]\n",
    "\n",
    "#         # Convert the recist criteria to 0,1,2\n",
    "#         yTrain = [x-1 for x in yTrain]\n",
    "#         yTest = [x-1 for x in yTest]\n",
    "\n",
    "#         #count the categorization splits of the train and test set\n",
    "#         trainRecistSplit = {y: yTrain.count(y) for y in yTrain}\n",
    "#         testRecistSplit = {y: yTest.count(y) for y in yTest}\n",
    "\n",
    "#         # Format the splits nicely into an ordered dictionary\n",
    "#         myKeys = list(trainRecistSplit.keys())\n",
    "#         myKeys.sort()\n",
    "#         trainRecistSplitDisplay = {i: trainRecistSplit[i] for i in myKeys}\n",
    "#         myKeys = list(testRecistSplit.keys())\n",
    "#         myKeys.sort()\n",
    "#         testRecistSplitDisplay = {i: testRecistSplit[i] for i in myKeys}\n",
    "\n",
    "#         print(f'train recist category split: {trainRecistSplitDisplay}')\n",
    "#         print(f'test recist category split: {testRecistSplitDisplay}')\n",
    "\n",
    "#         # ## Working with Numpy arrays\n",
    "#         xTrain = np.array(xTrain) \n",
    "#         xTest = np.array(xTest)\n",
    "#         yTrain = np.array(yTrain)\n",
    "#         yTest = np.array(yTest)\n",
    "#         xTrain =  np.expand_dims(xTrain,axis=-1)\n",
    "#         xTest =  np.expand_dims(xTest,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "details d:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\ipykernel_launcher.py\n",
      "details --f=c:\\Users\\johnz\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-15132FyFRdl58BoT6.json\n",
      "python d:\\SimpsonLab\\threeDresearchPip\\lib\\site-packages\\ipykernel_launcher.py --f=c:\\Users\\johnz\\AppData\\Roaming\\jupyter\\runtime\\kernel-v2-15132FyFRdl58BoT6.json\n",
      ": foldn1 RUN\n",
      "=========================================\n",
      "[(0, 9), (1, 9)]\n",
      "xTrain (71, 12, 224, 224)\n",
      "xVal (18, 12, 224, 224)\n",
      "xTest (18, 12, 224, 224)\n",
      "Using this device: cuda\n",
      "Start training\n",
      "Epoch 1/1\n",
      "Train Loss: 0.7321, Train Acc: 0.7183\n",
      "Val Loss: 0.6933, Val Acc: 0.5000\n",
      "ans length 18\n",
      "---------------------------------------\n",
      "Testing Metrics\n",
      "ans [True, True, False, False, False, True, True, False, True, True, False, False, True, False, False, False, True, True]\n",
      "predicts [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Predictions split: {0: 18, 1: 0}\n",
      "Answers split: {0: 9, 1: 9}\n",
      "Predictions: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Answers    : [True, True, False, False, False, True, True, False, True, True, False, False, True, False, False, False, True, True]\n",
      "Accuracy: 0.5\n",
      "F1 Score: [0.6666666666666666, 0.0]\n",
      "Recall: [1.0, 0.0]\n",
      "---------------------------------------\n",
      "Confusion Matrix:\n",
      "\n",
      "\n",
      "\n",
      ": foldn2 RUN\n",
      "=========================================\n",
      "[(0, 12), (1, 6)]\n",
      "xTrain (71, 12, 224, 224)\n",
      "xVal (18, 12, 224, 224)\n",
      "xTest (18, 12, 224, 224)\n",
      "Using this device: cuda\n",
      "Start training\n",
      "Epoch 1/1\n",
      "Train Loss: 0.7259, Train Acc: 0.6866\n",
      "Val Loss: 0.7037, Val Acc: 0.6667\n",
      "ans length 18\n",
      "---------------------------------------\n",
      "Testing Metrics\n",
      "ans [False, False, False, True, True, False, True, False, False, False, False, True, False, False, False, True, True, False]\n",
      "predicts [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Predictions split: {0: 18, 1: 0}\n",
      "Answers split: {0: 12, 1: 6}\n",
      "Predictions: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Answers    : [False, False, False, True, True, False, True, False, False, False, False, True, False, False, False, True, True, False]\n",
      "Accuracy: 0.6666666666666666\n",
      "F1 Score: [0.8, 0.0]\n",
      "Recall: [1.0, 0.0]\n",
      "---------------------------------------\n",
      "Confusion Matrix:\n",
      "\n",
      "\n",
      "\n",
      ": foldn3 RUN\n",
      "=========================================\n",
      "[(0, 15), (1, 3)]\n",
      "xTrain (71, 12, 224, 224)\n",
      "xVal (18, 12, 224, 224)\n",
      "xTest (18, 12, 224, 224)\n",
      "Using this device: cuda\n",
      "Start training\n",
      "Epoch 1/1\n",
      "Train Loss: 0.7225, Train Acc: 0.6479\n",
      "Val Loss: 0.7105, Val Acc: 0.8333\n",
      "ans length 18\n",
      "---------------------------------------\n",
      "Testing Metrics\n",
      "ans [False, True, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False]\n",
      "predicts [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Predictions split: {0: 18, 1: 0}\n",
      "Answers split: {0: 15, 1: 3}\n",
      "Predictions: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Answers    : [False, True, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False]\n",
      "Accuracy: 0.8333333333333334\n",
      "F1 Score: [0.9090909090909091, 0.0]\n",
      "Recall: [1.0, 0.0]\n",
      "---------------------------------------\n",
      "Confusion Matrix:\n",
      "\n",
      "\n",
      "\n",
      ": foldn4 RUN\n",
      "=========================================\n",
      "[(0, 13), (1, 5)]\n",
      "xTrain (71, 12, 224, 224)\n",
      "xVal (18, 12, 224, 224)\n",
      "xTest (18, 12, 224, 224)\n",
      "Using this device: cuda\n",
      "Start training\n",
      "Epoch 1/1\n",
      "Train Loss: 0.7254, Train Acc: 0.6714\n",
      "Val Loss: 0.7089, Val Acc: 0.7222\n",
      "ans length 18\n",
      "---------------------------------------\n",
      "Testing Metrics\n",
      "ans [True, False, False, True, True, False, False, False, False, False, True, False, False, False, True, False, False, False]\n",
      "predicts [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Predictions split: {0: 18, 1: 0}\n",
      "Answers split: {0: 13, 1: 5}\n",
      "Predictions: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Answers    : [True, False, False, True, True, False, False, False, False, False, True, False, False, False, True, False, False, False]\n",
      "Accuracy: 0.7222222222222222\n",
      "F1 Score: [0.8387096774193548, 0.0]\n",
      "Recall: [1.0, 0.0]\n",
      "---------------------------------------\n",
      "Confusion Matrix:\n",
      "\n",
      "\n",
      "\n",
      ": foldn5 RUN\n",
      "=========================================\n",
      "[(0, 12), (1, 5)]\n",
      "xTrain (72, 12, 224, 224)\n",
      "xVal (17, 12, 224, 224)\n",
      "xTest (17, 12, 224, 224)\n",
      "Using this device: cuda\n",
      "Start training\n",
      "Epoch 1/1\n",
      "Train Loss: 0.7273, Train Acc: 0.6736\n",
      "Val Loss: 0.7094, Val Acc: 0.7059\n",
      "ans length 17\n",
      "---------------------------------------\n",
      "Testing Metrics\n",
      "ans [False, False, True, False, True, False, True, False, False, True, False, False, False, False, False, False, True]\n",
      "predicts [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Predictions split: {0: 17, 1: 0}\n",
      "Answers split: {0: 12, 1: 5}\n",
      "Predictions: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
      "Answers    : [False, False, True, False, True, False, True, False, False, True, False, False, False, False, False, False, True]\n",
      "Accuracy: 0.7058823529411765\n",
      "F1 Score: [0.8275862068965517, 0.0]\n",
      "Recall: [1.0, 0.0]\n",
      "---------------------------------------\n",
      "Confusion Matrix:\n",
      "\n",
      "\n",
      "\n",
      "Prediction averages: {0: 17.8, 1: 0.0}\n",
      "Accuracy: [0.6856209150326797, 0.1500096714091254, [0.5, 0.6666666666666666, 0.8333333333333334, 0.7222222222222222, 0.7058823529411765]]\n",
      "F1 Score: [0.40420534600734825, {0: 0.8084106920146965, 1: 0.0}, [[0.6666666666666666, 0.0], [0.8, 0.0], [0.9090909090909091, 0.0], [0.8387096774193548, 0.0], [0.8275862068965517, 0.0]]]\n",
      "Recall: [0.5, {0: 1.0, 1: 0.0}, [[1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Make all transforms that I am going to test:\n",
    "transformsTested = {\n",
    "    \"0\":None\n",
    "    # \"20\":generateTransform(RandomRotationValue=20, RandomElaticTransform=[20,2], brightnessConstant=20, contrastConstant=20, kernelSize=3, sigmaRange=(0.001,0.4)),\n",
    "    # \"40\":generateTransform(RandomRotationValue=40, RandomElaticTransform=[40,4], brightnessConstant=40, contrastConstant=40, kernelSize=3, sigmaRange=(0.001,0.8)),\n",
    "    # \"60\":generateTransform(RandomRotationValue=60, RandomElaticTransform=[60,6], brightnessConstant=60, contrastConstant=60, kernelSize=3, sigmaRange=(0.001,1.2)),\n",
    "    # \"80\":generateTransform(RandomRotationValue=80, RandomElaticTransform=[80,8], brightnessConstant=80, contrastConstant=80, kernelSize=3, sigmaRange=(0.001,1.6)),\n",
    "    # \"100\":generateTransform(RandomRotationValue=100, RandomElaticTransform=[100,10], brightnessConstant=100, contrastConstant=100, kernelSize=3, sigmaRange=(0.001,2.0))    \n",
    "}\n",
    "\n",
    "## Open the dataframe and add the evaluation details\n",
    "dataframePath='testResults.xlsx'\n",
    "columns = ['name','numOfEpochs','batchSize','learningRate','dropoutRate','weight_decay', 'commandRan','RandomRotation','ElasticTransform','Brightness','Contrast','GaussianBlur','RandomHorizontalFlip','RandomVerticalFlip','PredictionAverage',\n",
    "            'AccuracyAverage','F1Average', 'RecallAverage','AccuracySTD','F1STD','RecallSTD','AccuracyData','F1Data','RecallData', 'EndingEpoch']\n",
    "dataframe = pd.read_excel('testResults.xlsx', header=None, names=columns)\n",
    "addEvalDetailToModel(evalDetailLine, dataframe)\n",
    "\n",
    "# Generate the command ran for the test \n",
    "commandRan = 'python'\n",
    "for details in sys.argv:\n",
    "    print('details',details)\n",
    "    stringArgs = ['evalDetailLine','modelChosen','votingSystem']\n",
    "    if details in stringArgs:\n",
    "        detailArray = details.split('=') \n",
    "        details = f'{detailArray[0]}=\\'{detailArray[1]}\\''\n",
    "    commandRan += f' {details}'   \n",
    "print(commandRan)\n",
    "\n",
    "modelInformation = { 'learningRate': learningRate, 'dropoutRate': dropoutRate, 'batchSize': batchSize, 'numOfEpochs':numOfEpochs, 'weight_decay':weight_decay, 'commandRan': commandRan, 'model': modelChosen, 'votingSystem': votingSystem}\n",
    "# modelInformation = { 'learningRate': learningRate, 'dropoutRate': dropoutRate, 'batchSize': batchSize, 'numOfEpochs':1, 'weight_decay':weight_decay, 'commandRan': commandRan, 'model': modelChosen, 'votingSystem': votingSystem}\n",
    "\n",
    "# Run the tests\n",
    "for key, value in transformsTested.items():\n",
    "    generateKFoldsValidation(evalDetailLine,\"-\", k=5,trainingTransform=value, modelInformation = modelInformation, grouped2D=grouped2D)\n",
    "\n",
    "dataframe.to_excel(dataframePath, index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
