{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to python script, remember to delete/comment the next line in the actual file\n",
    "# ! jupyter nbconvert --to python generatePreprocessCombinations.ipynb --output generatePreprocessCombinations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image reading and file handling \n",
    "import pandas as pd\n",
    "import SimpleITK as sitk \n",
    "import os \n",
    "import shutil\n",
    "from matplotlib import pyplot as plt \n",
    "# Image agumentaitons \n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# import scipy\n",
    "\n",
    "# Dataset building\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip freeze > requirements.txt\n",
    "# ! pip uninstall -y -r requirements.txt\n",
    "\n",
    "## Download necessary packages \n",
    "# ! pip install matplotlib opencv-python scipy simpleitk pandas openpyxl scikit-learn nbconvert\n",
    "# ! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \n",
    "\n",
    "## For 3D image classification\n",
    "# ! pip install foundation-cancer-image-biomarker -qq\n",
    "# ! pip install foundation-cancer-image-biomarker\n",
    "# ! pip install torchio\n",
    "\n",
    "## Check python version and packages\n",
    "# ! python --version\n",
    "# ! pip freeze > research.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data from the .xsxl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   TAPS_CaseIDs_PreNAT RECIST_PostNAT Slice_Thickness\n",
      "41             CASE520              1               6\n",
      "53             CASE550              2               6\n",
      "71             CASE586              2               6\n",
      "82             CASE605              2               6\n",
      "89\n",
      "Distribution of cases based on RECIST criteria Counter({2: 40, 3: 28, 1: 21})\n"
     ]
    }
   ],
   "source": [
    "columns = ['TAPS_CaseIDs_PreNAT','RECIST_PostNAT', 'Slice_Thickness']\n",
    "data = pd.read_excel('PDAC-Response_working.xlsx', header=None,names = columns)\n",
    "data.drop(0, inplace=True) # Remove the header row\n",
    "data=data.sort_values(by=['TAPS_CaseIDs_PreNAT'])\n",
    "\n",
    "print(data[data['Slice_Thickness']==6])\n",
    "# There is 1 case that is of 3 slice, remainder are of 6-29\n",
    "data.drop(data[data['Slice_Thickness'] < 6].index, inplace = True)\n",
    "\n",
    "# # Get the entire datasheet\n",
    "cases = list(data['TAPS_CaseIDs_PreNAT'])\n",
    "recistCriteria = list(data['RECIST_PostNAT'])\n",
    "\n",
    "sliceThickness = list(data['Slice_Thickness'])\n",
    "sliceThickness.sort()\n",
    "print(len(sliceThickness))\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "recistCounter = Counter(recistCriteria)\n",
    "print('Distribution of cases based on RECIST criteria',recistCounter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the images aligned if not. Mainly just checks\n",
    "#==========================================================================================\n",
    "def makeAlign(image1,image2):\n",
    "    image1.SetDirection(image2.GetDirection())\n",
    "    image1.SetOrigin(image2.GetOrigin())\n",
    "    image1.SetSpacing(image2.GetSpacing())\n",
    "    return image1, image2\n",
    "\n",
    "def isAligned(image1, image2):\n",
    "    return image1.GetDirection() == image2.GetDirection() and image1.GetOrigin() == image2.GetOrigin() and image1.GetSpacing() == image2.GetSpacing()   \n",
    "\n",
    "def resampleSizes(wholeHeader, segmentHeader):\n",
    "    \"\"\" \n",
    "    Resamples the sitk image to have the same size based on the one with the largest size.\n",
    "    \"\"\"\n",
    "    if wholeHeader.GetSize()[-1] >= segmentHeader.GetSize()[-1]:\n",
    "        imageLarge = wholeHeader\n",
    "        imageSmall = segmentHeader\n",
    "        wholeThenSegmentOrder = True\n",
    "    else:\n",
    "        imageLarge = segmentHeader\n",
    "        imageSmall = wholeHeader \n",
    "        wholeThenSegmentOrder = False\n",
    "\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetReferenceImage(imageLarge)  \n",
    "    resample.SetInterpolator(sitk.sitkLinear)  # Choose the interpolation method (sitkLinear, sitkNearestNeighbor, etc.)\n",
    "    resample.SetDefaultPixelValue(0)  # Set default pixel value for areas outside the original image\n",
    "\n",
    "    imageSmall = resample.Execute(imageSmall)\n",
    "\n",
    "    print(f'imageLarge: {imageLarge.GetSize()}')\n",
    "    print(f'imageSmall: {imageSmall.GetSize()}')\n",
    "    \n",
    "\n",
    "    if wholeThenSegmentOrder:\n",
    "        return imageLarge, imageSmall # whole, then segment\n",
    "    else:\n",
    "        return imageSmall, imageLarge # segment, then whole\n",
    "    \n",
    "def twoImageAlignProceess(wholeHeader,segmentHeader,verbose):    \n",
    "\n",
    "    error = False\n",
    "    # Check if the images are aligned\n",
    "    wholeHeader, segmentHeader = makeAlign(wholeHeader, segmentHeader)\n",
    "    imagesAreAligned = isAligned(wholeHeader, segmentHeader)\n",
    "    print(f'Are the two images aligned now?: {imagesAreAligned}' if verbose==2 else '',end='')\n",
    "\n",
    "    if not imagesAreAligned:\n",
    "        error = True\n",
    "        return None, None, True\n",
    "    \n",
    "    # Set the spacing of the image to 1x1x1mm voxel spacing\n",
    "    wholeHeader.SetSpacing([1,1,1])\n",
    "    segmentHeader.SetSpacing([1,1,1])\n",
    "    imagesSpacingAligned = wholeHeader.GetSpacing() == segmentHeader.GetSpacing() \n",
    "    print(f'Are the two images aligned in terms of spacing?: {imagesSpacingAligned}' if verbose==2 else '',end='')\n",
    "\n",
    "    if not imagesSpacingAligned:\n",
    "        error = True\n",
    "        return None, None, True\n",
    "    \n",
    "    imagesSizeAligned = wholeHeader.GetSize() == segmentHeader.GetSize() \n",
    "    print(f'Are the two images aligned in terms of size?: {imagesSizeAligned}' if verbose==2 else '',end='')\n",
    "\n",
    "    if not imagesSizeAligned:\n",
    "        wholeHeader, segmentHeader = resampleSizes(wholeHeader, segmentHeader)\n",
    "        print(f'whole size: {wholeHeader.GetSize()}')\n",
    "        print(f'segment size: {segmentHeader.GetSize()}')\n",
    "        imagesSizeAligned = wholeHeader.GetSize() == segmentHeader.GetSize() \n",
    "        print(f'Are the two images aligned in terms of size now?: {imagesSizeAligned}' if verbose==2 else '',end='')\n",
    "        if not imagesSizeAligned:\n",
    "            error = True\n",
    "            return None, None, True\n",
    "\n",
    "    return wholeHeader, segmentHeader, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image changing and conversion\n",
    "#==========================================================================================\n",
    "\n",
    "def window_image_to_adbomen(image, window_center, window_width):\n",
    "    img_max = window_center + int(window_width / 2)\n",
    "    img_min = window_center - int(window_width / 2)\n",
    "    return np.clip(image, img_min, img_max)\n",
    "\n",
    "def centerXYOfImage(overlay_mask, segment_mask, segmentedSlices, padding=(5,5), scaledBoxes = None):\n",
    "    \"\"\" \n",
    "    Centers the X and Y of the image to crop the image. segmentedSlices is given as an array of z-value slices because the same approach to x_indicies and y_indicies does not work on overlay_segment (works for x and y though)\n",
    "    \"\"\"\n",
    "    _, x_indices, y_indices = np.where(segment_mask == 1)\n",
    "    # Get the bounding box for x and y dimensions\n",
    "    min_x, max_x = x_indices.min(), x_indices.max()\n",
    "    min_y, max_y = y_indices.min(), y_indices.max()\n",
    "\n",
    "    center_x = (min_x + max_x) // 2\n",
    "    center_y = (min_y + max_y) // 2\n",
    "\n",
    "    if scaledBoxes == None: # Define width and height in regards of the single image\n",
    "        width = abs(max_x - min_x) // 2\n",
    "        height = abs(max_y - min_y) // 2\n",
    "    else:\n",
    "        width = scaledBoxes[0]//2\n",
    "        height = scaledBoxes[1]//2\n",
    "\n",
    "    # Get the dimensions of the cropped image\n",
    "    start_x = max(0, center_x - width - padding[0]//2)\n",
    "    end_x = min(segment_mask.shape[1], center_x + width + padding[0]//2)\n",
    "    start_y = max(0, center_y - height - padding[1]//2)\n",
    "    end_y = min(segment_mask.shape[2], center_y + height + padding[1]//2)\n",
    "\n",
    "    return overlay_mask[np.array(segmentedSlices), start_x:end_x, start_y:end_y]\n",
    "\n",
    "def resizeImageToDesiredResolution(images, resolution = (232,232)):\n",
    "    \"\"\" Resize the image to the given resoloution\"\"\"\n",
    "    resizedImages = []\n",
    "\n",
    "    print('\\nImages Shape:', images.shape)\n",
    "    print(f\"Input array dtype: {images.dtype}\")\n",
    "    for idx in range(images.shape[0]):\n",
    "        try:\n",
    "            image = images[idx,:,:]\n",
    "            image = image.reshape((image.shape[0],image.shape[1])).astype('float32') #Need to convert to float to resize image\n",
    "            image = cv2.resize(image, resolution, interpolation=cv2.INTER_CUBIC)\n",
    "            resizedImages.append(image) #Conver the image back to int32 after resizing\n",
    "             \n",
    "        except cv2.error as e:\n",
    "            print(f\"Error during resizing slice {idx}: {e}\")\n",
    "\n",
    "    resizedImages = np.array(resizedImages)\n",
    "    print(f\"Output array size after CV2: {resizedImages.shape}\")\n",
    "    return resizedImages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying segments\n",
    "#==========================================================================================\n",
    "\n",
    "def displayCroppedSegmentations(croppedSegment):\n",
    "    print(f'CroppedSegment shape: {croppedSegment.shape}')\n",
    "    # Display the segmented image slices \n",
    "\n",
    "    columnLen = 10\n",
    "    rowLen = max(2,croppedSegment.shape[0] // columnLen + 1) \n",
    "    figure,axis = plt.subplots( rowLen, columnLen, figsize=(10, 10))\n",
    "    \n",
    "    rowIdx = 0\n",
    "    for idx in range(croppedSegment.shape[0]):        \n",
    "        if idx%columnLen == 0 and idx>0:\n",
    "            rowIdx += 1        \n",
    "        axis[rowIdx][idx%columnLen].imshow(croppedSegment[idx,:,:] , cmap=\"gray\")\n",
    "        axis[rowIdx][idx%columnLen].axis('off')\n",
    "\n",
    "    # Turn off the axis of the rest of the subplots\n",
    "    for i in range(idx+1, rowLen*columnLen):\n",
    "        if i%columnLen == 0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][i%columnLen].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def displayOverlayedSegmentations(segmentedSlices, augmented_whole, augmented_segment):\n",
    "    # Display the segmented image slices \n",
    "    columnLen = 10\n",
    "    rowLen = max(2,len(segmentedSlices) // columnLen + 1) \n",
    "    figure,axis = plt.subplots( rowLen, columnLen, figsize=(20, 20))\n",
    "    rowIdx = 0\n",
    "    for idx in range(len(segmentedSlices)):        \n",
    "        if idx%columnLen == 0 and idx>0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][idx%columnLen].imshow(augmented_whole[segmentedSlices[idx],:,:], cmap=\"gray\")\n",
    "        axis[rowIdx][idx%columnLen].imshow(augmented_segment[segmentedSlices[idx],:,:], cmap=\"Blues\", alpha=0.25)\n",
    "        axis[rowIdx][idx%columnLen].axis('off')\n",
    "\n",
    "    # Turn off the axis of the rest of the subplots\n",
    "    for i in range(idx+1, rowLen*columnLen):\n",
    "        if i%columnLen == 0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][i%columnLen].axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting optimal slice(s) to use\n",
    "#==========================================================================================\n",
    "\n",
    "\n",
    "    \n",
    "def getLargestSlice(mask):\n",
    " \n",
    "    segmentedSlices = [] \n",
    "    for index in range(mask.shape[0]):\n",
    "        if len(np.unique(mask[index,:,:])) > 1:\n",
    "            segmentedSlices.append(index)\n",
    "\n",
    "    mask_areas = []\n",
    "    for slice_ in segmentedSlices:\n",
    "        mask_areas.append(np.sum(mask[slice_,:,:]))\n",
    "\n",
    "\n",
    "    indices = [i for i in range(len(segmentedSlices))]\n",
    "\n",
    "    values = dict(zip(mask_areas, indices))\n",
    "    values = dict(sorted(values.items()))\n",
    "    \n",
    "    return list(values.values())[-1], values\n",
    "\n",
    "    \n",
    "\n",
    "# def getLargestSlice(croppedSegment):\n",
    "#     \"\"\"\n",
    "#     Finds the index with the largest slice in the croppedSegment and returns the index as well as the sorted number of slices each index has\n",
    "#     \"\"\"\n",
    "#     max = 0\n",
    "#     maxIndex = 0\n",
    "    \n",
    "#     indices = []\n",
    "#     sliceTotals = []\n",
    "#     for idx in range(croppedSegment.shape[0]):\n",
    "#         unique, counts = np.unique(croppedSegment[idx,:,:], return_counts=True)\n",
    "#         values = dict(zip(unique, counts))\n",
    "#         sliceTotal = 0\n",
    "#         for value,count in values.items():\n",
    "#             sliceTotal += count if value > 0 else 0 \n",
    "        \n",
    "#         indices.append(idx)\n",
    "#         sliceTotals.append(sliceTotal)\n",
    "        \n",
    "#         if sliceTotal > max: \n",
    "#             max = sliceTotal\n",
    "#             maxIndex = idx \n",
    "\n",
    "#     values = dict(zip(sliceTotals,indices))\n",
    "#     values = dict(sorted(values.items())) # Sort the values by number of slices\n",
    "\n",
    "#     return maxIndex, values\n",
    "\n",
    "def updateSlices(croppedSegment, mask, desiredNumberOfSlices=1):\n",
    "    \"\"\"\n",
    "    Updates the number of slices to the number of slices given. \n",
    "    If the numberOfSlices > the number of slices in the croppedSegment, it will duplicate the slices of the largest slices \n",
    "    If the numberOfSlices < the number of slices in the croppedSegment, it will remove the slices with the least amount of information \n",
    "    If the numberOfSlices == the number of slices in the croppedSegment, it will do nothing     \n",
    "    \"\"\"\n",
    "    print('desiredNumberOfSlices',desiredNumberOfSlices)\n",
    "    if croppedSegment.shape[0] == desiredNumberOfSlices:\n",
    "        return croppedSegment\n",
    "    elif croppedSegment.shape[0] < desiredNumberOfSlices: # Duplicate slices from the largest slice\n",
    "\n",
    "        # Specifications of croppedSegment\n",
    "        original = np.copy(croppedSegment)\n",
    "        largestSliceIndex, _ = getLargestSlice(mask)\n",
    "        maxUpperBound = croppedSegment.shape[0] -1\n",
    "        minLowerBound = 0\n",
    "        \n",
    "        # Specification of the values to duplicate\n",
    "        numToDuplication = desiredNumberOfSlices - croppedSegment.shape[0] \n",
    "        ends = numToDuplication//2\n",
    "        lowerRemainder = abs(largestSliceIndex - ends) if (largestSliceIndex - ends) < minLowerBound else 0   \n",
    "        upperRemainder = abs(maxUpperBound - (largestSliceIndex + ends)) if largestSliceIndex + ends > maxUpperBound else 0 \n",
    "\n",
    "        #Printing of the of the specifications\n",
    "        print(f'LargestSegmentIdx = {largestSliceIndex}\\nNumber of slices to duplicate: {numToDuplication}\\n Ends: {ends}, \\nlowerRemainder: {lowerRemainder},\\n upperRemainder: {upperRemainder}')\n",
    "        \n",
    "        #Making of the range to center the slices to duplicate\n",
    "        duplicationRange = list(range( largestSliceIndex - ends - upperRemainder + lowerRemainder , largestSliceIndex + ends + lowerRemainder - upperRemainder))\n",
    "\n",
    "        print('preAdd',duplicationRange)\n",
    "        #Edge case where we only need 1 extra slice\n",
    "        if len(duplicationRange) == 0:\n",
    "            duplicationRange = [largestSliceIndex]\n",
    "\n",
    "        # Fixes the slices if we are off by 1\n",
    "        if len(duplicationRange)+croppedSegment.shape[0] == desiredNumberOfSlices: \n",
    "            pass \n",
    "        else:\n",
    "            # Add to the right side if the left will be out of bounds\n",
    "            if duplicationRange[-1] -1 < minLowerBound:\n",
    "                duplicationRange = duplicationRange + [duplicationRange[-1] + 1]\n",
    "            # Add to the left side if the right will be out of bounds\n",
    "            elif duplicationRange[-1] +1 > maxUpperBound:\n",
    "                duplicationRange = [duplicationRange[0] - 1] + duplicationRange\n",
    "            else: #Default, add to the right side\n",
    "                duplicationRange = duplicationRange + [duplicationRange[-1] + 1]\n",
    "                \n",
    "        print(f'CroppedSlices={list(range(0,croppedSegment.shape[0]))}\\nSlices: {duplicationRange}')\n",
    "        print(len(duplicationRange)+croppedSegment.shape[0])\n",
    "        assert(len(duplicationRange)+croppedSegment.shape[0]== desiredNumberOfSlices) #Ensure that the desired number of slices is met\n",
    "\n",
    "        #Insert the values\n",
    "        croppedSegment = np.insert(croppedSegment, duplicationRange, original[duplicationRange,:,:], axis=0)\n",
    "            \n",
    "        print('greater than')\n",
    "        return croppedSegment\n",
    "    else:\n",
    "        # Specifications of croppedSegment\n",
    "        _, sliceValues = getLargestSlice(mask)\n",
    "        numberOfSlicesToRemove =  croppedSegment.shape[0] - desiredNumberOfSlices \n",
    "        print(croppedSegment.shape)\n",
    "        # Remove the slices with the least amount of information\n",
    "        print(list(sliceValues.values()))\n",
    "        print(list(sliceValues.values())[:numberOfSlicesToRemove])\n",
    "        \n",
    "        croppedSegment = np.delete(croppedSegment,list(sliceValues.values())[:numberOfSlicesToRemove], axis=0)\n",
    "        print(croppedSegment.shape, f'Removed this many slices: {numberOfSlicesToRemove}')\n",
    "        print('Less than')\n",
    "        return croppedSegment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting and finding the optimal dimensions for the bounding box\n",
    "#==========================================================================================\n",
    "def getSegmentBoxDimensions(preSegmentHeader):\n",
    "    segment = sitk.GetArrayFromImage(preSegmentHeader)\n",
    "\n",
    "    _, x_indices, y_indices = np.where(segment == 1)\n",
    "    # Get the bounding box for x and y dimensions\n",
    "    min_x, max_x = x_indices.min(), x_indices.max()\n",
    "    min_y, max_y = y_indices.min(), y_indices.max()\n",
    "\n",
    "    width = abs(max_x - min_x)\n",
    "    height = abs(max_y - min_y)\n",
    "    \n",
    "    return width, height\n",
    "\n",
    "def findLargestBoxSize(cases): \n",
    "    allFolders = ['CASE244','CASE246','CASE247','CASE251','CASE254','CASE256','CASE263','CASE264','CASE265','CASE270','CASE272','CASE274',\n",
    "                    'CASE467','CASE468','CASE470','CASE471','CASE472','CASE476','CASE479','CASE480','CASE482','CASE484','CASE485','CASE488','CASE494','CASE496','CASE499',\n",
    "                    'CASE500','CASE505','CASE515','CASE520','CASE523','CASE525','CASE531','CASE533','CASE534','CASE535','CASE537','CASE539','CASE541','CASE543','CASE546','CASE547','CASE548','CASE549','CASE550','CASE551','CASE554','CASE555','CASE557','CASE559','CASE560','CASE562','CASE563','CASE564','CASE565','CASE568','CASE569','CASE572','CASE574','CASE575','CASE577','CASE578','CASE580','CASE581','CASE585','CASE586','CASE587','CASE588','CASE589','CASE593','CASE594','CASE596','CASE598',\n",
    "                    'CASE600','CASE601','CASE602','CASE603','CASE604','CASE605','CASE608','CASE610','CASE611','CASE615','CASE616','CASE621','CASE622','CASE623','CASE624','CASE629','CASE630','CASE632','CASE635']\n",
    "\n",
    "\n",
    "    onlySeeTheseCases = allFolders\n",
    "    baseFilepath = 'Pre-treatment-only-pres/'\n",
    "\n",
    "    largestWidth, largestHeight = 0,0\n",
    "    # Find the largest box that fits all slices\n",
    "    for folder in os.listdir(baseFilepath):\n",
    "        # Skip cases that are not in the excel sheet\n",
    "        if folder not in cases:\n",
    "            continue\n",
    "        # Exclude to cases that we haven't seen yet\n",
    "        if folder not in onlySeeTheseCases:\n",
    "            continue \n",
    "        count = 0\n",
    "        for file in os.listdir(os.path.join(baseFilepath,folder)):\n",
    "            if 'TUM' in file or 'SMV' in file: # pre-treatment segmentation \n",
    "                preSegmentHeader = sitk.ReadImage(os.path.join(baseFilepath,folder,file))\n",
    "            else: \n",
    "                continue\n",
    "\n",
    "            print(folder)\n",
    "            width, height = getSegmentBoxDimensions(preSegmentHeader)\n",
    "            print(f'Width= {width}, Height= {height} (Largest? {width > largestWidth} and {height > largestHeight})')\n",
    "            print('=============================')\n",
    "\n",
    "            # Get the largest width and height\n",
    "            if width > largestWidth:\n",
    "                largestWidth = width\n",
    "            if height > largestHeight:  \n",
    "                largestHeight = height\n",
    "\n",
    "\n",
    "    dimensions = (largestWidth,largestHeight)\n",
    "    return dimensions\n",
    "\n",
    "\n",
    "#print('Largest dimension', findLargestBoxSize(cases))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform preprocessing on multiple images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(wholeHeader, segmentHeader, verbose=0, useBackground = False, scaledBoxes = None, desiredSliceNumber=12):\n",
    "    \"\"\"\n",
    "    Preprocesses the wholeHeader and segmentHeader sitk images to be ready for augmentation \n",
    "    Verbose = 0: No output\n",
    "    Verbose = 1: Only the CT scans slices and the array of slices it uses\n",
    "    Verbose = 2: Everything\n",
    "    Verbose = 3: Show the segment mask on top of the whole CT scan\n",
    "\n",
    "    Returns: a np array windowed whole image, a np array cropped segment image to 64x64x[] resolution, and boolean error flag.\n",
    "    \"\"\"\n",
    "    error = False # Error flag to check if there was an error in the preprocessing\n",
    "\n",
    "    # Align the two images \n",
    "    wholeHeader, segmentHeader, error = twoImageAlignProceess(wholeHeader, segmentHeader, verbose) \n",
    "    if error:\n",
    "        return None, None, error\n",
    "    \n",
    "    # Convert the images into numpy arrays for further processing, take the transpose as the format is z,y,x\n",
    "    whole = sitk.GetArrayFromImage(wholeHeader)\n",
    "    segment = sitk.GetArrayFromImage(segmentHeader)\n",
    "\n",
    "    print(f'Spacing of whole:{whole.shape}' if verbose==2 else '',end='')\n",
    "    print(f'Spacing of segment:{segment.shape}' if verbose==2 else '',end='')\n",
    "    \n",
    "    # Windowing parameters for the abdomen\n",
    "    window_center = 40\n",
    "    window_width = 350\n",
    "    \n",
    "    # Window and resample the whole image\n",
    "    augmented_whole = window_image_to_adbomen(whole, window_center, window_width)\n",
    "\n",
    "\n",
    "    # Get the slice indices where the segment is present in \n",
    "    augmented_segment = segment\n",
    "    segmentedSlices = [] \n",
    "    for index in range(augmented_segment.shape[0]):\n",
    "        if len(np.unique(augmented_segment[index,:,:])) > 1:\n",
    "            segmentedSlices.append(index)\n",
    "\n",
    "    print(f'Segment slice indices:{segmentedSlices}' if verbose==2 else '',end='')\n",
    "\n",
    "    # Min-Max Normalize the values of the mask so the lower bound of the window is 0.0 and the upper bound is 1.0\n",
    "    augmented_whole = (augmented_whole - np.min(augmented_whole)) / (np.max(augmented_whole) - np.min(augmented_whole))\n",
    "    overlay_segment = augmented_whole * augmented_segment\n",
    "\n",
    "    \n",
    "\n",
    "    # Get the necessary padding for adding to the cropped image to make it a square\n",
    "    paddingDim = [10,10]\n",
    "    if scaledBoxes != None:\n",
    "        # Rectangle, needs to reshape. Add padding to the smaller of the two dimensions\n",
    "        print('Scaledboxes', scaledBoxes)\n",
    "        \n",
    "        difference = abs(scaledBoxes[1] - scaledBoxes[0])\n",
    "        if scaledBoxes[0] >= scaledBoxes[1]:\n",
    "            paddingWidth = difference\n",
    "            paddingHeight = 0 \n",
    "        else:\n",
    "            paddingWidth = 0\n",
    "            paddingHeight = difference\n",
    "        paddingDim[0] += paddingHeight\n",
    "        paddingDim[1] += paddingWidth\n",
    "\n",
    "\n",
    "    print('paddingDim', paddingDim)\n",
    "    # Crop the image to the center of the segmented region\n",
    "    if useBackground:\n",
    "        croppedSegment = centerXYOfImage(augmented_whole,augmented_segment,segmentedSlices, padding=paddingDim, scaledBoxes = scaledBoxes) \n",
    "    else:\n",
    "        croppedSegment = centerXYOfImage(overlay_segment,augmented_segment,segmentedSlices, padding=paddingDim, scaledBoxes = scaledBoxes)      \n",
    "\n",
    "    # Resize image to 224x224, or 299x299\n",
    "    #croppedSegment = resizeImageToDesiredResolution(croppedSegment)\n",
    "\n",
    "\n",
    "    if desiredSliceNumber==1:\n",
    "        largestSlice, _ = getLargestSlice(augmented_segment)\n",
    "        croppedSegment = croppedSegment[largestSlice,:,:]\n",
    "    else:\n",
    "        croppedSegment = updateSlices(croppedSegment,augmented_segment,desiredSliceNumber)\n",
    "\n",
    "    #Display the results of preprocessing\n",
    "    if verbose==1 or verbose==2:\n",
    "        displayCroppedSegmentations(croppedSegment)\n",
    "    elif verbose==3:\n",
    "        displayOverlayedSegmentations(segmentedSlices, augmented_whole, augmented_segment)\n",
    "    \n",
    "    return whole, croppedSegment, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current System: d:\\SimpsonLab\\researchpip\\lib\\site-packages\\ipykernel_launcher.py\n",
      "Has Background: False, Uses Largest Box: True, Segments Multiple: 12\n"
     ]
    }
   ],
   "source": [
    "#ADD argparser\n",
    "import argparse\n",
    "import sys\n",
    "print('Current System:',sys.argv[0])\n",
    "\n",
    "# python testSamples22-7.py -batchSize=8 -epochs=100 -lr=0.001 -evalDetailLine=\"\" -hasBackground=False -usesGlobalSize=True\n",
    "#Current\n",
    "# python testSamples31-7-224x224.py -batchSize=8 -epochs=100 -lr=0.001 -evalDetailLine=\"view with 224x224 majourity voting\" -hasBackground=t -usesLargestBox=f -segmentsMultiple=12 -dropoutRate=0.2 \n",
    "\n",
    "# python generatePreprocessCombinations.py -hasBackground=f -usesLargestBox=f -segmentsMultiple=12 &&\n",
    "# python generatePreprocessCombinations.py -hasBackground=t -usesLargestBox=t -segmentsMultiple=12 &&\n",
    "# python generatePreprocessCombinations.py -hasBackground=f -usesLargestBox=t -segmentsMultiple=12 &&\n",
    "# python generatePreprocessCombinations.py -hasBackground=t -usesLargestBox=f -segmentsMultiple=12 \n",
    "\n",
    "\n",
    "#Check if we are using a notebook or not\n",
    "if 'ipykernel_launcher' in sys.argv[0]:\n",
    "    hasBackground = False\n",
    "    usesLargestBox = True\n",
    "    segmentsMultiple = 12\n",
    "\n",
    "\n",
    "else:\n",
    "    parser = argparse.ArgumentParser(description=\"Model information\")\n",
    "    parser.add_argument('-hasBackground', type=str, help='Whether to use the background (t to, f to not)', default='f')\n",
    "    parser.add_argument('-usesLargestBox', type=str, help='Where to use the size of the largest box (t) or independent tumor boxes (f)', default='t')\n",
    "    parser.add_argument('-segmentsMultiple', type=int, help='Segments a # of slices, 12 by default', default=12)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    hasBackground = True if args.hasBackground=='t' else False\n",
    "    usesLargestBox = True if args.usesLargestBox=='t' else False\n",
    "    segmentsMultiple = args.segmentsMultiple\n",
    "\n",
    "\n",
    "\n",
    "print(f'Has Background: {hasBackground}, Uses Largest Box: {usesLargestBox}, Segments Multiple: {segmentsMultiple}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using box dimension of: (82, 109)\n",
      "==============================================================\n",
      "CASE244 All files read:\n",
      "Scaledboxes (82, 109)\n",
      "paddingDim [37, 10]\n",
      "desiredNumberOfSlices 6\n",
      "(17, 118, 118)\n",
      "[0, 16, 14, 1, 15, 2, 13, 11, 12, 3, 10, 8, 4, 9, 7, 6, 5]\n",
      "[0, 16, 14, 1, 15, 2, 13, 11, 12, 3, 10]\n",
      "(6, 118, 118) Removed this many slices: 11\n",
      "Less than\n",
      "Final Shape (6, 118, 118)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 140 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6, 118, 118)\n"
     ]
    }
   ],
   "source": [
    "def generatePreprocessCombinations(hasBackground, usesLargestBox, segmentsMultiple):\n",
    "    allFolders = ['CASE244','CASE246','CASE247','CASE251','CASE254','CASE256','CASE263','CASE264','CASE265','CASE270','CASE272','CASE274',\n",
    "                    'CASE467','CASE468','CASE470','CASE471','CASE472','CASE476','CASE479','CASE480','CASE482','CASE484','CASE485','CASE488','CASE494','CASE496','CASE499',\n",
    "                    'CASE500','CASE505','CASE515','CASE520','CASE523','CASE525','CASE531','CASE533','CASE534','CASE535','CASE537','CASE539','CASE541','CASE543','CASE546','CASE547','CASE548','CASE549','CASE550','CASE551','CASE554','CASE555','CASE557','CASE559','CASE560','CASE562','CASE563','CASE564','CASE565','CASE568','CASE569','CASE572','CASE574','CASE575','CASE577','CASE578','CASE580','CASE581','CASE585','CASE586','CASE587','CASE588','CASE589','CASE593','CASE594','CASE596','CASE598',\n",
    "                    'CASE600','CASE601','CASE602','CASE603','CASE604','CASE605','CASE608','CASE610','CASE611','CASE615','CASE616','CASE621','CASE622','CASE623','CASE624','CASE629','CASE630','CASE632','CASE635']\n",
    "\n",
    "\n",
    "    # onlySeeTheseCases = allFolders#['CASE537','CASE585','CASE587']\n",
    "    onlySeeTheseCases = ['CASE244']\n",
    "    baseFilepath = 'Pre-treatment-only-pres/'\n",
    "    desiredSliceNumber=segmentsMultiple\n",
    "    croppedSegmentsList = []\n",
    "    groups = []\n",
    "\n",
    "    # Decide to use what size of box, either the largest box size for all images or one that just fits the tumor and resizes\n",
    "    if usesLargestBox: # This is the largest of the whole dataset, including all patients\n",
    "        dimensions = (82, 109)\n",
    "        #dimensions = findLargestBoxSize(cases) #Find the largest dimension\n",
    "    else:\n",
    "        dimensions = None\n",
    "    print('Using box dimension of:',dimensions)\n",
    "\n",
    "\n",
    "    # Get all cropped segments\n",
    "    for folder in sorted(os.listdir(baseFilepath)):\n",
    "        # Skip cases that are not in the excel sheet\n",
    "        if folder not in cases:\n",
    "            continue\n",
    "        # Exclude to cases that we haven't seen yet\n",
    "        if folder not in onlySeeTheseCases:\n",
    "            continue \n",
    "        count = 0\n",
    "        preSegmentHeader = None\n",
    "        wholeHeader = None\n",
    "        for file in os.listdir(os.path.join(baseFilepath,folder)):\n",
    "                \n",
    "            if 'TUM' in file or 'SMV' in file: # pre-treatment segmentation \n",
    "                # segment, segmentHeader = nrrd.read(os.path.join(baseFilepath,folder,file))\n",
    "                preSegmentHeader = sitk.ReadImage(os.path.join(baseFilepath,folder,file))\n",
    "            elif file.endswith('CT.nrrd'): # whole ct scan\n",
    "                wholeHeader = sitk.ReadImage(os.path.join(baseFilepath,folder,file))\n",
    "        \n",
    "        print('==============================================================')\n",
    "        print(folder, 'All files read:')\n",
    "        \n",
    "        whole, croppedSegment,error = preprocess(wholeHeader, preSegmentHeader, verbose=0, useBackground=hasBackground, scaledBoxes=dimensions, desiredSliceNumber=segmentsMultiple) \n",
    "        if error:\n",
    "            print('Error in preprocessing')\n",
    "            # continue\n",
    "        \n",
    "        print('Final Shape', croppedSegment.shape)\n",
    "        # groups += [folder]*desiredSliceNumber # Add the segment slices to the group\n",
    "            \n",
    "        croppedSegmentsList.append(croppedSegment)\n",
    "\n",
    "\n",
    "        # ====================================\n",
    "        whole = sitk.GetArrayFromImage(wholeHeader)\n",
    "        segment = whole = sitk.GetArrayFromImage(preSegmentHeader)\n",
    "        displayOverlayedSegmentations([i for i in range(whole.shape[0])], whole, segment)\n",
    "        # augmented_whole = window_image_to_adbomen(whole, window_center, window_width)\n",
    "        # displayCroppedSegmentations(whole)\n",
    "        # displayCroppedSegmentations(croppedSegment)\n",
    "        \n",
    "    print(np.array(croppedSegmentsList).shape)\n",
    "    #Save the results of the different combinations of backgrounds and sizes\n",
    "    # name = f'preprocessCombinations/hasBackground={hasBackground}-usesLargestBox={usesLargestBox}-segmentsMultiple={segmentsMultiple}-size=(119,119)'\n",
    "\n",
    "    # np.save(f'{name}.npy',np.array(croppedSegmentsList))\n",
    "\n",
    "    # def loadFromNumpySave(name):\n",
    "    #     data = np.load(f'{name}.npy')\n",
    "    #     return data\n",
    "\n",
    "    # readingTemp = loadFromNumpySave(name)\n",
    "    # print('readingTemp Shape:', readingTemp.shape)     \n",
    "\n",
    "\n",
    "# generatePreprocessCombinations(hasBackground, usesLargestBox, segmentsMultiple)\n",
    "generatePreprocessCombinations(True, True, 6)\n",
    "# generatePreprocessCombinations(False, False, 1)\n",
    "# generatePreprocessCombinations(True, False, 1)\n",
    "# generatePreprocessCombinations(False, True, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3430182940.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    CUDA_VISIBILE_DEVICES=0 python testSamples4-8-inceptionv3-finetune.py -batchSize=16 -epochs=100 -lr=0.001 -evalDetailLine='Small2D undersampled with 224x224 singleLargest' -hasBackground=t -usesLargestBox=t -segmentsMultiple=1 -dropoutRate=0.2 -grouped2D=f -modelChosen='Small2D' -votingSystem='singleLargest' ;\u001b[0m\n\u001b[1;37m                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CUDA_VISIBILE_DEVICES=0 python testSamples4-8-inceptionv3-finetune.py -batchSize=16 -epochs=100 -lr=0.001 -evalDetailLine='Small2D undersampled with 224x224 singleLargest' -hasBackground=t -usesLargestBox=t -segmentsMultiple=1 -dropoutRate=0.2 -grouped2D=f -modelChosen='Small2D' -votingSystem='singleLargest' ;\n",
    "CUDA_VISIBILE_DEVICES=0 python testSamples4-8-inceptionv3-finetune.py -batchSize=16 -epochs=100 -lr=0.001 -evalDetailLine='Small2D undersampled with 224x224 average voting' -hasBackground=t -usesLargestBox=t -segmentsMultiple=1 -dropoutRate=0.2 -grouped2D=f -modelChosen='Small2D' -votingSystem='average'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "readingTemp Shape: (89, 6, 232, 232)\n"
     ]
    }
   ],
   "source": [
    "# name = f'preprocessCombinations/hasBackground={hasBackground}-usesLargestBox={usesLargestBox}-segmentsMultiple={segmentsMultiple}'\n",
    "name = f'preprocessCombinations/hasBackground=True-usesLargestBox=True-segmentsMultiple=6-size=(119,119)'\n",
    "\n",
    "# np.save(f'{name}.npy',np.array(croppedSegmentsList))\n",
    "    \n",
    "def loadFromNumpySave(name):\n",
    "    data = np.load(f'{name}.npy')\n",
    "    return data\n",
    "\n",
    "readingTemp = loadFromNumpySave(name)\n",
    "print('readingTemp Shape:', readingTemp.shape)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
