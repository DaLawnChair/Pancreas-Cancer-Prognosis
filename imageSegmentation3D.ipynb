{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook imageSegmentationWithBackground.ipynb to python\n",
      "[NbConvertApp] Writing 60187 bytes to testSamples24-7.py\n"
     ]
    }
   ],
   "source": [
    "# Convert to python script, remember to delete/comment the next line in the actual file\n",
    "# ! jupyter nbconvert --to python imageSegmentationWithBackground.ipynb --output testSamples24-7.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### # Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image reading and file handling \n",
    "import pandas as pd\n",
    "import SimpleITK as sitk \n",
    "import os \n",
    "import shutil\n",
    "\n",
    "# Image agumentaitons \n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# import scipy\n",
    "\n",
    "# Information saving\n",
    "import pickle\n",
    "\n",
    "# Train test set spliting\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# Dataset building\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Model building\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "\n",
    "# Evaluation metrics and Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip freeze > requirements.txt\n",
    "# ! pip uninstall -y -r requirements.txt\n",
    "\n",
    "## Download necessary packages \n",
    "# ! pip install matplotlib opencv-python scipy simpleitk pandas openpyxl scikit-learn nbconvert\n",
    "# ! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 \n",
    "\n",
    "## For 3D image classification\n",
    "# ! pip install foundation-cancer-image-biomarker -qq\n",
    "# ! pip install foundation-cancer-image-biomarker\n",
    "# ! pip install torchio\n",
    "\n",
    "## Check python version and packages\n",
    "# ! python --version\n",
    "# ! pip freeze > research.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data from the .xsxl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['TAPS_CaseIDs_PreNAT','RECIST_PostNAT', 'Slice_Thickness']\n",
    "data = pd.read_excel('PDAC-Response_working.xlsx', header=None,names = columns)\n",
    "data.drop(0, inplace=True) # Remove the header row\n",
    "data=data.sort_values(by=['TAPS_CaseIDs_PreNAT'])\n",
    "# # Get the entire datasheet\n",
    "cases = list(data['TAPS_CaseIDs_PreNAT'])\n",
    "recistCriteria = list(data['RECIST_PostNAT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the images aligned if not\n",
    "#==========================================================================================\n",
    "def makeAlign(image1,image2):\n",
    "    image1.SetDirection(image2.GetDirection())\n",
    "    image1.SetOrigin(image2.GetOrigin())\n",
    "    image1.SetSpacing(image2.GetSpacing())\n",
    "    return image1, image2\n",
    "\n",
    "def isAligned(image1, image2):\n",
    "    return image1.GetDirection() == image2.GetDirection() and image1.GetOrigin() == image2.GetOrigin() and image1.GetSpacing() == image2.GetSpacing()   \n",
    "\n",
    "def resampleSizes(wholeHeader, segmentHeader):\n",
    "    \"\"\" \n",
    "    Resamples the sitk image to have the same size based on the one with the largest size.\n",
    "    \"\"\"\n",
    "    if wholeHeader.GetSize()[-1] >= segmentHeader.GetSize()[-1]:\n",
    "        imageLarge = wholeHeader\n",
    "        imageSmall = segmentHeader\n",
    "        wholeThenSegmentOrder = True\n",
    "    else:\n",
    "        imageLarge = segmentHeader\n",
    "        imageSmall = wholeHeader \n",
    "        wholeThenSegmentOrder = False\n",
    "\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetReferenceImage(imageLarge)  \n",
    "    resample.SetInterpolator(sitk.sitkLinear)  # Choose the interpolation method (sitkLinear, sitkNearestNeighbor, etc.)\n",
    "    resample.SetDefaultPixelValue(0)  # Set default pixel value for areas outside the original image\n",
    "\n",
    "    imageSmall = resample.Execute(imageSmall)\n",
    "\n",
    "    print(f'imageLarge: {imageLarge.GetSize()}')\n",
    "    print(f'imageSmall: {imageSmall.GetSize()}')\n",
    "    \n",
    "\n",
    "    if wholeThenSegmentOrder:\n",
    "        return imageLarge, imageSmall # whole, then segment\n",
    "    else:\n",
    "        return imageSmall, imageLarge # segment, then whole\n",
    "    \n",
    "def twoImageAlignProceess(wholeHeader,segmentHeader,verbose):    \n",
    "\n",
    "    error = False\n",
    "    # Check if the images are aligned\n",
    "    wholeHeader, segmentHeader = makeAlign(wholeHeader, segmentHeader)\n",
    "    imagesAreAligned = isAligned(wholeHeader, segmentHeader)\n",
    "    print(f'Are the two images aligned now?: {imagesAreAligned}' if verbose==2 else '',end='')\n",
    "\n",
    "    if not imagesAreAligned:\n",
    "        error = True\n",
    "        return None, None, True\n",
    "    \n",
    "    # Set the spacing of the image to 1x1x1mm voxel spacing\n",
    "    wholeHeader.SetSpacing([1,1,1])\n",
    "    segmentHeader.SetSpacing([1,1,1])\n",
    "    imagesSpacingAligned = wholeHeader.GetSpacing() == segmentHeader.GetSpacing() \n",
    "    print(f'Are the two images aligned in terms of spacing?: {imagesSpacingAligned}' if verbose==2 else '',end='')\n",
    "\n",
    "    if not imagesSpacingAligned:\n",
    "        error = True\n",
    "        return None, None, True\n",
    "    \n",
    "    imagesSizeAligned = wholeHeader.GetSize() == segmentHeader.GetSize() \n",
    "    print(f'Are the two images aligned in terms of size?: {imagesSizeAligned}' if verbose==2 else '',end='')\n",
    "\n",
    "    if not imagesSizeAligned:\n",
    "        wholeHeader, segmentHeader = resampleSizes(wholeHeader, segmentHeader)\n",
    "        print(f'whole size: {wholeHeader.GetSize()}')\n",
    "        print(f'segment size: {segmentHeader.GetSize()}')\n",
    "        imagesSizeAligned = wholeHeader.GetSize() == segmentHeader.GetSize() \n",
    "        print(f'Are the two images aligned in terms of size now?: {imagesSizeAligned}' if verbose==2 else '',end='')\n",
    "        if not imagesSizeAligned:\n",
    "            error = True\n",
    "            return None, None, True\n",
    "\n",
    "    return wholeHeader, segmentHeader, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image changing and conversion\n",
    "#==========================================================================================\n",
    "\n",
    "def window_image_to_adbomen(image, window_center, window_width):\n",
    "    img_max = window_center + int(window_width / 2)\n",
    "    img_min = window_center - int(window_width / 2)\n",
    "    return np.clip(image, img_min, img_max)\n",
    "\n",
    "def centerXYOfImage(overlay_mask, segment_mask, segmentedSlices, padding=10, scaledBoxes = None):\n",
    "    \"\"\" \n",
    "    Centers the X and Y of the image to crop the image. segmentedSlices is given as an array of z-value slices because the same approach to x_indicies and y_indicies does not work on overlay_segment (works for x and y though)\n",
    "    \"\"\"\n",
    "    _, x_indices, y_indices = np.where(segment_mask == 1)\n",
    "    # Get the bounding box for x and y dimensions\n",
    "    min_x, max_x = x_indices.min(), x_indices.max()\n",
    "    min_y, max_y = y_indices.min(), y_indices.max()\n",
    "\n",
    "    center_x = (min_x + max_x) // 2\n",
    "    center_y = (min_y + max_y) // 2\n",
    "\n",
    "    if scaledBoxes == None: # Define width and height in regards of the single image\n",
    "        width = abs(max_x - min_x) // 2\n",
    "        height = abs(max_y - min_y) // 2\n",
    "    else:\n",
    "        width = scaledBoxes[0]//2\n",
    "        height = scaledBoxes[1]//2\n",
    "\n",
    "    # Get the dimensions of the cropped image\n",
    "    start_x = max(0, center_x - width - padding)\n",
    "    end_x = min(segment_mask.shape[1], center_x + width + padding)\n",
    "    start_y = max(0, center_y - height - padding)\n",
    "    end_y = min(segment_mask.shape[2], center_y + height + padding)\n",
    "\n",
    "    return overlay_mask[np.array(segmentedSlices), start_x:end_x, start_y:end_y]\n",
    "\n",
    "def convertNdArrayToCV2Image(images, resolution = (64,64)):\n",
    "    \"\"\" Converts a numpy array to a cv2 images\"\"\"\n",
    "    # images = images.T\n",
    "    if resolution == None:\n",
    "        resolution = (64,64)#(64,64, len(images))\n",
    "\n",
    "    images = images.T\n",
    "    resizedImages = []\n",
    "    print('\\nImages Shape:', images.shape)\n",
    "    print(f\"Input array dtype: {images.dtype}\")\n",
    "    for idx in range(images.shape[-1]):\n",
    "        try:\n",
    "            image = images[:,:,idx]\n",
    "            image = image.reshape((image.shape[0],image.shape[1])).astype('float32') #Need to convert to float to resize image\n",
    "            resizedImages.append(cv2.resize(image, resolution, interpolation=cv2.INTER_LINEAR).astype('int32')) #Conver the image back to int32 after resizing\n",
    "             \n",
    "        except cv2.error as e:\n",
    "            print(f\"Error during resizing slice {idx}: {e}\")\n",
    "\n",
    "    resizedImages = np.array(resizedImages)\n",
    "    print(f\"Output array size after CV2: {resizedImages.shape}\")\n",
    "    return resizedImages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying segments\n",
    "#==========================================================================================\n",
    "\n",
    "def displayCroppedSegmentations(croppedSegment):\n",
    "    print(f'CroppedSegment shape: {croppedSegment.shape}')\n",
    "    # Display the segmented image slices \n",
    "\n",
    "    columnLen = 10\n",
    "    rowLen = max(2,croppedSegment.shape[0] // columnLen + 1) \n",
    "    figure,axis = plt.subplots( rowLen, columnLen, figsize=(10, 10))\n",
    "    \n",
    "    rowIdx = 0\n",
    "    for idx in range(croppedSegment.shape[0]):        \n",
    "        if idx%columnLen == 0 and idx>0:\n",
    "            rowIdx += 1        \n",
    "        axis[rowIdx][idx%columnLen].imshow(croppedSegment[idx,:,:] , cmap=\"gray\", vmin = 40-(350)/2, vmax=40+(350)/2)\n",
    "        axis[rowIdx][idx%columnLen].axis('off')\n",
    "\n",
    "    # Turn off the axis of the rest of the subplots\n",
    "    for i in range(idx+1, rowLen*columnLen):\n",
    "        if i%columnLen == 0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][i%columnLen].axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def displayOverlayedSegmentations(segmentedSlices, augmented_whole, augmented_segment):\n",
    "    # Display the segmented image slices \n",
    "    columnLen = 10\n",
    "    rowLen = max(2,len(segmentedSlices) // columnLen + 1) \n",
    "    figure,axis = plt.subplots( rowLen, columnLen, figsize=(10, 10))\n",
    "    rowIdx = 0\n",
    "    for idx in range(len(segmentedSlices)):        \n",
    "        if idx%columnLen == 0 and idx>0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][idx%columnLen].imshow(augmented_whole[segmentedSlices[idx],:,:], cmap=\"gray\")\n",
    "        axis[rowIdx][idx%columnLen].imshow(augmented_segment[segmentedSlices[idx],:,:], cmap=\"Blues\", alpha=0.75)\n",
    "        axis[rowIdx][idx%columnLen].axis('off')\n",
    "\n",
    "    # Turn off the axis of the rest of the subplots\n",
    "    for i in range(idx+1, rowLen*columnLen):\n",
    "        if i%columnLen == 0:\n",
    "            rowIdx += 1\n",
    "        axis[rowIdx][i%columnLen].axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting optimal slice(s) to use\n",
    "#==========================================================================================\n",
    "\n",
    "def getLargestSlice(croppedSegment):\n",
    "    \"\"\"\n",
    "    Finds the index with the largest slice in the croppedSegment and returns the index as well as the sorted number of slices each index has\n",
    "    \"\"\"\n",
    "    max = 0\n",
    "    maxIndex = 0\n",
    "    \n",
    "    indices = []\n",
    "    sliceTotals = []\n",
    "    for idx in range(croppedSegment.shape[0]):\n",
    "        unique, counts = np.unique(croppedSegment[idx,:,:], return_counts=True)\n",
    "        values = dict(zip(unique, counts))\n",
    "        sliceTotal = 0\n",
    "        for value,count in values.items():\n",
    "            sliceTotal += count if value > 0 else 0 \n",
    "        \n",
    "        indices.append(idx)\n",
    "        sliceTotals.append(sliceTotal)\n",
    "        \n",
    "        if sliceTotal > max: \n",
    "            max = sliceTotal\n",
    "            maxIndex = idx \n",
    "\n",
    "    values = dict(zip(sliceTotals,indices))\n",
    "    values = dict(sorted(values.items())) # Sort the values by number of slices\n",
    "\n",
    "    return maxIndex, values\n",
    "\n",
    "def updateSlices(croppedSegment, desiredNumberOfSlices=1):\n",
    "    \"\"\"\n",
    "    Updates the number of slices to the number of slices given. \n",
    "    If the numberOfSlices > the number of slices in the croppedSegment, it will duplicate the slices of the largest slices \n",
    "    If the numberOfSlices < the number of slices in the croppedSegment, it will remove the slices with the least amount of information \n",
    "    If the numberOfSlices == the number of slices in the croppedSegment, it will do nothing     \n",
    "    \"\"\"\n",
    "    print('desiredNumberOfSlices',desiredNumberOfSlices)\n",
    "    if croppedSegment.shape[0] == desiredNumberOfSlices:\n",
    "        return croppedSegment\n",
    "    elif croppedSegment.shape[0] < desiredNumberOfSlices: # Duplicate slices from the largest slice\n",
    "\n",
    "        # Specifications of croppedSegment\n",
    "        original = np.copy(croppedSegment)\n",
    "        largestSliceIndex, _ = getLargestSlice(croppedSegment)\n",
    "        maxUpperBound = croppedSegment.shape[0] -1\n",
    "        minLowerBound = 0\n",
    "        \n",
    "        # Specification of the values to duplicate\n",
    "        numToDuplication = desiredNumberOfSlices - croppedSegment.shape[0] \n",
    "        ends = numToDuplication//2\n",
    "        lowerRemainder = abs(largestSliceIndex - ends) if (largestSliceIndex - ends) < minLowerBound else 0   \n",
    "        upperRemainder = abs(maxUpperBound - (largestSliceIndex + ends)) if largestSliceIndex + ends > maxUpperBound else 0 \n",
    "\n",
    "        #Printing of the of the specifications\n",
    "        print(f'LargestSegmentIdx = {largestSliceIndex}\\nNumber of slices to duplicate: {numToDuplication}\\n Ends: {ends}, \\nlowerRemainder: {lowerRemainder},\\n upperRemainder: {upperRemainder}')\n",
    "        \n",
    "        #Making of the range to center the slices to duplicate\n",
    "        duplicationRange = list(range( largestSliceIndex - ends - upperRemainder + lowerRemainder , largestSliceIndex + ends + lowerRemainder - upperRemainder))\n",
    "\n",
    "        print('preAdd',duplicationRange)\n",
    "        #Edge case where we only need 1 extra slice\n",
    "        if len(duplicationRange) == 0:\n",
    "            duplicationRange = [largestSliceIndex]\n",
    "\n",
    "        # Fixes the slices if we are off by 1\n",
    "        if len(duplicationRange)+croppedSegment.shape[0] == desiredNumberOfSlices: \n",
    "            pass \n",
    "        else:\n",
    "            # Add to the right side if the left will be out of bounds\n",
    "            if duplicationRange[-1] -1 < minLowerBound:\n",
    "                duplicationRange = duplicationRange + [duplicationRange[-1] + 1]\n",
    "            # Add to the left side if the right will be out of bounds\n",
    "            elif duplicationRange[-1] +1 > maxUpperBound:\n",
    "                duplicationRange = [duplicationRange[0] - 1] + duplicationRange\n",
    "            else: #Default, add to the right side\n",
    "                duplicationRange = duplicationRange + [duplicationRange[-1] + 1]\n",
    "                \n",
    "        print(f'CroppedSlices={list(range(0,croppedSegment.shape[0]))}\\nSlices: {duplicationRange}')\n",
    "        print(len(duplicationRange)+croppedSegment.shape[0])\n",
    "        assert(len(duplicationRange)+croppedSegment.shape[0]== desiredNumberOfSlices) #Ensure that the desired number of slices is met\n",
    "\n",
    "        #Insert the values\n",
    "        croppedSegment = np.insert(croppedSegment, duplicationRange, original[duplicationRange,:,:], axis=0)\n",
    "            \n",
    "        print('greater than')\n",
    "        return croppedSegment\n",
    "    else:\n",
    "        # Specifications of croppedSegment\n",
    "        _, sliceValues = getLargestSlice(croppedSegment)\n",
    "        numberOfSlicesToRemove =  croppedSegment.shape[0] - desiredNumberOfSlices \n",
    "        print(croppedSegment.shape)\n",
    "        # Remove the slices with the least amount of information\n",
    "        print(list(sliceValues.values()))\n",
    "        print(list(sliceValues.values())[:numberOfSlicesToRemove])\n",
    "        \n",
    "        croppedSegment = np.delete(croppedSegment,list(sliceValues.values())[:numberOfSlicesToRemove], axis=0)\n",
    "        print(croppedSegment.shape, f'Removed this many slices: {numberOfSlicesToRemove}')\n",
    "        print('Less than')\n",
    "        return croppedSegment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting and finding the optimal dimensions for the bounding box\n",
    "#==========================================================================================\n",
    "def getSegmentBoxDimensions(preSegmentHeader):\n",
    "    segment = sitk.GetArrayFromImage(preSegmentHeader)\n",
    "\n",
    "    _, x_indices, y_indices = np.where(segment == 1)\n",
    "    # Get the bounding box for x and y dimensions\n",
    "    min_x, max_x = x_indices.min(), x_indices.max()\n",
    "    min_y, max_y = y_indices.min(), y_indices.max()\n",
    "\n",
    "    width = abs(max_x - min_x)\n",
    "    height = abs(max_y - min_y)\n",
    "    \n",
    "    return width, height\n",
    "\n",
    "def findLargestBoxSize(cases): \n",
    "    allFolders = ['CASE244','CASE246','CASE247','CASE251','CASE254','CASE256','CASE263','CASE264','CASE265','CASE270','CASE272','CASE274',\n",
    "                    'CASE467','CASE468','CASE470','CASE471','CASE472','CASE476','CASE479','CASE480','CASE482','CASE484','CASE485','CASE488','CASE494','CASE496','CASE499',\n",
    "                    'CASE500','CASE505','CASE515','CASE520','CASE523','CASE525','CASE531','CASE533','CASE534','CASE535','CASE537','CASE539','CASE541','CASE543','CASE546','CASE547','CASE548','CASE549','CASE550','CASE551','CASE554','CASE555','CASE557','CASE559','CASE560','CASE562','CASE563','CASE564','CASE565','CASE568','CASE569','CASE572','CASE574','CASE575','CASE577','CASE578','CASE580','CASE581','CASE585','CASE586','CASE587','CASE588','CASE589','CASE593','CASE594','CASE596','CASE598',\n",
    "                    'CASE600','CASE601','CASE602','CASE603','CASE604','CASE605','CASE608','CASE610','CASE611','CASE615','CASE616','CASE621','CASE622','CASE623','CASE624','CASE629','CASE630','CASE632','CASE635']\n",
    "\n",
    "\n",
    "    onlySeeTheseCases = allFolders\n",
    "    baseFilepath = 'Pre-treatment-only-pres/'\n",
    "\n",
    "    largestWidth, largestHeight = 0,0\n",
    "    # Find the largest box that fits all slices\n",
    "    for folder in os.listdir(baseFilepath):\n",
    "        # Skip cases that are not in the excel sheet\n",
    "        if folder not in cases:\n",
    "            continue\n",
    "        # Exclude to cases that we haven't seen yet\n",
    "        if folder not in onlySeeTheseCases:\n",
    "            continue \n",
    "        count = 0\n",
    "        for file in os.listdir(os.path.join(baseFilepath,folder)):\n",
    "            if 'TUM' in file or 'SMV' in file: # pre-treatment segmentation \n",
    "                preSegmentHeader = sitk.ReadImage(os.path.join(baseFilepath,folder,file))\n",
    "            else: \n",
    "                continue\n",
    "\n",
    "            print(folder)\n",
    "            width, height = getSegmentBoxDimensions(preSegmentHeader)\n",
    "            print(f'Width= {width}, Height= {height} (Largest? {width > largestWidth} and {height > largestHeight})')\n",
    "            print('=============================')\n",
    "\n",
    "            # Get the largest width and height\n",
    "            if width > largestWidth:\n",
    "                largestWidth = width\n",
    "            if height > largestHeight:  \n",
    "                largestHeight = height\n",
    "\n",
    "\n",
    "    dimensions = (largestWidth,largestHeight)\n",
    "    return dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform preprocessing on multiple images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(wholeHeader, segmentHeader, verbose=0, useBackground = False, scaledBoxes = None):\n",
    "    \"\"\"\n",
    "    Preprocesses the wholeHeader and segmentHeader sitk images to be ready for augmentation \n",
    "    Verbose = 0: No output\n",
    "    Verbose = 1: Only the CT scans slices and the array of slices it uses\n",
    "    Verbose = 2: Everything\n",
    "    Verbose = 3: Show the segment mask on top of the whole CT scan\n",
    "\n",
    "    Returns: a np array windowed whole image, a np array cropped segment image to 64x64x[] resolution, and boolean error flag.\n",
    "    \"\"\"\n",
    "    error = False # Error flag to check if there was an error in the preprocessing\n",
    "\n",
    "    # Align the two images \n",
    "    wholeHeader, segmentHeader, error = twoImageAlignProceess(wholeHeader, segmentHeader, verbose) \n",
    "    if error:\n",
    "        return None, None, error\n",
    "    \n",
    "    # Convert the images into numpy arrays for further processing, take the transpose as the format is z,y,x\n",
    "    whole = sitk.GetArrayFromImage(wholeHeader)\n",
    "    segment = sitk.GetArrayFromImage(segmentHeader)\n",
    "\n",
    "    print(f'Spacing of whole:{whole.shape}' if verbose==2 else '',end='')\n",
    "    print(f'Spacing of segment:{segment.shape}' if verbose==2 else '',end='')\n",
    "    \n",
    "    # Windowing parameters for the abdomen\n",
    "    window_center = 40\n",
    "    window_width = 350\n",
    "    \n",
    "    # Window and resample the whole image\n",
    "    augmented_whole = window_image_to_adbomen(whole, window_center, window_width)\n",
    "\n",
    "    # Get the slice indices where the segment is present in \n",
    "    augmented_segment = segment\n",
    "    segmentedSlices = [] \n",
    "    for index in range(augmented_segment.shape[0]):\n",
    "        if len(np.unique(augmented_segment[index,:,:])) > 1:\n",
    "            segmentedSlices.append(index)\n",
    "\n",
    "    print(f'Segment slice indices:{segmentedSlices}' if verbose==2 else '',end='')\n",
    "\n",
    "\n",
    "    #Segment the whole image with the segment mask\n",
    "    overlay_segment = augmented_whole * augmented_segment    \n",
    " \n",
    "    if useBackground:\n",
    "        croppedSegment = centerXYOfImage(augmented_whole,augmented_segment,segmentedSlices, padding=5, scaledBoxes = scaledBoxes) # Crop the image to the center of the segmented region     \n",
    "    else:\n",
    "        croppedSegment = centerXYOfImage(overlay_segment,augmented_segment,segmentedSlices, padding=5, scaledBoxes = scaledBoxes) # Crop the image to the center of the segmented region     \n",
    "\n",
    "    # Resize image if no scaling is given already\n",
    "    if scaledBoxes==None:\n",
    "        croppedSegment = convertNdArrayToCV2Image(croppedSegment)\n",
    "\n",
    "    #Display the results of preprocessing\n",
    "    if verbose==1 or verbose==2:\n",
    "        displayCroppedSegmentations(croppedSegment)\n",
    "    elif verbose==3:\n",
    "        displayOverlayedSegmentations(segmentedSlices, augmented_whole, augmented_segment)\n",
    "    \n",
    "    return whole, croppedSegment, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current System: d:\\SimpsonLab\\researchpip\\lib\\site-packages\\ipykernel_launcher.py\n",
      "BatchSize: 8, Epochs: 100, Learning Rate: 0.001, Eval Detail Line: , Has Background: False, Uses Largest Box: True, Segments Multiple: 13, Dropout Rate: 0.2\n"
     ]
    }
   ],
   "source": [
    "#ADD argparser\n",
    "import argparse\n",
    "import sys\n",
    "print('Current System:',sys.argv[0])\n",
    "\n",
    "# python testSamples22-7.py -batchSize=8 -epochs=100 -lr=0.001 -evalDetailLine=\"\" -hasBackground=False -usesGlobalSize=True\n",
    "#Current\n",
    "# python testSamples23-7.py -batchSize=8 -epochs=100 -lr=0.001 -evalDetailLine=\"testLoading\" -hasBackground=f -usesLargestBox=f -segmentsMultiple=1 -dropoutRate=0.2 &&\n",
    "# python generatePreprocessCombinations.py -batchSize=8 -epochs=100 -lr=0.001 -evalDetailLine=\"testLoading\" -hasBackground=f -usesLargestBox=f -segmentsMultiple=13 -dropoutRate=0.2 &&\n",
    "# python generatePreprocessCombinations.py -batchSize=8 -epochs=100 -lr=0.001 -evalDetailLine=\"testLoading\" -hasBackground=t -usesLargestBox=t -segmentsMultiple=13 -dropoutRate=0.2 &&\n",
    "# python generatePreprocessCombinations.py -batchSize=8 -epochs=100 -lr=0.001 -evalDetailLine=\"testLoading\" -hasBackground=f -usesLargestBox=t -segmentsMultiple=13 -dropoutRate=0.2 &&\n",
    "# python generatePreprocessCombinations.py -batchSize=8 -epochs=100 -lr=0.001 -evalDetailLine=\"testLoading\" -hasBackground=t -usesLargestBox=f -segmentsMultiple=13 -dropoutRate=0.2\n",
    "\n",
    "\n",
    "#Check if we are using a notebook or not\n",
    "if 'ipykernel_launcher' in sys.argv[0]:\n",
    "    BATCHSIZE = 8\n",
    "    NUM_EPOCHS = 100\n",
    "    evalDetailLine = \"\"\n",
    "    learningRate = 0.001\n",
    "    hasBackground = False\n",
    "    usesLargestBox = True\n",
    "    segmentsMultiple = 13\n",
    "    dropoutRate = 0.2\n",
    "    grouped2D = False\n",
    "\n",
    "else:\n",
    "    parser = argparse.ArgumentParser(description=\"Model information\")\n",
    "    parser.add_argument('-batchSize', type=int, help='batch size', default=8)\n",
    "    parser.add_argument('-epochs', type=int, help='Number of Epochs', default=100)\n",
    "    parser.add_argument('-lr', type=float, help='Learning Rate', default=0.001)\n",
    "    parser.add_argument('-evalDetailLine', type=str, help='Details of the evaluation', default='')\n",
    "    parser.add_argument('-hasBackground', type=str, help='Whether to use the background (t to, f to not)', default='f')\n",
    "    parser.add_argument('-usesLargestBox', type=str, help='Where to use the size of the largest box (t) or independent tumor boxes (f)', default='t')\n",
    "    parser.add_argument('-segmentsMultiple', type=int, help='Segments a # of slices, 1 by default', default=1)\n",
    "    parser.add_argument('-dropoutRate', type=float, help='Dropout rate for the model', default=0.2)\n",
    "    parser.add_argument('-grouped2D', type=str, help='Grouping the 3D scans as individual 2D images', default='f')\n",
    "\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    BATCHSIZE = args.batchSize\n",
    "    NUM_EPOCHS = args.epochs\n",
    "    evalDetailLine = args.evalDetailLine\n",
    "    learningRate = args.lr\n",
    "    hasBackground = True if args.hasBackground=='t' else False\n",
    "    usesLargestBox = True if args.usesLargestBox=='t' else False\n",
    "    segmentsMultiple = args.segmentsMultiple\n",
    "    dropoutRate = args.dropoutRate\n",
    "    grouped2D = True if args.grouped2D=='t' else False\n",
    "\n",
    "\n",
    "print(f'BatchSize: {BATCHSIZE}, Epochs: {NUM_EPOCHS}, Learning Rate: {learningRate}, Eval Detail Line: {evalDetailLine}, Has Background: {hasBackground}, Uses Largest Box: {usesLargestBox}, Segments Multiple: {segmentsMultiple}, Dropout Rate: {dropoutRate}, Grouped2D: {grouped2D}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using box dimension of: (82, 109)\n",
      "===================================================\n",
      "PERFORMING GROUPINGS\n",
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "1105\n",
      "['CASE244', 'CASE244', 'CASE244', 'CASE244', 'CASE244', 'CASE244', 'CASE244', 'CASE244', 'CASE244', 'CASE244', 'CASE244', 'CASE244', 'CASE244', 'CASE246', 'CASE246', 'CASE246', 'CASE246', 'CASE246', 'CASE246', 'CASE246', 'CASE246', 'CASE246', 'CASE246', 'CASE246', 'CASE246', 'CASE246', 'CASE247', 'CASE247', 'CASE247', 'CASE247', 'CASE247', 'CASE247', 'CASE247', 'CASE247', 'CASE247', 'CASE247', 'CASE247', 'CASE247', 'CASE247', 'CASE251', 'CASE251', 'CASE251', 'CASE251', 'CASE251', 'CASE251', 'CASE251', 'CASE251', 'CASE251', 'CASE251', 'CASE251', 'CASE251', 'CASE251', 'CASE254', 'CASE254', 'CASE254', 'CASE254', 'CASE254', 'CASE254', 'CASE254', 'CASE254', 'CASE254', 'CASE254', 'CASE254', 'CASE254', 'CASE254', 'CASE256', 'CASE256', 'CASE256', 'CASE256', 'CASE256', 'CASE256', 'CASE256', 'CASE256', 'CASE256', 'CASE256', 'CASE256', 'CASE256', 'CASE256', 'CASE263', 'CASE263', 'CASE263', 'CASE263', 'CASE263', 'CASE263', 'CASE263', 'CASE263', 'CASE263', 'CASE263', 'CASE263', 'CASE263', 'CASE263', 'CASE264', 'CASE264', 'CASE264', 'CASE264', 'CASE264', 'CASE264', 'CASE264', 'CASE264', 'CASE264', 'CASE264', 'CASE264', 'CASE264', 'CASE264', 'CASE265', 'CASE265', 'CASE265', 'CASE265', 'CASE265', 'CASE265', 'CASE265', 'CASE265', 'CASE265', 'CASE265', 'CASE265', 'CASE265', 'CASE265', 'CASE270', 'CASE270', 'CASE270', 'CASE270', 'CASE270', 'CASE270', 'CASE270', 'CASE270', 'CASE270', 'CASE270', 'CASE270', 'CASE270', 'CASE270', 'CASE274', 'CASE274', 'CASE274', 'CASE274', 'CASE274', 'CASE274', 'CASE274', 'CASE274', 'CASE274', 'CASE274', 'CASE274', 'CASE274', 'CASE274', 'CASE467', 'CASE467', 'CASE467', 'CASE467', 'CASE467', 'CASE467', 'CASE467', 'CASE467', 'CASE467', 'CASE467', 'CASE467', 'CASE467', 'CASE467', 'CASE468', 'CASE468', 'CASE468', 'CASE468', 'CASE468', 'CASE468', 'CASE468', 'CASE468', 'CASE468', 'CASE468', 'CASE468', 'CASE468', 'CASE468', 'CASE471', 'CASE471', 'CASE471', 'CASE471', 'CASE471', 'CASE471', 'CASE471', 'CASE471', 'CASE471', 'CASE471', 'CASE471', 'CASE471', 'CASE471', 'CASE472', 'CASE472', 'CASE472', 'CASE472', 'CASE472', 'CASE472', 'CASE472', 'CASE472', 'CASE472', 'CASE472', 'CASE472', 'CASE472', 'CASE472', 'CASE476', 'CASE476', 'CASE476', 'CASE476', 'CASE476', 'CASE476', 'CASE476', 'CASE476', 'CASE476', 'CASE476', 'CASE476', 'CASE476', 'CASE476', 'CASE479', 'CASE479', 'CASE479', 'CASE479', 'CASE479', 'CASE479', 'CASE479', 'CASE479', 'CASE479', 'CASE479', 'CASE479', 'CASE479', 'CASE479', 'CASE480', 'CASE480', 'CASE480', 'CASE480', 'CASE480', 'CASE480', 'CASE480', 'CASE480', 'CASE480', 'CASE480', 'CASE480', 'CASE480', 'CASE480', 'CASE482', 'CASE482', 'CASE482', 'CASE482', 'CASE482', 'CASE482', 'CASE482', 'CASE482', 'CASE482', 'CASE482', 'CASE482', 'CASE482', 'CASE482', 'CASE484', 'CASE484', 'CASE484', 'CASE484', 'CASE484', 'CASE484', 'CASE484', 'CASE484', 'CASE484', 'CASE484', 'CASE484', 'CASE484', 'CASE484', 'CASE485', 'CASE485', 'CASE485', 'CASE485', 'CASE485', 'CASE485', 'CASE485', 'CASE485', 'CASE485', 'CASE485', 'CASE485', 'CASE485', 'CASE485', 'CASE488', 'CASE488', 'CASE488', 'CASE488', 'CASE488', 'CASE488', 'CASE488', 'CASE488', 'CASE488', 'CASE488', 'CASE488', 'CASE488', 'CASE488', 'CASE494', 'CASE494', 'CASE494', 'CASE494', 'CASE494', 'CASE494', 'CASE494', 'CASE494', 'CASE494', 'CASE494', 'CASE494', 'CASE494', 'CASE494', 'CASE496', 'CASE496', 'CASE496', 'CASE496', 'CASE496', 'CASE496', 'CASE496', 'CASE496', 'CASE496', 'CASE496', 'CASE496', 'CASE496', 'CASE496', 'CASE499', 'CASE499', 'CASE499', 'CASE499', 'CASE499', 'CASE499', 'CASE499', 'CASE499', 'CASE499', 'CASE499', 'CASE499', 'CASE499', 'CASE499', 'CASE500', 'CASE500', 'CASE500', 'CASE500', 'CASE500', 'CASE500', 'CASE500', 'CASE500', 'CASE500', 'CASE500', 'CASE500', 'CASE500', 'CASE500', 'CASE505', 'CASE505', 'CASE505', 'CASE505', 'CASE505', 'CASE505', 'CASE505', 'CASE505', 'CASE505', 'CASE505', 'CASE505', 'CASE505', 'CASE505', 'CASE515', 'CASE515', 'CASE515', 'CASE515', 'CASE515', 'CASE515', 'CASE515', 'CASE515', 'CASE515', 'CASE515', 'CASE515', 'CASE515', 'CASE515', 'CASE523', 'CASE523', 'CASE523', 'CASE523', 'CASE523', 'CASE523', 'CASE523', 'CASE523', 'CASE523', 'CASE523', 'CASE523', 'CASE523', 'CASE523', 'CASE525', 'CASE525', 'CASE525', 'CASE525', 'CASE525', 'CASE525', 'CASE525', 'CASE525', 'CASE525', 'CASE525', 'CASE525', 'CASE525', 'CASE525', 'CASE531', 'CASE531', 'CASE531', 'CASE531', 'CASE531', 'CASE531', 'CASE531', 'CASE531', 'CASE531', 'CASE531', 'CASE531', 'CASE531', 'CASE531', 'CASE534', 'CASE534', 'CASE534', 'CASE534', 'CASE534', 'CASE534', 'CASE534', 'CASE534', 'CASE534', 'CASE534', 'CASE534', 'CASE534', 'CASE534', 'CASE535', 'CASE535', 'CASE535', 'CASE535', 'CASE535', 'CASE535', 'CASE535', 'CASE535', 'CASE535', 'CASE535', 'CASE535', 'CASE535', 'CASE535', 'CASE537', 'CASE537', 'CASE537', 'CASE537', 'CASE537', 'CASE537', 'CASE537', 'CASE537', 'CASE537', 'CASE537', 'CASE537', 'CASE537', 'CASE537', 'CASE539', 'CASE539', 'CASE539', 'CASE539', 'CASE539', 'CASE539', 'CASE539', 'CASE539', 'CASE539', 'CASE539', 'CASE539', 'CASE539', 'CASE539', 'CASE541', 'CASE541', 'CASE541', 'CASE541', 'CASE541', 'CASE541', 'CASE541', 'CASE541', 'CASE541', 'CASE541', 'CASE541', 'CASE541', 'CASE541', 'CASE543', 'CASE543', 'CASE543', 'CASE543', 'CASE543', 'CASE543', 'CASE543', 'CASE543', 'CASE543', 'CASE543', 'CASE543', 'CASE543', 'CASE543', 'CASE546', 'CASE546', 'CASE546', 'CASE546', 'CASE546', 'CASE546', 'CASE546', 'CASE546', 'CASE546', 'CASE546', 'CASE546', 'CASE546', 'CASE546', 'CASE547', 'CASE547', 'CASE547', 'CASE547', 'CASE547', 'CASE547', 'CASE547', 'CASE547', 'CASE547', 'CASE547', 'CASE547', 'CASE547', 'CASE547', 'CASE548', 'CASE548', 'CASE548', 'CASE548', 'CASE548', 'CASE548', 'CASE548', 'CASE548', 'CASE548', 'CASE548', 'CASE548', 'CASE548', 'CASE548', 'CASE549', 'CASE549', 'CASE549', 'CASE549', 'CASE549', 'CASE549', 'CASE549', 'CASE549', 'CASE549', 'CASE549', 'CASE549', 'CASE549', 'CASE549', 'CASE551', 'CASE551', 'CASE551', 'CASE551', 'CASE551', 'CASE551', 'CASE551', 'CASE551', 'CASE551', 'CASE551', 'CASE551', 'CASE551', 'CASE551', 'CASE554', 'CASE554', 'CASE554', 'CASE554', 'CASE554', 'CASE554', 'CASE554', 'CASE554', 'CASE554', 'CASE554', 'CASE554', 'CASE554', 'CASE554', 'CASE555', 'CASE555', 'CASE555', 'CASE555', 'CASE555', 'CASE555', 'CASE555', 'CASE555', 'CASE555', 'CASE555', 'CASE555', 'CASE555', 'CASE555', 'CASE557', 'CASE557', 'CASE557', 'CASE557', 'CASE557', 'CASE557', 'CASE557', 'CASE557', 'CASE557', 'CASE557', 'CASE557', 'CASE557', 'CASE557', 'CASE559', 'CASE559', 'CASE559', 'CASE559', 'CASE559', 'CASE559', 'CASE559', 'CASE559', 'CASE559', 'CASE559', 'CASE559', 'CASE559', 'CASE559', 'CASE560', 'CASE560', 'CASE560', 'CASE560', 'CASE560', 'CASE560', 'CASE560', 'CASE560', 'CASE560', 'CASE560', 'CASE560', 'CASE560', 'CASE560', 'CASE562', 'CASE562', 'CASE562', 'CASE562', 'CASE562', 'CASE562', 'CASE562', 'CASE562', 'CASE562', 'CASE562', 'CASE562', 'CASE562', 'CASE562', 'CASE563', 'CASE563', 'CASE563', 'CASE563', 'CASE563', 'CASE563', 'CASE563', 'CASE563', 'CASE563', 'CASE563', 'CASE563', 'CASE563', 'CASE563', 'CASE564', 'CASE564', 'CASE564', 'CASE564', 'CASE564', 'CASE564', 'CASE564', 'CASE564', 'CASE564', 'CASE564', 'CASE564', 'CASE564', 'CASE564', 'CASE565', 'CASE565', 'CASE565', 'CASE565', 'CASE565', 'CASE565', 'CASE565', 'CASE565', 'CASE565', 'CASE565', 'CASE565', 'CASE565', 'CASE565', 'CASE568', 'CASE568', 'CASE568', 'CASE568', 'CASE568', 'CASE568', 'CASE568', 'CASE568', 'CASE568', 'CASE568', 'CASE568', 'CASE568', 'CASE568', 'CASE569', 'CASE569', 'CASE569', 'CASE569', 'CASE569', 'CASE569', 'CASE569', 'CASE569', 'CASE569', 'CASE569', 'CASE569', 'CASE569', 'CASE569', 'CASE572', 'CASE572', 'CASE572', 'CASE572', 'CASE572', 'CASE572', 'CASE572', 'CASE572', 'CASE572', 'CASE572', 'CASE572', 'CASE572', 'CASE572', 'CASE574', 'CASE574', 'CASE574', 'CASE574', 'CASE574', 'CASE574', 'CASE574', 'CASE574', 'CASE574', 'CASE574', 'CASE574', 'CASE574', 'CASE574', 'CASE575', 'CASE575', 'CASE575', 'CASE575', 'CASE575', 'CASE575', 'CASE575', 'CASE575', 'CASE575', 'CASE575', 'CASE575', 'CASE575', 'CASE575', 'CASE577', 'CASE577', 'CASE577', 'CASE577', 'CASE577', 'CASE577', 'CASE577', 'CASE577', 'CASE577', 'CASE577', 'CASE577', 'CASE577', 'CASE577', 'CASE578', 'CASE578', 'CASE578', 'CASE578', 'CASE578', 'CASE578', 'CASE578', 'CASE578', 'CASE578', 'CASE578', 'CASE578', 'CASE578', 'CASE578', 'CASE580', 'CASE580', 'CASE580', 'CASE580', 'CASE580', 'CASE580', 'CASE580', 'CASE580', 'CASE580', 'CASE580', 'CASE580', 'CASE580', 'CASE580', 'CASE581', 'CASE581', 'CASE581', 'CASE581', 'CASE581', 'CASE581', 'CASE581', 'CASE581', 'CASE581', 'CASE581', 'CASE581', 'CASE581', 'CASE581', 'CASE585', 'CASE585', 'CASE585', 'CASE585', 'CASE585', 'CASE585', 'CASE585', 'CASE585', 'CASE585', 'CASE585', 'CASE585', 'CASE585', 'CASE585', 'CASE587', 'CASE587', 'CASE587', 'CASE587', 'CASE587', 'CASE587', 'CASE587', 'CASE587', 'CASE587', 'CASE587', 'CASE587', 'CASE587', 'CASE587', 'CASE588', 'CASE588', 'CASE588', 'CASE588', 'CASE588', 'CASE588', 'CASE588', 'CASE588', 'CASE588', 'CASE588', 'CASE588', 'CASE588', 'CASE588', 'CASE589', 'CASE589', 'CASE589', 'CASE589', 'CASE589', 'CASE589', 'CASE589', 'CASE589', 'CASE589', 'CASE589', 'CASE589', 'CASE589', 'CASE589', 'CASE593', 'CASE593', 'CASE593', 'CASE593', 'CASE593', 'CASE593', 'CASE593', 'CASE593', 'CASE593', 'CASE593', 'CASE593', 'CASE593', 'CASE593', 'CASE594', 'CASE594', 'CASE594', 'CASE594', 'CASE594', 'CASE594', 'CASE594', 'CASE594', 'CASE594', 'CASE594', 'CASE594', 'CASE594', 'CASE594', 'CASE596', 'CASE596', 'CASE596', 'CASE596', 'CASE596', 'CASE596', 'CASE596', 'CASE596', 'CASE596', 'CASE596', 'CASE596', 'CASE596', 'CASE596', 'CASE598', 'CASE598', 'CASE598', 'CASE598', 'CASE598', 'CASE598', 'CASE598', 'CASE598', 'CASE598', 'CASE598', 'CASE598', 'CASE598', 'CASE598', 'CASE600', 'CASE600', 'CASE600', 'CASE600', 'CASE600', 'CASE600', 'CASE600', 'CASE600', 'CASE600', 'CASE600', 'CASE600', 'CASE600', 'CASE600', 'CASE601', 'CASE601', 'CASE601', 'CASE601', 'CASE601', 'CASE601', 'CASE601', 'CASE601', 'CASE601', 'CASE601', 'CASE601', 'CASE601', 'CASE601', 'CASE602', 'CASE602', 'CASE602', 'CASE602', 'CASE602', 'CASE602', 'CASE602', 'CASE602', 'CASE602', 'CASE602', 'CASE602', 'CASE602', 'CASE602', 'CASE603', 'CASE603', 'CASE603', 'CASE603', 'CASE603', 'CASE603', 'CASE603', 'CASE603', 'CASE603', 'CASE603', 'CASE603', 'CASE603', 'CASE603', 'CASE604', 'CASE604', 'CASE604', 'CASE604', 'CASE604', 'CASE604', 'CASE604', 'CASE604', 'CASE604', 'CASE604', 'CASE604', 'CASE604', 'CASE604', 'CASE608', 'CASE608', 'CASE608', 'CASE608', 'CASE608', 'CASE608', 'CASE608', 'CASE608', 'CASE608', 'CASE608', 'CASE608', 'CASE608', 'CASE608', 'CASE610', 'CASE610', 'CASE610', 'CASE610', 'CASE610', 'CASE610', 'CASE610', 'CASE610', 'CASE610', 'CASE610', 'CASE610', 'CASE610', 'CASE610', 'CASE611', 'CASE611', 'CASE611', 'CASE611', 'CASE611', 'CASE611', 'CASE611', 'CASE611', 'CASE611', 'CASE611', 'CASE611', 'CASE611', 'CASE611', 'CASE615', 'CASE615', 'CASE615', 'CASE615', 'CASE615', 'CASE615', 'CASE615', 'CASE615', 'CASE615', 'CASE615', 'CASE615', 'CASE615', 'CASE615', 'CASE616', 'CASE616', 'CASE616', 'CASE616', 'CASE616', 'CASE616', 'CASE616', 'CASE616', 'CASE616', 'CASE616', 'CASE616', 'CASE616', 'CASE616', 'CASE621', 'CASE621', 'CASE621', 'CASE621', 'CASE621', 'CASE621', 'CASE621', 'CASE621', 'CASE621', 'CASE621', 'CASE621', 'CASE621', 'CASE621', 'CASE622', 'CASE622', 'CASE622', 'CASE622', 'CASE622', 'CASE622', 'CASE622', 'CASE622', 'CASE622', 'CASE622', 'CASE622', 'CASE622', 'CASE622', 'CASE623', 'CASE623', 'CASE623', 'CASE623', 'CASE623', 'CASE623', 'CASE623', 'CASE623', 'CASE623', 'CASE623', 'CASE623', 'CASE623', 'CASE623', 'CASE624', 'CASE624', 'CASE624', 'CASE624', 'CASE624', 'CASE624', 'CASE624', 'CASE624', 'CASE624', 'CASE624', 'CASE624', 'CASE624', 'CASE624', 'CASE630', 'CASE630', 'CASE630', 'CASE630', 'CASE630', 'CASE630', 'CASE630', 'CASE630', 'CASE630', 'CASE630', 'CASE630', 'CASE630', 'CASE630', 'CASE632', 'CASE632', 'CASE632', 'CASE632', 'CASE632', 'CASE632', 'CASE632', 'CASE632', 'CASE632', 'CASE632', 'CASE632', 'CASE632', 'CASE632', 'CASE635', 'CASE635', 'CASE635', 'CASE635', 'CASE635', 'CASE635', 'CASE635', 'CASE635', 'CASE635', 'CASE635', 'CASE635', 'CASE635', 'CASE635']\n",
      "1105\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "allFolders = ['CASE244','CASE246','CASE247','CASE251','CASE254','CASE256','CASE263','CASE264','CASE265','CASE270','CASE272','CASE274',\n",
    "                'CASE467','CASE468','CASE470','CASE471','CASE472','CASE476','CASE479','CASE480','CASE482','CASE484','CASE485','CASE488','CASE494','CASE496','CASE499',\n",
    "                'CASE500','CASE505','CASE515','CASE520','CASE523','CASE525','CASE531','CASE533','CASE534','CASE535','CASE537','CASE539','CASE541','CASE543','CASE546','CASE547','CASE548','CASE549','CASE550','CASE551','CASE554','CASE555','CASE557','CASE559','CASE560','CASE562','CASE563','CASE564','CASE565','CASE568','CASE569','CASE572','CASE574','CASE575','CASE577','CASE578','CASE580','CASE581','CASE585','CASE586','CASE587','CASE588','CASE589','CASE593','CASE594','CASE596','CASE598',\n",
    "                'CASE600','CASE601','CASE602','CASE603','CASE604','CASE605','CASE608','CASE610','CASE611','CASE615','CASE616','CASE621','CASE622','CASE623','CASE624','CASE629','CASE630','CASE632','CASE635']\n",
    "\n",
    "\n",
    "onlySeeTheseCases = allFolders#['CASE537','CASE585','CASE587']\n",
    "baseFilepath = 'Pre-treatment-only-pres/'\n",
    "desiredSliceNumber=13\n",
    "croppedSegmentsList = []\n",
    "groups = []\n",
    "\n",
    "# Decide to use what size of box, either the largest box size for all images or one that just fits the tumor and resizes\n",
    "if usesLargestBox:\n",
    "    dimensions = (82, 109)\n",
    "    #dimensions = findLargestBoxSize(cases) #Find the largest dimension\n",
    "else:\n",
    "    dimensions = None\n",
    "print('Using box dimension of:',dimensions)\n",
    "\n",
    "\n",
    "##================================================================================================\n",
    "# Add groupings to the dataset if specified to do so\n",
    "if grouped2D:\n",
    "    recistCriteriatempList = []\n",
    "    caseIDTempList = []\n",
    "    for caseID, criteria in zip(cases, recistCriteria):\n",
    "        recistCriteriatempList += [criteria]*desiredSliceNumber\n",
    "        caseIDTempList += [caseID]*desiredSliceNumber\n",
    "\n",
    "    cases = caseIDTempList\n",
    "    recistCriteria = recistCriteriatempList\n",
    "    print('===================================================')\n",
    "    print('PERFORMING GROUPINGS')\n",
    "    print('===================================================')\n",
    "##================================================================================================\n",
    "\n",
    "# ALREADY SAVED IN .pkl FILES\n",
    "\n",
    "# # Get all cropped segments\n",
    "# for folder in sorted(os.listdir(baseFilepath)):\n",
    "#     # Skip cases that are not in the excel sheet\n",
    "#     if folder not in cases:\n",
    "#         continue\n",
    "#     # Exclude to cases that we haven't seen yet\n",
    "#     if folder not in onlySeeTheseCases:\n",
    "#         continue \n",
    "#     count = 0\n",
    "#     preSegmentHeader = None\n",
    "#     wholeHeader = None\n",
    "#     for file in os.listdir(os.path.join(baseFilepath,folder)):\n",
    "            \n",
    "#         if 'TUM' in file or 'SMV' in file: # pre-treatment segmentation \n",
    "#             # segment, segmentHeader = nrrd.read(os.path.join(baseFilepath,folder,file))\n",
    "#             preSegmentHeader = sitk.ReadImage(os.path.join(baseFilepath,folder,file))\n",
    "#         elif file.endswith('CT.nrrd'): # whole ct scan\n",
    "#             wholeHeader = sitk.ReadImage(os.path.join(baseFilepath,folder,file))\n",
    "    \n",
    "#     print('==============================================================')\n",
    "#     print(folder, 'All files read:')\n",
    "    \n",
    "#     whole, croppedSegment,error = preprocess(wholeHeader, preSegmentHeader, verbose=0, useBackground=hasBackground, scaledBoxes=dimensions) \n",
    "#     if error:\n",
    "#         print('Error in preprocessing')\n",
    "#         # continue\n",
    "    \n",
    "#     # groups += [folder]*desiredSliceNumber # Add the segment slices to the group\n",
    "    \n",
    "#     if segmentsMultiple==1:\n",
    "#         largestSlice,_ = getLargestSlice(croppedSegment)\n",
    "#         updatedCroppedSegment = croppedSegment[largestSlice,:,:]\n",
    "#     else:\n",
    "#         updatedCroppedSegment = updateSlices(croppedSegment,desiredSliceNumber)\n",
    "        \n",
    "#     croppedSegmentsList.append(updatedCroppedSegment)\n",
    "    \n",
    "# #Uncomment if need to generate new data\n",
    "# name = f'{os.getcwd()}/preprocessCombinations/hasBackground={hasBackground}-usesLargestBox={usesLargestBox}-segmentsMultiple={segmentsMultiple}'\n",
    "# np.save(f'{name}.npy',croppedSegmentsList, allow_pickle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "croppedSegmentsList Shape: (1105, 92, 118)\n"
     ]
    }
   ],
   "source": [
    "#Save the results of the different combinations of backgrounds and sizes\n",
    "\n",
    "name = f'hasBackground={hasBackground}-usesLargestBox={usesLargestBox}-segmentsMultiple={segmentsMultiple}'        \n",
    "croppedSegmentsList = np.load(f'preprocessCombinations/{name}.npy')\n",
    "\n",
    "\n",
    "if len(recistCriteria) > 100: #if >100 then we are doing groupings of 2D images\n",
    "    temp = np.zeros((croppedSegmentsList.shape[0]*croppedSegmentsList.shape[1], croppedSegmentsList.shape[2], croppedSegmentsList.shape[3]))\n",
    "    for image in range(croppedSegmentsList.shape[0]):\n",
    "        for slice in range(croppedSegmentsList.shape[1]):\n",
    "            temp[image*croppedSegmentsList.shape[1]+slice,:,:] = croppedSegmentsList[image,slice,:,:]\n",
    "    \n",
    "    croppedSegmentsList = temp\n",
    "    \n",
    "print('croppedSegmentsList Shape:', croppedSegmentsList.shape)   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure,axis = plt.subplots(1,len(croppedSegmentsList),figsize=(200,100))\n",
    "# for idx in range(len(croppedSegmentsList)):        \n",
    "#     axis[idx].imshow(croppedSegmentsList[idx], cmap=\"gray\")\n",
    "#     axis[idx].axis('off')\n",
    "# plt.savefig('padding10.png')\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting information about the transformations\n",
    "def generateTransform(RandomHorizontalFlipValue=0.5,RandomVerticalFlipValue=0.5, RandomRotationValue=50, RandomElaticTransform=[0,0], brightnessConstant=0, contrastConstant=0, kernelSize=3, sigmaRange=(0.1,1.0)):\n",
    "    training_data_transforms = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomRotation(degrees=RandomRotationValue),\n",
    "        transforms.ElasticTransform(alpha=RandomElaticTransform[0], sigma=RandomElaticTransform[1]),\n",
    "        transforms.ColorJitter(brightnessConstant, contrastConstant),\n",
    "        transforms.GaussianBlur(kernel_size = kernelSize, sigma=sigmaRange),\n",
    "        transforms.RandomHorizontalFlip(p=RandomHorizontalFlipValue),\n",
    "        transforms.RandomVerticalFlip(p=RandomVerticalFlipValue),\n",
    "        transforms.ToTensor()\n",
    "    ]) \n",
    "    return training_data_transforms\n",
    "\n",
    "\n",
    "def getTransformValue(transform, desiredTranform, desiredTranformValue):\n",
    "    for t in transform.transforms:\n",
    "        if isinstance(t, desiredTranform):\n",
    "            return t.__getattribute__(desiredTranformValue)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For 2D images:\n",
    "# ## Working with pytorch tensors\n",
    "class TorchDataset(Dataset):\n",
    "    def __init__(self, image, classifications, transform=None):\n",
    "        self.data = image\n",
    "        temp = []\n",
    "        for classification in classifications:\n",
    "            convert = torch.tensor(classification, dtype=torch.int64) # casting to long\n",
    "            convert = convert.type(torch.LongTensor)\n",
    "            temp.append(convert)\n",
    "            \n",
    "        self.classification = temp\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.data[idx]\n",
    "        \n",
    "        # Normalize the images pre augmentation\n",
    "        image = (image - np.mean(image)) / np.std(image)\n",
    "        # Apply augmentations if there are any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # Normalize the images post augmentation\n",
    "        image = (image - torch.mean(image)) / torch.std(image)\n",
    "        label = self.classification[idx]\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "def convertDataToLoaders(xTrain, yTrain, xVal, yVal, xTest, yTest, training_data_transforms = None, batchSize=8):\n",
    "    ## Sample the data with 75% of the training set \n",
    "    # TrainBalancedSampler = WeightedRandomSampler(weightsForClasses, len(yTrain)//2+len(yTest)//4)\n",
    "        \n",
    "    # Testing data tranfrom, should be just the plain images\n",
    "    testing_data_transforms = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor()\n",
    "    ]) \n",
    "\n",
    "    # Convert the testing sets to data loaders\n",
    "    trainingData = TorchDataset(xTrain, yTrain, transform=training_data_transforms)\n",
    "    trainingData = DataLoader(trainingData, batch_size=batchSize, shuffle=False)#, sampler= TrainBalancedSampler)\n",
    "\n",
    "    validationData = TorchDataset(xVal, yVal, transform=testing_data_transforms)\n",
    "    validationData = DataLoader(validationData, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "    testingData = TorchDataset(xTest, yTest, transform=testing_data_transforms)\n",
    "    testingData = DataLoader(testingData, batch_size=batchSize, shuffle=False)\n",
    "\n",
    "    return trainingData, validationData, testingData, training_data_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For 3D images:\n",
    "\n",
    "# import torchio\n",
    "\n",
    "# # ## Working with pytorch tensors\n",
    "# class TorchDataset(Dataset):\n",
    "#     def __init__(self, images, classifications, transform=None):\n",
    "#         self.data = images\n",
    "#         temp = []\n",
    "#         for classification in classifications:\n",
    "#             convert = torch.tensor(classification, dtype=torch.int64) # casting to long\n",
    "#             convert = convert.type(torch.LongTensor)\n",
    "#             temp.append(convert)\n",
    "            \n",
    "#         self.classification = temp\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         images = self.data[idx]\n",
    "#         # Normalize the images\n",
    "#         images = (images - np.min(images)) / (np.max(images) - np.min(images))\n",
    "\n",
    "\n",
    "#         images = torch.from_numpy(images)\n",
    "#         images = torch.FloatTensor(images)\n",
    "\n",
    "#         # images = torch.from_numpy(images)\n",
    "#         print(type(images))\n",
    "        \n",
    "\n",
    "#         label = self.classification[idx]\n",
    "#         # Convert sample to a tensor\n",
    "#         # sample = torch.tensor(sample, dtype=torch.float32).permute(2, 0, 1)  # Change shape to (C, H, W)\n",
    "\n",
    "#         if self.transform:\n",
    "#             images = self.transform(images)\n",
    "        \n",
    "#         return images, label\n",
    "    \n",
    "\n",
    "# # Set the random seed for reproducibility\n",
    "# random.seed(0)\n",
    "# torch.manual_seed(0) \n",
    "\n",
    "# ## Sample the data with 75% of the training set \n",
    "# # TrainBalancedSampler = WeightedRandomSampler(weightsForClasses, len(yTrain)//2+len(yTest)//4)\n",
    "\n",
    "# # Define data augmentation transforms\n",
    "# training_data_transforms = torchio.Compose([\n",
    "#     torchio.RandomFlip(axes=('Left','Right'), flip_probability=RandomVerticalFlipProbablility),\n",
    "#     torchio.RandomFlip(axes=('Anterior','Posterior'), flip_probability=RandomHorizontalFlipProbablility),\n",
    "#     torchio.RandomNoise(std=(0, 0.1)),\n",
    "#     torchio.RandomBlur(std=(0, 1))\n",
    "# ]) \n",
    "# # testing_data_transforms = transforms.Compose([\n",
    "# #     transforms.ToPILImage(),\n",
    "# #     transforms.ToTensor()\n",
    "# # ]) \n",
    "\n",
    "\n",
    "# # Convert the testing sets to data loaders\n",
    "# trainingData = TorchDataset(xTrain, yTrain, transform=training_data_transforms)\n",
    "\n",
    "# trainingData = DataLoader(trainingData, batch_size=BATCHSIZE, shuffle=False)#, sampler= TrainBalancedSampler)\n",
    "\n",
    "# testingData = TorchDataset(xTest, yTest, transform=None)# testing_data_transforms)\n",
    "# testingData = DataLoader(testingData, batch_size=BATCHSIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50ClassificaitonModel(torch.nn.Module):\n",
    "    def __init__(self, dropoutRate=0.2):\n",
    "        super(ResNet50ClassificaitonModel, self).__init__()\n",
    "\n",
    "        #Resnet50 as first layer\n",
    "        self.resNet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "        # Modify the first convolutional layer to accept single-channel input\n",
    "        self.resNet50.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "\n",
    "        # Freeze all layers of the resNet\n",
    "        for param in self.resNet50.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        \n",
    "        # Hidden layers with batchnorms\n",
    "        self.batchNormalization0 = nn.BatchNorm1d(self.resNet50.fc.out_features)\n",
    "        self.hiddenLayer1 = nn.Linear(self.resNet50.fc.out_features, 528)\n",
    "        self.batchNormalization1 = nn.BatchNorm1d(528)\n",
    "        self.hiddenLayer2 = nn.Linear(528, 128)\n",
    "        self.batchNormalization2 = nn.BatchNorm1d(128)\n",
    "        self.hiddenLayer3 = nn.Linear(128, 64)\n",
    "        self.batchNormalization3 = nn.BatchNorm1d(64)\n",
    "\n",
    "        # Output layer\n",
    "        self.outputLayer = nn.Linear(64, 3)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        #Other layers\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropoutRate)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Into ResNet\n",
    "        x = self.resNet50(x)\n",
    "        x = self.batchNormalization0(x) if x.size(dim=0)>1 else x\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # Into hidden layer1\n",
    "        x = self.hiddenLayer1(x) \n",
    "        x = self.batchNormalization1(x) if x.size(dim=0)>1 else x\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # Into hidden layer2\n",
    "        x = self.hiddenLayer2(x)\n",
    "        x = self.batchNormalization2(x) if x.size(dim=0)>1 else x\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # Into hidden layer3\n",
    "        x = self.hiddenLayer3(x)\n",
    "        x = self.batchNormalization3(x) if x.size(dim=0)>1 else x\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        # Output layer\n",
    "        x = self.outputLayer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "def defineModel(dropoutRate=0.2,learningRate=0.001):\n",
    "    model = ResNet50ClassificaitonModel(dropoutRate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learningRate)\n",
    "\n",
    "    return model, criterion, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fmcib.models import fmcib_model\n",
    "\n",
    "# class FoundationClassificationModel(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(FoundationClassificationModel, self).__init__()\n",
    "#         # self.layers = nn.Sequential(\n",
    "#         self.foundationModel = fmcib_model()\n",
    "#         # Modify the first convolutional layer to accept single-channel input\n",
    "#         self.foundationModel.trunk.conv1 = nn.Conv3d(1, 128, kernel_size=(7, 7, 7), stride=(2, 2, 2), padding=(3, 3, 3), bias=False)\n",
    "#         self.activation = nn.ReLU(inplace=True)\n",
    "#         #Add when batchsize > 1\n",
    "#         # self.batchNormalization = nn.BatchNorm2d(self.resNet50.fc.out_features)\n",
    "        \n",
    "#         self.hiddenLayer1 = nn.Linear(len(self.foundationModel.trunk.avgpool.output_size), 64)\n",
    "#         self.softmax = nn.Softmax()\n",
    "#         self.outputLayer = nn.Linear(64, 3)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.foundationModel(x)\n",
    "#         x = self.activation(x)\n",
    "        \n",
    "#         # x = self.batchNormalization(x)\n",
    "#         x = self.hiddenLayer1(x)\n",
    "#         x = self.softmax(x)\n",
    "#         x = self.outputLayer(x)\n",
    "#         # x = self.softmax(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, minDelta=0):\n",
    "        self.patience = patience\n",
    "        self.minDelta = minDelta\n",
    "        self.counter = 0\n",
    "        self.minValLoss = float('inf')\n",
    "        \n",
    "    def earlyStoppingCheck(self, currValLoss):\n",
    "        if np.isnan(currValLoss):\n",
    "            return True\n",
    "        if currValLoss < self.minValLoss:\n",
    "            self.minValLoss = currValLoss\n",
    "            self.counter = 0\n",
    "        elif currValLoss > self.minValLoss + self.minDelta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            predictions.append(predicted)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / total\n",
    "    epoch_acc = correct / total\n",
    "    return epoch_loss, epoch_acc, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model, criterion, optimizer, trainingData, validationData, numberOfEpochs=100):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Using this device:', device)\n",
    "    #Send the model to the same device that the tensors are on\n",
    "    model.to(device)\n",
    "\n",
    "    earlyStopping = EarlyStopping(patience=10, minDelta=0)\n",
    "    train_loss, train_acc = [], []\n",
    "    val_loss, val_acc = [], []\n",
    "\n",
    "    for epoch in range(numberOfEpochs):\n",
    "        #Train model\n",
    "        curTrainLoss, curTrainAcc = train(model, trainingData, criterion, optimizer, device)    \n",
    "        print(f\"Epoch {epoch+1}/{numberOfEpochs}\")\n",
    "        print(f\"Train Loss: {curTrainLoss:.4f}, Train Acc: {curTrainAcc:.4f}\")\n",
    "        #Evaluate on validation set\n",
    "        curValLoss, curValAcc, _ = evaluate(model, validationData, criterion, device)    \n",
    "        print(f\"Val Loss: {curValLoss:.4f}, Val Acc: {curValAcc:.4f}\")\n",
    "\n",
    "        #Append metrics to lists\n",
    "        train_loss.append(curTrainLoss)\n",
    "        train_acc.append(curTrainAcc)\n",
    "        val_loss.append(curValLoss)\n",
    "        val_acc.append(curValAcc)\n",
    "\n",
    "        #Check for early stopping conditions\n",
    "        if earlyStopping.earlyStoppingCheck(curValLoss):\n",
    "            print(f'Early stopping - Val loss has not decreased in {earlyStopping.patience} epochs. Terminating training at epoch {epoch+1}.')\n",
    "            break\n",
    "\n",
    "    history = {'train_loss':train_loss, 'train_acc':train_acc, 'val_loss':val_loss, 'val_acc':val_acc}\n",
    "    return model, criterion, device, history, epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def read_history_from_pickle(testPathName):\n",
    "#     with open(testPathName+'/history.pkl', 'rb') as fp:\n",
    "#         history = pickle.load(fp)\n",
    "#     return history\n",
    "\n",
    "# #Read history\n",
    "# history = read_history_from_pickle(testPathName)\n",
    "\n",
    "# # Load and evalaute the model\n",
    "# modelWeightPath = testPathName+'/model.pt'\n",
    "# model = ResNet50ClassificaitonModel()\n",
    "# model.load_state_dict(torch.load(modelWeightPath))\n",
    "\n",
    "#Send the model to the device used\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print('Using this device:', device)\n",
    "# #Send the model to the same device that the tensors are on\n",
    "# model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the contents of this test\n",
    "\n",
    "def saveResults(testPathName, model, history, training_data_transforms, saveModel=True):\n",
    "    os.makedirs(testPathName, exist_ok=True)\n",
    "\n",
    "    #Save history as pickle\n",
    "    with open(testPathName+'/history.pkl', 'wb') as fp:\n",
    "        pickle.dump(history, fp)\n",
    "\n",
    "    # Save weigths of model\n",
    "    if saveModel:\n",
    "        torch.save(model.state_dict(), testPathName+'/model.pt')\n",
    "\n",
    "    #Save notebooks and scripts\n",
    "    # if os.path.exists('imageSegmentationMultipleSingleSlice.py'):\n",
    "    #     shutil.copy('imageSegmentationMultipleSingleSlice.py',testPathName+'/convertedScript.py')\n",
    "    # if os.path.exists('imageSegmentationMultipleSingleSlice.ipynb'):\n",
    "    #     shutil.copy('imageSegmentationMultipleSingleSlice.ipynb',testPathName+'/notebook.ipynb')\n",
    "\n",
    "    # Save transformations for easy access\n",
    "    f = open(testPathName + '/training_data_transforms.txt', 'w')\n",
    "            \n",
    "    for line in training_data_transforms.__str__():\n",
    "        f.write(line)\n",
    "    f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTensorToPredictions(tensorList, dictionary):\n",
    "        \"\"\"Converts a list of tensors into a single list and then feeds those values into a given dictionary.\"\"\"\n",
    "        values = []\n",
    "        for batch in tensorList:\n",
    "            batchValues = batch.tolist()        \n",
    "            values+= batchValues\n",
    "        for value in values:\n",
    "            dictionary[value]+=1\n",
    "        return values, dictionary\n",
    "\n",
    "def evaluateModelOnTestSet(testPathName, model, testingData, criterion, device, saveConfusionMatrix = True, showConfusionMatrix=True):\n",
    "    test_loss, test_acc, predictions = evaluate(model, testingData, criterion, device)\n",
    "\n",
    "    predicts, predictsTotal = convertTensorToPredictions(predictions, {0:0,1:0,2:0})\n",
    "\n",
    "    originalLabels = [label.to(device) for image, label in testingData]\n",
    "    ans, ansTotal = convertTensorToPredictions( originalLabels, {0:0,1:0,2:0})\n",
    "\n",
    "    # Test metrics\n",
    "    print('---------------------------------------\\nTesting Metrics')\n",
    "    accuracy = accuracy_score(ans, predicts)\n",
    "    f1 = f1_score(ans, predicts, average='weighted')  # Use 'weighted' for multiclass classification\n",
    "    recall = recall_score(ans, predicts, average='weighted')  # Use 'weighted' for multiclass classification\n",
    "\n",
    "    testingMetrics = {'Evaluation accuracy': test_acc, 'Predictions split': predictsTotal, 'Answers split': ansTotal, 'Predictions': predicts, 'Answers    ': ans,  'Accuracy':accuracy, 'F1 Score':f1, 'Recall':recall}\n",
    "\n",
    "    file = open(testPathName+'/testingMetrics.txt','w')\n",
    "    for key, value in testingMetrics.items():\n",
    "        file.write(f'{key}: {value}\\n')\n",
    "    file.close()\n",
    "\n",
    "    for key, value in testingMetrics.items():\n",
    "        print(f'{key}: {value}')\n",
    "\n",
    "\n",
    "    print('---------------------------------------\\nConfusion Matrix:')\n",
    "    # Confusion Matrix\n",
    "    result = confusion_matrix(ans,predicts,normalize='pred')\n",
    "    disp = ConfusionMatrixDisplay(result)\n",
    "    \n",
    "    if saveConfusionMatrix:\n",
    "        plt.savefig(testPathName+'/confusion_matrix.png')\n",
    "    \n",
    "    if showConfusionMatrix:\n",
    "        plt.show()\n",
    "\n",
    "    plt.clf() \n",
    "    return disp, accuracy, f1, recall, predictsTotal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Performance on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTraining(testPathName, testName, history, saveFigure=True, showResult=True):\n",
    "    plt.style.use('default')\n",
    "    \n",
    "    figure, ax = plt.subplots( 1, 2, figsize=(20, 10))\n",
    "    # plt.suptitle('Accuracy', fontsize=10)\n",
    "    ax[0].set_title(\"Loss\")\n",
    "    ax[0].set_ylabel('Loss', fontsize=16)\n",
    "    ax[0].set_xlabel('Epoch', fontsize=16)\n",
    "    ax[0].plot(history['train_loss'], label='Training Loss')\n",
    "    ax[0].plot(history['val_loss'], label='Validation Loss')\n",
    "    ax[0].legend(loc='upper right')\n",
    "\n",
    "    ax[1].set_title(\"Accuracy\")\n",
    "    ax[1].set_ylabel('Accuracy', fontsize=16)\n",
    "    ax[1].set_xlabel('Epoch', fontsize=16)\n",
    "    \n",
    "    ax[1].plot(history['train_acc'], label='Training Accuracy')\n",
    "    ax[1].plot(history['val_acc'], label='Validation Accuracy')\n",
    "    ax[1].legend(loc='lower right')\n",
    "\n",
    "    if saveFigure:\n",
    "        plt.savefig(testPathName+'/training_history.png')\n",
    "    \n",
    "    if showResult:\n",
    "        plt.show()\n",
    "\n",
    "    plt.clf() \n",
    "\n",
    "    return figure\n",
    "\n",
    "\n",
    "def plotTrainingPerformances(testPathName, testName, histories, saveFigure=True, showResult=True):\n",
    "    plt.style.use('default')\n",
    "\n",
    "    figure, ax = plt.subplots( 2, len(histories), figsize=(80, 20))\n",
    "    for idx, history in enumerate(histories):\n",
    "        # plt.suptitle('Accuracy', fontsize=10)\n",
    "        ax[0][idx].set_title(\"Loss\")\n",
    "        ax[0][idx].set_ylabel('Loss', fontsize=16)\n",
    "        ax[0][idx].set_xlabel('Epoch', fontsize=16)\n",
    "        ax[0][idx].plot(history['train_loss'], label='Training Loss')\n",
    "        ax[0][idx].plot(history['val_loss'], label='Validation Loss')\n",
    "        ax[0][idx].legend(loc='upper right')\n",
    "\n",
    "        ax[1][idx].set_title(\"Accuracy\")\n",
    "        ax[1][idx].set_ylabel('Accuracy', fontsize=16)\n",
    "        ax[1][idx].set_xlabel('Epoch', fontsize=16)\n",
    "        ax[1][idx].plot(history['train_acc'], label='Training Accuracy')\n",
    "        ax[1][idx].plot(history['val_acc'], label='Validation Accuracy')\n",
    "        ax[1][idx].legend(loc='lower right')\n",
    "\n",
    "    plt.suptitle(f'{testName} \\nTrainining Performance', fontsize=30)\n",
    "\n",
    "\n",
    "    if saveFigure:\n",
    "        plt.savefig(testPathName+'/training_history.png')\n",
    "    \n",
    "    if showResult:\n",
    "        plt.show()\n",
    "\n",
    "    plt.clf() \n",
    "\n",
    "    return figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run FullStack of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotConfusionMatricies(testPathName, testName, confusion_matricies):\n",
    "    figure,axis = plt.subplots(1,len(confusion_matricies),figsize=(20, 5))\n",
    "    for idx in range(len(confusion_matricies)):        \n",
    "        confusion_matricies[idx].plot(ax=axis[idx])\n",
    "        confusion_matricies[idx].im_.colorbar.remove()\n",
    "\n",
    "    figure.suptitle(f'{testName}\\nConfusion Matricies')\n",
    "    plt.savefig(testPathName+'/confusion_matrix.png')\n",
    "    plt.show()\n",
    "    plt.clf() \n",
    "    \n",
    "def meanConfidenceInterval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), scipy.stats.sem(a)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "\n",
    "    min = np.min(a)\n",
    "    max = np.max(a)\n",
    "\n",
    "    return [m, h, [min, max], data]\n",
    "\n",
    "def averagePredictionTotals(predictions, numberOfTrials=5):\n",
    "    average = {0:0,1:0,2:0}\n",
    "    for prediction in predictions:\n",
    "        for key, value in prediction.items():\n",
    "            average[key] += value\n",
    "    \n",
    "    for key,value in average.items():\n",
    "        average[key] = value/numberOfTrials\n",
    "\n",
    "    return average\n",
    "\n",
    "#Runs all model evaluations steps\n",
    "def runModelFullStack(testPathName, testName, xTrain, yTrain, xVal, yVal, xTest, yTest, trainingTransform=None, learningRate = 0.001, dropoutRate=0.2,batchSize=8, numberOfEpochs=100):\n",
    "    trainingData, validationData, testingData, training_data_transforms = convertDataToLoaders(xTrain, yTrain, xVal, yVal, xTest, yTest, training_data_transforms = trainingTransform, batchSize=batchSize)\n",
    "    model, criterion, optimizer = defineModel(dropoutRate=dropoutRate, learningRate = learningRate)\n",
    "    model, criterion, device, history, endingEpoch = trainModel(model, criterion, optimizer, trainingData,validationData, numberOfEpochs=numberOfEpochs)\n",
    "    saveResults(testPathName, model, history, training_data_transforms, saveModel=False)\n",
    "    confusionMatrix, accuracy, f1, recall, predictsTotal = evaluateModelOnTestSet(testPathName, model, testingData, criterion, device, saveConfusionMatrix = False, showConfusionMatrix=False)\n",
    "    # plotTraining(testPathName, testName, history, saveFigure=True, showResult=True)\n",
    "\n",
    "    return confusionMatrix, history, accuracy, f1, recall, predictsTotal, endingEpoch+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addEvalDetailToModel(evalDetailLine, dataframe):\n",
    "    dataframe.loc[dataframe.shape[0]] = [evalDetailLine] + ['']*(len(dataframe.columns)-1)\n",
    "    \n",
    "    \n",
    "def appendMetricsToXLSX(testPathName, trainingTransform, accuracyInformation, f1Information, recallInformation, predictsTotal, endingEpochs, batchSize, numberOfEpochs, learningRate, dataframe):\n",
    "    rotation = getTransformValue(trainingTransform, desiredTranform=transforms.RandomRotation, desiredTranformValue='degrees')\n",
    "    elasticTransform = [getTransformValue(trainingTransform, desiredTranform=transforms.ElasticTransform, desiredTranformValue='alpha'), getTransformValue(trainingTransform, desiredTranform=transforms.ElasticTransform, desiredTranformValue='sigma')]\n",
    "    brightness = getTransformValue(trainingTransform, desiredTranform=transforms.ColorJitter, desiredTranformValue='brightness')\n",
    "    contrast = getTransformValue(trainingTransform, desiredTranform=transforms.ColorJitter, desiredTranformValue='contrast')\n",
    "    guassianBlur = [ getTransformValue(trainingTransform, desiredTranform=transforms.GaussianBlur, desiredTranformValue='kernel_size'), getTransformValue(trainingTransform, desiredTranform=transforms.GaussianBlur, desiredTranformValue='sigma')]\n",
    "    randomHorizontalFlip = getTransformValue(trainingTransform, desiredTranform=transforms.RandomHorizontalFlip, desiredTranformValue='p')\n",
    "    randomVerticalFlip = getTransformValue(trainingTransform, desiredTranform=transforms.RandomVerticalFlip, desiredTranformValue='p')\n",
    "\n",
    "\n",
    "    exportValue = [testPathName, numberOfEpochs, batchSize, learningRate, rotation, elasticTransform, brightness, contrast, guassianBlur, randomHorizontalFlip, randomVerticalFlip, predictsTotal, accuracyInformation[0], accuracyInformation[1], \\\n",
    "      accuracyInformation[3], f1Information[0], f1Information[1], f1Information[3], recallInformation[0], recallInformation[1], recallInformation[3], endingEpochs]\n",
    "    dataframe.loc[dataframe.shape[0]] = exportValue + ['']*(len(dataframe.columns)-len(exportValue))\n",
    "    \n",
    "    \n",
    "def generateKFoldsValidation(identifier,identifierValue, k=5,trainingTransform=None, learningRate = 0.001, dropoutRate = 0.2,batchSize=8, numberOfEpochs=100,hasBackground=False, isEvenWeights=False):\n",
    "\n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(0)\n",
    "    torch.manual_seed(0) \n",
    "    \n",
    "    #Keep history of values\n",
    "    confusion_matricies = []\n",
    "    histories = []\n",
    "    accuracies = []\n",
    "    f1s = []\n",
    "    recalls = []\n",
    "    predictionSplits = []\n",
    "    endingEpochs = []\n",
    "\n",
    "    # run datasplit with stratified kfolds:\n",
    "    \n",
    "    if len(croppedSegmentsList) > 100: #if >100 then we are doing groupings of 2D images\n",
    "        stratifiedGroupFolds = StratifiedGroupKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        stratifiedGroupFolds.get_n_splits(croppedSegmentsList, recistCriteria)\n",
    "        splits = enumerate(stratifiedGroupFolds.split(croppedSegmentsList, recistCriteria, cases))\n",
    "        \n",
    "    else:\n",
    "        stratifiedFolds = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        stratifiedFolds.get_n_splits(croppedSegmentsList, recistCriteria)\n",
    "        splits = enumerate(stratifiedFolds.split(croppedSegmentsList, recistCriteria))\n",
    "        \n",
    "    for i, (train_index, test_index) in splits:\n",
    "        \n",
    "        #Set the name of the test\n",
    "        #testName = f'ResNet2D_5fold_SingleSlices_TransformBatch{currTransformIdx}_epochs-{NUM_EPOCHS}_batchsze-{BATCHSIZE}_onLaptop_3classes_dataArgs_hasBackground-{hasBackground}_evenWeights-{False}'\n",
    "        testName = f'{identifier}-{identifierValue}_batchsze-{batchSize}_hasBackground-{hasBackground}_evenWeights-{isEvenWeights}'\n",
    "        \n",
    "        testPathName = 'Tests/'+testName+f'/foldn{i+1}'\n",
    "        print(f'{identifier} RUN\\n=========================================')\n",
    "        xTest, yTest = [croppedSegmentsList[i] for i in test_index], [recistCriteria[i] for i in test_index]\n",
    "        xTrain, yTrain = [croppedSegmentsList[i] for i in train_index[len(xTest):]], [recistCriteria[i] for i in train_index[len(yTest):]]\n",
    "        xVal, yVal = [croppedSegmentsList[i] for i in train_index[:len(xTest)]], [recistCriteria[i] for i in train_index[:len(yTest)]]\n",
    "        \n",
    "        # Convert the recist criteria to 0,1,2\n",
    "        yTrain = [x-1 for x in yTrain]\n",
    "        yVal = [x-1 for x in yVal]\n",
    "        yTest = [x-1 for x in yTest]\n",
    "\n",
    "        # ## Working with Numpy arrays\n",
    "        xTrain = np.array(xTrain) \n",
    "        xTest = np.array(xTest)\n",
    "        xVal = np.array(xVal)\n",
    "        yTrain = np.array(yTrain)\n",
    "        yVal = np.array(yVal)\n",
    "        yTest = np.array(yTest)\n",
    "\n",
    "        xTrain = np.expand_dims(xTrain,axis=-1)\n",
    "        xVal = np.expand_dims(xVal,axis=-1)\n",
    "        xTest = np.expand_dims(xTest,axis=-1)\n",
    "\n",
    "        # Get and save results for each fold        \n",
    "        confusionMatrix, history, accuracy, f1, recall, predictsTotal, endingEpoch = runModelFullStack(testPathName, testName, xTrain, yTrain, xVal, yVal, xTest, yTest, trainingTransform=trainingTransform, learningRate = learningRate,dropoutRate=dropoutRate,batchSize=batchSize, numberOfEpochs=numberOfEpochs) \n",
    "\n",
    "        confusion_matricies.append(confusionMatrix)\n",
    "        histories.append(history)\n",
    "        accuracies.append(accuracy)\n",
    "        f1s.append(f1)\n",
    "        recalls.append(recall)\n",
    "        predictionSplits.append(predictsTotal)\n",
    "        endingEpochs.append(endingEpoch)\n",
    "        print('\\n\\n')\n",
    "\n",
    "\n",
    "\n",
    "    # Calculate the average of the metrics for the kfolds of this transformation and save it\n",
    "    kFoldsTestMetrics = {'Prediction averages': averagePredictionTotals(predictionSplits), 'Accuracy':meanConfidenceInterval(accuracies), 'F1 Score':meanConfidenceInterval(f1s), 'Recall':meanConfidenceInterval(recalls)}\n",
    "    file = open('Tests/'+testName+'/kFoldsTestMetrics.txt','w')\n",
    "    for key, value in kFoldsTestMetrics.items():\n",
    "        file.write(f'{key}: {value}\\n')\n",
    "        print(f'{key}: {value}')\n",
    "    file.close()\n",
    "\n",
    "    # Plot training and confusion matrix for each fold as a single .png\n",
    "    plotConfusionMatricies('Tests/'+testName, testName, confusion_matricies)\n",
    "    plotTrainingPerformances('Tests/'+testName, testName, histories, saveFigure=True, showResult=True)\n",
    "\n",
    "    # Append results to the xlsx file\n",
    "    appendMetricsToXLSX(testPathName, trainingTransform, meanConfidenceInterval(accuracies), meanConfidenceInterval(f1s), meanConfidenceInterval(recalls), averagePredictionTotals(predictionSplits), endingEpochs, batchSize, numberOfEpochs, learningRate, dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train recist category split: {0: 16, 1: 29, 2: 23}\n",
      "test recist category split: {0: 4, 1: 8, 2: 5}\n",
      "train recist category split: {0: 16, 1: 29, 2: 23}\n",
      "test recist category split: {0: 4, 1: 8, 2: 5}\n",
      "train recist category split: {0: 16, 1: 30, 2: 22}\n",
      "test recist category split: {0: 4, 1: 7, 2: 6}\n",
      "train recist category split: {0: 16, 1: 30, 2: 22}\n",
      "test recist category split: {0: 4, 1: 7, 2: 6}\n",
      "train recist category split: {0: 16, 1: 30, 2: 22}\n",
      "test recist category split: {0: 4, 1: 7, 2: 6}\n"
     ]
    }
   ],
   "source": [
    "# # View what the splits are for the models, the test set is {0: 4, 1: 7-8, 2: 6-5}\n",
    "# stratifiedFolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# stratifiedFolds.get_n_splits(croppedSegmentsList, recistCriteria)\n",
    "# for i, (train_index, test_index) in enumerate(stratifiedFolds.split(croppedSegmentsList, recistCriteria)):\n",
    "#         xTrain, xTest = [croppedSegmentsList[i] for i in train_index], [croppedSegmentsList[i] for i in test_index]\n",
    "#         yTrain, yTest = [recistCriteria[i] for i in train_index], [recistCriteria[i] for i in test_index]\n",
    "\n",
    "#         # Convert the recist criteria to 0,1,2\n",
    "#         yTrain = [x-1 for x in yTrain]\n",
    "#         yTest = [x-1 for x in yTest]\n",
    "\n",
    "#         #count the categorization splits of the train and test set\n",
    "#         trainRecistSplit = {y: yTrain.count(y) for y in yTrain}\n",
    "#         testRecistSplit = {y: yTest.count(y) for y in yTest}\n",
    "\n",
    "#         # Format the splits nicely into an ordered dictionary\n",
    "#         myKeys = list(trainRecistSplit.keys())\n",
    "#         myKeys.sort()\n",
    "#         trainRecistSplitDisplay = {i: trainRecistSplit[i] for i in myKeys}\n",
    "#         myKeys = list(testRecistSplit.keys())\n",
    "#         myKeys.sort()\n",
    "#         testRecistSplitDisplay = {i: testRecistSplit[i] for i in myKeys}\n",
    "\n",
    "#         print(f'train recist category split: {trainRecistSplitDisplay}')\n",
    "#         print(f'test recist category split: {testRecistSplitDisplay}')\n",
    "\n",
    "#         # ## Working with Numpy arrays\n",
    "#         xTrain = np.array(xTrain) \n",
    "#         xTest = np.array(xTest)\n",
    "#         yTrain = np.array(yTrain)\n",
    "#         yTest = np.array(yTest)\n",
    "#         xTrain =  np.expand_dims(xTrain,axis=-1)\n",
    "#         xTest =  np.expand_dims(xTest,axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SimpsonLab\\researchpip\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\SimpsonLab\\researchpip\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using this device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SimpsonLab\\researchpip\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 1.0833, Train Acc: 0.3997\n",
      "Val Loss: 1.1031, Val Acc: 0.4118\n",
      "Epoch 2/100\n",
      "Train Loss: 1.0462, Train Acc: 0.5068\n",
      "Val Loss: 1.1056, Val Acc: 0.4072\n",
      "Epoch 3/100\n",
      "Train Loss: 1.0383, Train Acc: 0.5053\n",
      "Val Loss: 1.1048, Val Acc: 0.4118\n",
      "Epoch 4/100\n",
      "Train Loss: 1.0315, Train Acc: 0.5083\n",
      "Val Loss: 1.1124, Val Acc: 0.4118\n",
      "Epoch 5/100\n",
      "Train Loss: 1.0308, Train Acc: 0.5083\n",
      "Val Loss: 1.1094, Val Acc: 0.4118\n",
      "Epoch 6/100\n",
      "Train Loss: 1.0265, Train Acc: 0.5113\n",
      "Val Loss: 1.1152, Val Acc: 0.4118\n",
      "Epoch 7/100\n",
      "Train Loss: 1.0346, Train Acc: 0.5128\n",
      "Val Loss: 1.1168, Val Acc: 0.4118\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Run the tests\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m transformsTested\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 29\u001b[0m     \u001b[43mgenerateKFoldsValidation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtrainingTransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearningRate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearningRate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropoutRate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropoutRate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCHSIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumberOfEpochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m dataframe\u001b[38;5;241m.\u001b[39mto_excel(dataframePath, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[30], line 77\u001b[0m, in \u001b[0;36mgenerateKFoldsValidation\u001b[1;34m(identifier, identifierValue, k, trainingTransform, learningRate, dropoutRate, batchSize, numberOfEpochs, hasBackground, isEvenWeights)\u001b[0m\n\u001b[0;32m     74\u001b[0m xTest \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(xTest,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Get and save results for each fold        \u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m confusionMatrix, history, accuracy, f1, recall, predictsTotal, endingEpoch \u001b[38;5;241m=\u001b[39m \u001b[43mrunModelFullStack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtestPathName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myTrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxVal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myVal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxTest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myTest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainingTransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainingTransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearningRate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearningRate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdropoutRate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropoutRate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatchSize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatchSize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumberOfEpochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumberOfEpochs\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     79\u001b[0m confusion_matricies\u001b[38;5;241m.\u001b[39mappend(confusionMatrix)\n\u001b[0;32m     80\u001b[0m histories\u001b[38;5;241m.\u001b[39mappend(history)\n",
      "Cell \u001b[1;32mIn[26], line 38\u001b[0m, in \u001b[0;36mrunModelFullStack\u001b[1;34m(testPathName, testName, xTrain, yTrain, xVal, yVal, xTest, yTest, trainingTransform, learningRate, dropoutRate, batchSize, numberOfEpochs)\u001b[0m\n\u001b[0;32m     36\u001b[0m trainingData, validationData, testingData, training_data_transforms \u001b[38;5;241m=\u001b[39m convertDataToLoaders(xTrain, yTrain, xVal, yVal, xTest, yTest, training_data_transforms \u001b[38;5;241m=\u001b[39m trainingTransform, batchSize\u001b[38;5;241m=\u001b[39mbatchSize)\n\u001b[0;32m     37\u001b[0m model, criterion, optimizer \u001b[38;5;241m=\u001b[39m defineModel(dropoutRate\u001b[38;5;241m=\u001b[39mdropoutRate, learningRate \u001b[38;5;241m=\u001b[39m learningRate)\n\u001b[1;32m---> 38\u001b[0m model, criterion, device, history, endingEpoch \u001b[38;5;241m=\u001b[39m \u001b[43mtrainModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainingData\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidationData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumberOfEpochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumberOfEpochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m saveResults(testPathName, model, history, training_data_transforms, saveModel\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     40\u001b[0m confusionMatrix, accuracy, f1, recall, predictsTotal \u001b[38;5;241m=\u001b[39m evaluateModelOnTestSet(testPathName, model, testingData, criterion, device, saveConfusionMatrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, showConfusionMatrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m, in \u001b[0;36mtrainModel\u001b[1;34m(model, criterion, optimizer, trainingData, validationData, numberOfEpochs)\u001b[0m\n\u001b[0;32m      9\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(numberOfEpochs):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m#Train model\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     curTrainLoss, curTrainAcc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainingData\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumberOfEpochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurTrainLoss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurTrainAcc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m      4\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      5\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m      8\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32md:\\SimpsonLab\\researchpip\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\SimpsonLab\\researchpip\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\SimpsonLab\\researchpip\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\SimpsonLab\\researchpip\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[16], line 22\u001b[0m, in \u001b[0;36mTorchDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Normalize the images pre augmentation\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m image \u001b[38;5;241m=\u001b[39m (image \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(image)) \u001b[38;5;241m/\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Apply augmentations if there are any\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mstd\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32md:\\SimpsonLab\\researchpip\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3605\u001b[0m, in \u001b[0;36mstd\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[0;32m   3602\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3603\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m std(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 3605\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_std\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3606\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\SimpsonLab\\researchpip\\lib\\site-packages\\numpy\\core\\_methods.py:269\u001b[0m, in \u001b[0;36m_std\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_std\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ddof\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    268\u001b[0m          where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 269\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mddof\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddof\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m               \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    273\u001b[0m         ret \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39msqrt(ret, out\u001b[38;5;241m=\u001b[39mret)\n",
      "File \u001b[1;32md:\\SimpsonLab\\researchpip\\lib\\site-packages\\numpy\\core\\_methods.py:236\u001b[0m, in \u001b[0;36m_var\u001b[1;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[0;32m    231\u001b[0m     arrmean \u001b[38;5;241m=\u001b[39m arrmean \u001b[38;5;241m/\u001b[39m rcount\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# Compute sum of squared deviations from mean\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Note that x may not be inexact and that we need it to be an array,\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# not a scalar.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m x \u001b[38;5;241m=\u001b[39m asanyarray(\u001b[43marr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43marrmean\u001b[49m)\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, (nt\u001b[38;5;241m.\u001b[39mfloating, nt\u001b[38;5;241m.\u001b[39minteger)):\n\u001b[0;32m    239\u001b[0m     x \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39mmultiply(x, x, out\u001b[38;5;241m=\u001b[39mx)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Data augmentations\n",
    "\n",
    "# Balance the classes used in the model\n",
    "# class1Percentage = (0.3333/16)\n",
    "# class2Percentage = (0.3333/30)\n",
    "# class3Percentage = (0.3333/24)\n",
    "\n",
    "# weightsForClasses = [class1Percentage / (class1Percentage+class2Percentage+class3Percentage) , class3Percentage / (class1Percentage+class2Percentage+class3Percentage), class3Percentage / (class1Percentage+class2Percentage+class3Percentage)]\n",
    "\n",
    "#Make all transforms that I am going to test:\n",
    "transformsTested = {\n",
    "    \"0%\":generateTransform(RandomRotationValue=0, RandomElaticTransform=[0.01,0.01], brightnessConstant=0, contrastConstant=0.0, kernelSize=3, sigmaRange=(0.001,0.002)),\n",
    "    \"20%\":generateTransform(RandomRotationValue=20, RandomElaticTransform=[0.2,0.2], brightnessConstant=0.2, contrastConstant=0.2, kernelSize=3, sigmaRange=(0.001,0.2)),\n",
    "    \"40%\":generateTransform(RandomRotationValue=40, RandomElaticTransform=[0.40,0.40], brightnessConstant=0.4, contrastConstant=0.40, kernelSize=3, sigmaRange=(0.2,0.4)),\n",
    "    \"60%\":generateTransform(RandomRotationValue=60, RandomElaticTransform=[0.60,0.60], brightnessConstant=0.6, contrastConstant=0.6, kernelSize=3, sigmaRange=(0.4,0.6)),\n",
    "    \"80%\":generateTransform(RandomRotationValue=80, RandomElaticTransform=[0.8,0.8], brightnessConstant=0.8, contrastConstant=0.8, kernelSize=3, sigmaRange=(0.6,0.8)),\n",
    "    \"100%\":generateTransform(RandomRotationValue=100, RandomElaticTransform=[1.0,1.0], brightnessConstant=1.0, contrastConstant=1.0, kernelSize=3, sigmaRange=(0.8,1.0))    \n",
    "}\n",
    "\n",
    "## Open the dataframe and add the evaluation details\n",
    "dataframePath='testResults.xlsx'\n",
    "columns = ['name','numOfEpochs','batchSize','LearningRate','RandomRotation','ElasticTransform','Brightness','Contrast','GaussianBlur','RandomHorizontalFlip','RandomVerticalFlip','PredictionAverage',\n",
    "            'AccuracyAverage','AccuracySTD','AccuracyList','F1Average','F1STD','F1Data','RecallAverage','RecallSTD','RecallData', 'EndingEpoch']\n",
    "dataframe = pd.read_excel('testResults.xlsx', header=None, names=columns)\n",
    "addEvalDetailToModel(evalDetailLine, dataframe)\n",
    "\n",
    "# Run the tests\n",
    "for key, value in transformsTested.items():\n",
    "    generateKFoldsValidation(key,\"-\", k=5,trainingTransform=value, learningRate = learningRate, dropoutRate=dropoutRate,batchSize=BATCHSIZE, numberOfEpochs=NUM_EPOCHS)\n",
    "\n",
    "dataframe.to_excel(dataframePath, index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
